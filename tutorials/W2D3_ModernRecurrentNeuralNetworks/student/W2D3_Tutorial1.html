
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Tutorial 1: Modeling sequencies and encoding text — Neuromatch Academy: Deep Learning</title>
<!-- Loaded before other Sphinx assets -->
<link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/styles/sphinx-book-theme.css" rel="stylesheet" type="text/css">
<link href="../../../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/mystnb.css" rel="stylesheet" type="text/css">
<link href="../../../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf" rel="preload"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/togglebutton.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script src="../../../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
<script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
<script async="async" src="../../../_static/sphinx-thebe.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
<link href="../../../_static/nma-dl-logo-square-4xp.jpeg" rel="shortcut icon">
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
<link href="W2D3_Tutorial2.html" rel="next" title="Tutorial 2: Modern RNNs and their variants"/>
<link href="../chapter_title.html" rel="prev" title="Modern Recurrent Neural Networks"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="None" name="docsearch:language"/>
<!-- Google Analytics -->
</link></link></link></link></link></link></head>
<body data-offset="60" data-spy="scroll" data-target="#bd-toc-nav">
<!-- Checkboxes to toggle the left sidebar -->
<input aria-label="Toggle navigation sidebar" class="sidebar-toggle" id="__navigation" name="__navigation" type="checkbox"/>
<label class="overlay overlay-navbar" for="__navigation">
<div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input aria-label="Toggle in-page Table of Contents" class="sidebar-toggle" id="__page-toc" name="__page-toc" type="checkbox"/>
<label class="overlay overlay-pagetoc" for="__page-toc">
<div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
<div class="bd-sidebar__content">
<div class="bd-sidebar__top"><div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
<!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
<img alt="logo" class="logo" src="../../../_static/nma-dl-logo-square-4xp.jpeg"/>
<h1 class="site-logo" id="site-title">Neuromatch Academy: Deep Learning</h1>
</a>
</div><form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search this book..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." type="search"/>
</form><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<ul class="nav bd-sidenav bd-sidenav__home-link">
<li class="toctree-l1">
<a class="reference internal" href="../../intro.html">
                    Introduction
                </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Schedule/schedule_intro.html">
   Schedule
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/daily_schedules.html">
     General schedule
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/shared_calendars.html">
     Shared calendars
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/timezone_widget.html">
     Timezone widget
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../TechnicalHelp/tech_intro.html">
   Technical Help
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox">
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../TechnicalHelp/Jupyterbook.html">
     Using jupyterbook
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox">
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_colab.html">
       Using Google Colab
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_kaggle.html">
       Using Kaggle
      </a>
</li>
</ul>
</input></li>
<li class="toctree-l2">
<a class="reference internal" href="../../TechnicalHelp/Discord.html">
     Using Discord
    </a>
</li>
</ul>
</input></li>
</ul>
<p class="caption">
<span class="caption-text">
  The Basics
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D1_BasicsAndPytorch/chapter_title.html">
   Basics And Pytorch (W1D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_BasicsAndPytorch/student/W1D1_Tutorial1.html">
     Tutorial 1: PyTorch
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/chapter_title.html">
   Linear Deep Learning (W1D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial1.html">
     Tutorial 1: Gradient Descent and AutoGrad
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial2.html">
     Tutorial 2: Learning Hyperparameters
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial3.html">
     Tutorial 3: Deep linear neural networks
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/chapter_title.html">
   Multi Layer Perceptrons (W1D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial1.html">
     Tutorial 1: Biological vs. Artificial Neural Networks
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial2.html">
     Tutorial 2: Deep MLPs
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D4_Optimization/chapter_title.html">
   Optimization (W1D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_Optimization/student/W1D4_Tutorial1.html">
     Tutorial 1: Optimization techniques
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D5_Regularization/chapter_title.html">
   Regularization (W1D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
<label for="toctree-checkbox-8">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_Regularization/student/W1D5_Tutorial1.html">
     Tutorial 1: Regularization techniques part 1
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_Regularization/student/W1D5_Tutorial2.html">
     Tutorial 2: Regularization techniques part 2
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/TheBasics.html">
   Deep Learning: The Basics Wrap-up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Doing More With Fewer Parameters
 </span>
</p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/chapter_title.html">
   Convnets And Recurrent Neural Networks (W2D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
<label for="toctree-checkbox-9">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial1.html">
     Tutorial 1: Introduction to CNNs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial2.html">
     Tutorial 2: Introduction to RNNs
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D2_ModernConvnets/chapter_title.html">
   Modern Convnets (W2D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
<label for="toctree-checkbox-10">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_ModernConvnets/student/W2D2_Tutorial1.html">
     Tutorial 1: Learn how to use modern convnets
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_ModernConvnets/student/W2D2_Tutorial2.html">
     (Bonus) Tutorial 2: Facial recognition using modern convnets
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 current active has-children">
<a class="reference internal" href="../chapter_title.html">
   Modern Recurrent Neural Networks (W2D3)
  </a>
<input checked="" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
<label for="toctree-checkbox-11">
<i class="fas fa-chevron-down">
</i>
</label>
<ul class="current">
<li class="toctree-l2 current active">
<a class="current reference internal" href="#">
     Tutorial 1: Modeling sequencies and encoding text
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W2D3_Tutorial2.html">
     Tutorial 2: Modern RNNs and their variants
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D4_AttentionAndTransformers/chapter_title.html">
   Attention And Transformers (W2D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
<label for="toctree-checkbox-12">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_AttentionAndTransformers/student/W2D4_Tutorial1.html">
     Tutorial 1: Learn how to work with Transformers
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D5_GenerativeModels/chapter_title.html">
   Generative Models (W2D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
<label for="toctree-checkbox-13">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial1.html">
     Tutorial 1: Variational Autoencoders (VAEs)
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial2.html">
     Tutorial 2: Introduction to GANs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial3.html">
     Tutorial 3: Conditional GANs and Implications of GAN Technology
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial4.html">
     (Bonus) Tutorial 4: Deploying Neural Networks on the Web
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/DoingMoreWithFewerParameters.html">
   Deep Learning: Doing more with fewer parameters Wrap-up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Advanced Topics
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D1_UnsupervisedAndSelfSupervisedLearning/chapter_title.html">
   Unsupervised And Self Supervised Learning (W3D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
<label for="toctree-checkbox-14">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_UnsupervisedAndSelfSupervisedLearning/student/W3D1_Tutorial1.html">
     Tutorial 1: Un/Self-supervised learning methods
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D2_BasicReinforcementLearning/chapter_title.html">
   Basic Reinforcement Learning (W3D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
<label for="toctree-checkbox-15">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_BasicReinforcementLearning/student/W3D2_Tutorial1.html">
     Tutorial 1: Introduction to Reinforcement Learning
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D3_ReinforcementLearningForGames/chapter_title.html">
   Reinforcement Learning For Games (W3D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
<label for="toctree-checkbox-16">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_ReinforcementLearningForGames/student/W3D3_Tutorial1.html">
     Tutorial 1: Learn to play games with RL
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D4_ContinualLearning/chapter_title.html">
   Continual Learning (W3D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
<label for="toctree-checkbox-17">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ContinualLearning/student/W3D4_Tutorial1.html">
     Tutorial 1: Introduction to Continual Learning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ContinualLearning/student/W3D4_Tutorial2.html">
     Tutorial 2: Out-of-distribution (OOD) Learning
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/AdvancedTopics.html">
   Deep Learning: Advanced Topics Wrap-up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Project Booklet
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/README.html">
   Introduction to projects
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_guidance.html">
   Daily guide for projects
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/modelingsteps/intro.html">
   Modeling Step-by-Step Guide
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
<label for="toctree-checkbox-18">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_1through2_DL.html">
     Modeling Steps 1 - 2
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_3through4_DL.html">
     Modeling Steps 3 - 4
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_5through6_DL.html">
     Modeling Steps 5 - 6
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_7through9_DL.html">
     Modeling Steps 7 - 9
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_10_DL.html">
     Modeling Steps 10
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionDataProjectDL.html">
     Example Data Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionModelingProjectDL.html">
     Example Model Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/Example_Deep_Learning_Project.html">
     Example Deep Learning Project
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/projects_overview.html">
   Project Templates
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
<label for="toctree-checkbox-19">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/ComputerVision/README.html">
     Computer Vision
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
<label for="toctree-checkbox-20">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/em_synapses.html">
       Knowledge Extraction from a Convolutional Neural Network
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/spectrogram_analysis.html">
       Music classification and generation with spectrograms
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/screws.html">
       Something Screwy - image recognition, detection, and classification of screws
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/image_alignment.html">
       Image Alignment
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/data_augmentation.html">
       Data Augmentation in image classification models
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/transfer_learning.html">
       Transfer Learning
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/ReinforcementLearning/README.html">
     Reinforcement Learning
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
<label for="toctree-checkbox-21">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/robolympics.html">
       NMA Robolympics: Controlling robots using reinforcement learning
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/lunar_lander.html">
       Performance Analysis of DQN Algorithm on the Lunar Lander task
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/human_rl.html">
       Using RL to Model Cognitive Tasks
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/README.html">
     Natural Language Processing
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
<label for="toctree-checkbox-22">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/sentiment_analysis.html">
       Twitter Sentiment Analysis
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/machine_translation.html">
       Machine Translation
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/Neuroscience/README.html">
     Neuroscience
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
<label for="toctree-checkbox-23">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/pose_estimation.html">
       Animal Pose Estimation
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/cellular_segmentation.html">
       Segmentation and Denoising
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/algonauts_videos.html">
       Load algonauts videos
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/blurry_vision.html">
       Vision with Lost Glasses: Modelling how the brain deals with noisy input
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/finetuning_fmri.html">
       Moving beyond Labels: Finetuning CNNs on BOLD response
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/neuro_seq_to_seq.html">
       Focus on what matters: inferring low-dimensional dynamics from neural recordings
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/datasets_and_models.html">
   Models and Data sets
  </a>
</li>
</ul>
</div>
</nav></div>
<div class="bd-sidebar__bottom">
<!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
</div>
</div>
<div id="rtd-footer-container"></div>
</div>
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
<div class="header-article row sticky-top noprint">
<div class="col py-1 d-flex header-article-main">
<div class="header-article__left">
<label class="headerbtn" data-placement="right" data-toggle="tooltip" for="__navigation" title="Toggle navigation">
<span class="headerbtn__icon-container">
<i class="fas fa-bars"></i>
</span>
</label>
</div>
<div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
<button aria-label="Launch interactive content" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-rocket"></i>
</button>
<div class="menu-dropdown__content">
<ul>
</ul>
</div>
</div>
<button class="headerbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="headerbtn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<div class="menu-dropdown menu-dropdown-repository-buttons">
<button aria-label="Source repositories" class="headerbtn menu-dropdown__trigger">
<i class="fab fa-github"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="https://github.com/NeuromatchAcademy/course-content-dl" title="Source repository">
<span class="headerbtn__icon-container">
<i class="fab fa-github"></i>
</span>
<span class="headerbtn__text-container">repository</span>
</a>
</li>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="https://github.com/NeuromatchAcademy/course-content-dl/issues/new?title=Issue%20on%20page%20%2Ftutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial1.html&amp;body=Your%20issue%20content%20here." title="Open an issue">
<span class="headerbtn__icon-container">
<i class="fas fa-lightbulb"></i>
</span>
<span class="headerbtn__text-container">open issue</span>
</a>
</li>
</ul>
</div>
</div>
<div class="menu-dropdown menu-dropdown-download-buttons">
<button aria-label="Download this page" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-download"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="../../../_sources/tutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial1.ipynb" title="Download source file">
<span class="headerbtn__icon-container">
<i class="fas fa-file"></i>
</span>
<span class="headerbtn__text-container">.ipynb</span>
</a>
</li>
<li>
<button class="headerbtn" data-placement="left" data-toggle="tooltip" onclick="printPdf(this)" title="Print to PDF">
<span class="headerbtn__icon-container">
<i class="fas fa-file-pdf"></i>
</span>
<span class="headerbtn__text-container">.pdf</span>
</button>
</li>
</ul>
</div>
</div>
<label class="headerbtn headerbtn-page-toc" for="__page-toc">
<span class="headerbtn__icon-container">
<i class="fas fa-list"></i>
</span>
</label>
</div>
</div>
<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
    </div>
<nav aria-label="Page" id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 1: Modeling sequencies and encoding text
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial objectives
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-slides">
     Tutorial slides
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
     Setup
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#install-dependencies">
       Install dependencies
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#there-may-be-errors-and-or-warnings-reported-during-the-installation-however-they-are-to-be-ignored">
         There may be
         <em>
          errors
         </em>
         and/or
         <em>
          warnings
         </em>
         reported during the installation. However, they are to be ignored.
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
       Figure Settings
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#load-dataset-from-nltk">
       Load Dataset from
       <code class="docutils literal notranslate">
<span class="pre">
         nltk
        </span>
</code>
</a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
       Helper functions
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-random-seed">
       Set random seed
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-device-gpu-or-cpu-execute-set-device">
       Set device (GPU or CPU). Execute
       <code class="docutils literal notranslate">
<span class="pre">
         set_device()
        </span>
</code>
</a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-sequences-markov-chains-hmms">
   Section 1: Sequences, Markov Chains &amp; HMMs
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-sequences-markov-processes">
     Video 1: Sequences &amp; Markov Processes
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#why-is-this-relevant-how-are-these-sequences-related-to-modern-recurrent-neural-networks">
     Why is this relevant? How are these sequences related to modern recurrent neural networks?
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-1-what-data-are-sequences">
     Section 1.1: What data are sequences?
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#creating-matrices-and-distinct-words">
       Creating Matrices and Distinct Words
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#populating-matrix-that-tracks-next-word">
       Populating Matrix that tracks next word
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-2-what-is-a-markov-chain-or-model">
     Section 1.2: What is a Markov Chain or Model?
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#modeling-transitions-between-states">
       Modeling transitions between states
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-for-most-likely-word">
         Function for most likely word
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-for-building-naive-chain">
         Function for building Naive Chain
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-for-weighted-choice">
         Function for weighted choice
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-for-sampling-next-word-with-weights">
         Function for sampling next word with weights
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-for-a-stochastic-chain-using-weighted-choice">
         Function for a stochastic chain using weighted choice
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#code-to-populate-matrix-of-sets-of-words">
         Code to populate matrix of sets of words
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-for-stochastic-chain-for-sets-of-words">
         Function for Stochastic Chain for sets of words
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-to-sample-next-word-in-sequence">
         Function to sample next word in sequence
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#think-1-2-how-does-changing-parameters-affect-the-generated-sentences">
       Think! 1.2: How does changing parameters affect the generated sentences?
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#student-response">
         Student Response
        </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-3-what-is-a-hidden-markov-model">
     Section 1.3: What is a Hidden Markov Model?
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-to-create-default-multinomial-hmm-model">
       Function to create Default Multinomial HMM model
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-to-create-default-multinomial-hmm-model-information-of-relative-frequencies-of-words">
       Function to create Default Multinomial HMM model information of relative frequencies of words
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-to-generate-words-given-hmm-model">
       Function to generate words given HMM model
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-exercise-1-3-transition-probabilities">
       (Bonus) Exercise 1.3: Transition probabilities
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#useful-links-for-markov-models-and-hmm">
       Useful links for Markov Models and HMM:
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-word-embeddings">
   Section 2: Word Embeddings
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-textual-dimension-reduction">
     Video 2: Textual Dimension Reduction
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-1-creating-word-embeddings">
     Section 2.1: Creating Word Embeddings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-visualizing-word-embedding">
     Section 2.2: Visualizing Word Embedding
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-3-exploring-meaning-with-word-embeddings">
     Section 2.3: Exploring meaning with word embeddings
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#download-fasttext-english-embeddings-of-dimension-100">
       Download FastText English Embeddings of Dimension 100
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#word-similarity">
       Word Similarity
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-3-semantic-measurements">
         Video 3: Semantic Measurements
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#homonym-words-dagger">
       Homonym Words
       <span class="math notranslate nohighlight">
        \(^\dagger\)
       </span>
</a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#word-analogies">
       Word Analogies
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-neural-net-with-word-embeddings">
   Section 3: Neural Net with word embeddings
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-3-1-simple-feed-forward-net">
     Coding Exercise 3.1: Simple Feed Forward Net
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#download-embeddings-and-clear-old-variables-to-clean-memory">
       Download embeddings and clear old variables to clean memory.
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#execute-this-cell">
         Execute this cell!
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#training-and-testing-functions">
       Training and Testing Functions
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#summary">
   Summary
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#airtable-submission-link">
     Airtable Submission Link
    </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class="article row">
<div class="col pl-md-3 pl-lg-5 content-container">
<!-- Table of contents that is only displayed when printing the page -->
<div class="onlyprint" id="jb-print-docs-body">
<h1>Tutorial 1: Modeling sequencies and encoding text</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 1: Modeling sequencies and encoding text
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial objectives
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-slides">
     Tutorial slides
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
     Setup
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#install-dependencies">
       Install dependencies
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#there-may-be-errors-and-or-warnings-reported-during-the-installation-however-they-are-to-be-ignored">
         There may be
         <em>
          errors
         </em>
         and/or
         <em>
          warnings
         </em>
         reported during the installation. However, they are to be ignored.
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
       Figure Settings
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#load-dataset-from-nltk">
       Load Dataset from
       <code class="docutils literal notranslate">
<span class="pre">
         nltk
        </span>
</code>
</a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
       Helper functions
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-random-seed">
       Set random seed
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-device-gpu-or-cpu-execute-set-device">
       Set device (GPU or CPU). Execute
       <code class="docutils literal notranslate">
<span class="pre">
         set_device()
        </span>
</code>
</a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-sequences-markov-chains-hmms">
   Section 1: Sequences, Markov Chains &amp; HMMs
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-sequences-markov-processes">
     Video 1: Sequences &amp; Markov Processes
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#why-is-this-relevant-how-are-these-sequences-related-to-modern-recurrent-neural-networks">
     Why is this relevant? How are these sequences related to modern recurrent neural networks?
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-1-what-data-are-sequences">
     Section 1.1: What data are sequences?
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#creating-matrices-and-distinct-words">
       Creating Matrices and Distinct Words
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#populating-matrix-that-tracks-next-word">
       Populating Matrix that tracks next word
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-2-what-is-a-markov-chain-or-model">
     Section 1.2: What is a Markov Chain or Model?
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#modeling-transitions-between-states">
       Modeling transitions between states
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-for-most-likely-word">
         Function for most likely word
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-for-building-naive-chain">
         Function for building Naive Chain
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-for-weighted-choice">
         Function for weighted choice
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-for-sampling-next-word-with-weights">
         Function for sampling next word with weights
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-for-a-stochastic-chain-using-weighted-choice">
         Function for a stochastic chain using weighted choice
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#code-to-populate-matrix-of-sets-of-words">
         Code to populate matrix of sets of words
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-for-stochastic-chain-for-sets-of-words">
         Function for Stochastic Chain for sets of words
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-to-sample-next-word-in-sequence">
         Function to sample next word in sequence
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#think-1-2-how-does-changing-parameters-affect-the-generated-sentences">
       Think! 1.2: How does changing parameters affect the generated sentences?
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#student-response">
         Student Response
        </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-3-what-is-a-hidden-markov-model">
     Section 1.3: What is a Hidden Markov Model?
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-to-create-default-multinomial-hmm-model">
       Function to create Default Multinomial HMM model
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-to-create-default-multinomial-hmm-model-information-of-relative-frequencies-of-words">
       Function to create Default Multinomial HMM model information of relative frequencies of words
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-to-generate-words-given-hmm-model">
       Function to generate words given HMM model
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-exercise-1-3-transition-probabilities">
       (Bonus) Exercise 1.3: Transition probabilities
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#useful-links-for-markov-models-and-hmm">
       Useful links for Markov Models and HMM:
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-word-embeddings">
   Section 2: Word Embeddings
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-textual-dimension-reduction">
     Video 2: Textual Dimension Reduction
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-1-creating-word-embeddings">
     Section 2.1: Creating Word Embeddings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-visualizing-word-embedding">
     Section 2.2: Visualizing Word Embedding
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-3-exploring-meaning-with-word-embeddings">
     Section 2.3: Exploring meaning with word embeddings
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#download-fasttext-english-embeddings-of-dimension-100">
       Download FastText English Embeddings of Dimension 100
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#word-similarity">
       Word Similarity
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-3-semantic-measurements">
         Video 3: Semantic Measurements
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#homonym-words-dagger">
       Homonym Words
       <span class="math notranslate nohighlight">
        \(^\dagger\)
       </span>
</a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#word-analogies">
       Word Analogies
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-neural-net-with-word-embeddings">
   Section 3: Neural Net with word embeddings
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-3-1-simple-feed-forward-net">
     Coding Exercise 3.1: Simple Feed Forward Net
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#download-embeddings-and-clear-old-variables-to-clean-memory">
       Download embeddings and clear old variables to clean memory.
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#execute-this-cell">
         Execute this cell!
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#training-and-testing-functions">
       Training and Testing Functions
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#summary">
   Summary
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#airtable-submission-link">
     Airtable Submission Link
    </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
</div>
<main id="main-content" role="main">
<div>
<p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial1.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a>   <a href="https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial1.ipynb" target="_blank"><img alt="Open in Kaggle" src="https://kaggle.com/static/images/open-in-kaggle.svg"/></a></p>
<div class="section" id="tutorial-1-modeling-sequencies-and-encoding-text">
<h1>Tutorial 1: Modeling sequencies and encoding text<a class="headerlink" href="#tutorial-1-modeling-sequencies-and-encoding-text" title="Permalink to this headline">¶</a></h1>
<p><strong>Week 2, Day 3: Modern RNNs</strong></p>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators:</strong> Bhargav Srinivasa Desikan, Anis Zahedifard, James Evans</p>
<p><strong>Content reviewers:</strong> Lily Cheng, Melvin Selim Atay, Ezekiel Williams, Kelson Shilling-Scrivo</p>
<p><strong>Content editors:</strong> Nina Kudryashova, Spiros Chavlis</p>
<p><strong>Production editors:</strong> Roberto Guidotti, Spiros Chavlis</p>
<p><strong>Post-Production team:</strong> Gagana B, Spiros Chavlis</p>
<p><strong>Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs</strong></p>
<p align="center"><img src="https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True"/></p></div>
<hr class="docutils"/>
<div class="section" id="tutorial-objectives">
<h1>Tutorial objectives<a class="headerlink" href="#tutorial-objectives" title="Permalink to this headline">¶</a></h1>
<p>Before we begin with exploring how RNNs excel at modelling sequences, we will explore some of the other ways we can model sequences, encode text, and make meaningful measurements using such encodings and embeddings.</p>
<div class="section" id="tutorial-slides">
<h2>Tutorial slides<a class="headerlink" href="#tutorial-slides" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<iframe allowfullscreen="" frameborder="0" height="480" src="https://mfr.ca-1.osf.io/render?url=https://osf.io/n263c/?direct%26mode=render%26action=download%26mode=render" width="854"></iframe>
</div></div>
</div>
<p>These are the slides for the videos in this tutorial. If you want to locally download the slides, click <a class="reference external" href="https://osf.io/n263c/download">here</a>.</p>
</div>
<hr class="docutils"/>
<div class="section" id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h2>
<div class="section" id="install-dependencies">
<h3>Install dependencies<a class="headerlink" href="#install-dependencies" title="Permalink to this headline">¶</a></h3>
<div class="section" id="there-may-be-errors-and-or-warnings-reported-during-the-installation-however-they-are-to-be-ignored">
<h4>There may be <em>errors</em> and/or <em>warnings</em> reported during the installation. However, they are to be ignored.<a class="headerlink" href="#there-may-be-errors-and-or-warnings-reported-during-the-installation-however-they-are-to-be-ignored" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Install dependencies</span>

<span class="c1"># @markdown #### There may be *errors* and/or *warnings* reported during the installation. However, they are to be ignored.</span>
<span class="o">!</span>pip install torchtext --quiet
<span class="o">!</span>pip install --upgrade gensim --quiet
<span class="o">!</span>pip install unidecode --quiet
<span class="o">!</span>pip install hmmlearn --quiet
<span class="o">!</span>pip install fasttext --quiet
<span class="o">!</span>pip install nltk --quiet
<span class="o">!</span>pip install pandas --quiet
<span class="o">!</span>pip install python-Levenshtein --quiet

<span class="o">!</span>pip install git+https://github.com/NeuromatchAcademy/evaltools --quiet
<span class="kn">from</span> <span class="nn">evaltools.airtable</span> <span class="kn">import</span> <span class="n">AirtableForm</span>

<span class="c1"># Generate airtable form</span>
<span class="n">atform</span> <span class="o">=</span> <span class="n">AirtableForm</span><span class="p">(</span><span class="s1">'appn7VdPRseSoMXEG'</span><span class="p">,</span><span class="s1">'W2D3_T1'</span><span class="p">,</span><span class="s1">'https://portal.neuromatchacademy.org/api/redirect/to/9c55f6cb-cdf9-4429-ac1c-ec44fe64c303'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Yellow">WARNING: You are using pip version 22.0.4; however, version 22.1 is available.</span>
<span class="-Color -Color-Yellow">You should consider upgrading via the '/opt/hostedtoolcache/Python/3.7.13/x64/bin/python -m pip install --upgrade pip' command.</span>

</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Yellow">WARNING: You are using pip version 22.0.4; however, version 22.1 is available.</span>
<span class="-Color -Color-Yellow">You should consider upgrading via the '/opt/hostedtoolcache/Python/3.7.13/x64/bin/python -m pip install --upgrade pip' command.</span>

</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Yellow">WARNING: You are using pip version 22.0.4; however, version 22.1 is available.</span>
<span class="-Color -Color-Yellow">You should consider upgrading via the '/opt/hostedtoolcache/Python/3.7.13/x64/bin/python -m pip install --upgrade pip' command.</span>

</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Yellow">WARNING: You are using pip version 22.0.4; however, version 22.1 is available.</span>
<span class="-Color -Color-Yellow">You should consider upgrading via the '/opt/hostedtoolcache/Python/3.7.13/x64/bin/python -m pip install --upgrade pip' command.</span>

</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Yellow">WARNING: You are using pip version 22.0.4; however, version 22.1 is available.</span>
<span class="-Color -Color-Yellow">You should consider upgrading via the '/opt/hostedtoolcache/Python/3.7.13/x64/bin/python -m pip install --upgrade pip' command.</span>

</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Yellow">WARNING: You are using pip version 22.0.4; however, version 22.1 is available.</span>
<span class="-Color -Color-Yellow">You should consider upgrading via the '/opt/hostedtoolcache/Python/3.7.13/x64/bin/python -m pip install --upgrade pip' command.</span>

</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Yellow">WARNING: You are using pip version 22.0.4; however, version 22.1 is available.</span>
<span class="-Color -Color-Yellow">You should consider upgrading via the '/opt/hostedtoolcache/Python/3.7.13/x64/bin/python -m pip install --upgrade pip' command.</span>

</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Yellow">WARNING: You are using pip version 22.0.4; however, version 22.1 is available.</span>
<span class="-Color -Color-Yellow">You should consider upgrading via the '/opt/hostedtoolcache/Python/3.7.13/x64/bin/python -m pip install --upgrade pip' command.</span>

</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Yellow">WARNING: You are using pip version 22.0.4; however, version 22.1 is available.</span>
<span class="-Color -Color-Yellow">You should consider upgrading via the '/opt/hostedtoolcache/Python/3.7.13/x64/bin/python -m pip install --upgrade pip' command.</span>

</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">fasttext</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.cm</span> <span class="k">as</span> <span class="nn">cm</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="kn">from</span> <span class="nn">hmmlearn</span> <span class="kn">import</span> <span class="n">hmm</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">dok_matrix</span>

<span class="kn">from</span> <span class="nn">torchtext.legacy</span> <span class="kn">import</span> <span class="n">data</span><span class="p">,</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">torchtext.vocab</span> <span class="kn">import</span> <span class="n">FastText</span>

<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">FreqDist</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">brown</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>

<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>

<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>

<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm_notebook</span> <span class="k">as</span> <span class="n">tqdm</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_72706</span><span class="o">/</span><span class="mf">600144970.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span> <span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">dok_matrix</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span> 
<span class="ne">---&gt; </span><span class="mi">16</span> <span class="kn">from</span> <span class="nn">torchtext.legacy</span> <span class="kn">import</span> <span class="n">data</span><span class="p">,</span> <span class="n">datasets</span>
<span class="g g-Whitespace">     </span><span class="mi">17</span> <span class="kn">from</span> <span class="nn">torchtext.vocab</span> <span class="kn">import</span> <span class="n">FastText</span>
<span class="g g-Whitespace">     </span><span class="mi">18</span> 

<span class="ne">ModuleNotFoundError</span>: No module named 'torchtext.legacy'
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="figure-settings">
<h3>Figure Settings<a class="headerlink" href="#figure-settings" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Figure Settings</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="load-dataset-from-nltk">
<h3>Load Dataset from <code class="docutils literal notranslate"><span class="pre">nltk</span></code><a class="headerlink" href="#load-dataset-from-nltk" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title  Load Dataset from `nltk`</span>
<span class="c1"># No critical warnings, so we suppress it</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">"ignore"</span><span class="p">)</span>

<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">'punkt'</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">'averaged_perceptron_tagger'</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">'brown'</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">'webtext'</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
</div>
<div class="section" id="helper-functions">
<h3>Helper functions<a class="headerlink" href="#helper-functions" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Helper functions</span>

<span class="kn">import</span> <span class="nn">requests</span>

<span class="k">def</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">vec_a</span><span class="p">,</span> <span class="n">vec_b</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Compute cosine similarity between vec_a and vec_b</span>

<span class="sd">    Args:</span>
<span class="sd">      vec_a: np.ndarray</span>
<span class="sd">        Vector #1</span>
<span class="sd">      vec_b: np.ndarray</span>
<span class="sd">        Vector #2</span>

<span class="sd">    Returns:</span>
<span class="sd">      Cosine similarity between vec_a and vec_b</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vec_a</span><span class="p">,</span> <span class="n">vec_b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">vec_a</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">vec_b</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">sentences</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Tokenize the sentence</span>
<span class="sd">  ~ from nltk.tokenize library use word_tokenize</span>

<span class="sd">  Args:</span>
<span class="sd">    sentences: string</span>
<span class="sd">      Sentence to be tokenized</span>

<span class="sd">  Returns:</span>
<span class="sd">    token: list</span>
<span class="sd">      List of tokens generated from sentences</span>
<span class="sd">  """</span>
  <span class="n">token</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">token</span>


<span class="k">def</span> <span class="nf">plot_train_val</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">train_label</span><span class="p">,</span> <span class="n">val_label</span><span class="p">,</span>
                   <span class="n">title</span><span class="p">,</span> <span class="n">y_label</span><span class="p">,</span> <span class="n">color</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Plots training/validation performance per epoch</span>

<span class="sd">  Args:</span>
<span class="sd">    x: np.ndarray</span>
<span class="sd">      Input data</span>
<span class="sd">    train: list</span>
<span class="sd">      Training data performance</span>
<span class="sd">    val: list</span>
<span class="sd">      Validation data performance</span>
<span class="sd">    train_label: string</span>
<span class="sd">      Train Label [specifies training criterion]</span>
<span class="sd">    color: string</span>
<span class="sd">      Specifies color of plot</span>
<span class="sd">    val_label: string</span>
<span class="sd">      Validation Label [specifies validation criterion]</span>
<span class="sd">    title: string</span>
<span class="sd">      Specifies title of plot</span>
<span class="sd">    y_label: string</span>
<span class="sd">      Specifies performance criterion</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">train_label</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">val_label</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'lower right'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Epoch'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">y_label</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="n">emb_vectors</span><span class="p">,</span> <span class="n">sentence_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">2021</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Load dataset</span>

<span class="sd">  Args:</span>
<span class="sd">    sentence_length: int</span>
<span class="sd">      Length of sentence</span>
<span class="sd">    seed: int</span>
<span class="sd">      Set seed for reproducibility</span>
<span class="sd">    emb_vectors: FastText type</span>
<span class="sd">      Embedding vectors of size 111051</span>

<span class="sd">  Returns:</span>
<span class="sd">    TEXT: Field instance</span>
<span class="sd">      Text</span>
<span class="sd">    vocab_size: int</span>
<span class="sd">      Specifies size of TEXT</span>
<span class="sd">    train_iter: BucketIterator</span>
<span class="sd">      Training iterator</span>
<span class="sd">    valid_iter: BucketIterator</span>
<span class="sd">      Validation iterator</span>
<span class="sd">    test_iter: BucketIterator</span>
<span class="sd">      Test iterator</span>
<span class="sd">  """</span>
  <span class="n">TEXT</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">sequential</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">tokenize</span><span class="o">=</span><span class="n">tokenize</span><span class="p">,</span>
                    <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">include_lengths</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">fix_length</span><span class="o">=</span><span class="n">sentence_length</span><span class="p">)</span>
  <span class="n">LABEL</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">LabelField</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

  <span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">IMDB</span><span class="o">.</span><span class="n">splits</span><span class="p">(</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">LABEL</span><span class="p">)</span>

  <span class="n">TEXT</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">vectors</span><span class="o">=</span><span class="n">emb_vectors</span><span class="p">)</span>
  <span class="n">LABEL</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>

  <span class="n">train_data</span><span class="p">,</span> <span class="n">valid_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">split_ratio</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
                                            <span class="n">random_state</span><span class="o">=</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">))</span>
  <span class="n">train_iter</span><span class="p">,</span> <span class="n">valid_iter</span><span class="p">,</span> <span class="n">test_iter</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">BucketIterator</span><span class="o">.</span><span class="n">splits</span><span class="p">((</span><span class="n">train_data</span><span class="p">,</span>
                                                                  <span class="n">valid_data</span><span class="p">,</span>
                                                                  <span class="n">test_data</span><span class="p">),</span>
                                                                  <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                                                                  <span class="n">sort_key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">text</span><span class="p">),</span>
                                                                  <span class="n">repeat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                                  <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Data are loaded. sentence length: </span><span class="si">{</span><span class="n">sentence_length</span><span class="si">}</span><span class="s1"> '</span>
        <span class="sa">f</span><span class="s1">'seed: </span><span class="si">{</span><span class="n">seed</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">TEXT</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">valid_iter</span><span class="p">,</span> <span class="n">test_iter</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="set-random-seed">
<h3>Set random seed<a class="headerlink" href="#set-random-seed" title="Permalink to this headline">¶</a></h3>
<p>Executing <code class="docutils literal notranslate"><span class="pre">set_seed(seed=seed)</span></code> you are setting the seed</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set random seed</span>

<span class="c1"># @markdown Executing `set_seed(seed=seed)` you are setting the seed</span>

<span class="c1"># For DL its critical to set the random seed so that students can have a</span>
<span class="c1"># baseline to compare their results to expected results.</span>
<span class="c1"># Read more here: https://pytorch.org/docs/stable/notes/randomness.html</span>

<span class="c1"># Call `set_seed` function in the exercises to ensure reproducibility.</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Function that controls randomness.</span>
<span class="sd">  NumPy and random modules must be imported.</span>

<span class="sd">  Args:</span>
<span class="sd">    seed : Integer</span>
<span class="sd">      A non-negative integer that defines the random state. Default is `None`.</span>
<span class="sd">    seed_torch : Boolean</span>
<span class="sd">      If `True` sets the random seed for pytorch tensors, so pytorch module</span>
<span class="sd">      must be imported. Default is `True`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>
  <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">32</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">seed_torch</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Random seed </span><span class="si">{</span><span class="n">seed</span><span class="si">}</span><span class="s1"> has been set.'</span><span class="p">)</span>

<span class="c1"># In case that `DataLoader` is used</span>
<span class="k">def</span> <span class="nf">seed_worker</span><span class="p">(</span><span class="n">worker_id</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  DataLoader will reseed workers following randomness in</span>
<span class="sd">  multi-process data loading algorithm.</span>

<span class="sd">  Args:</span>
<span class="sd">    worker_id: integer</span>
<span class="sd">      ID of subprocess to seed. 0 means that</span>
<span class="sd">      the data will be loaded in the main process</span>
<span class="sd">      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">worker_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">initial_seed</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="set-device-gpu-or-cpu-execute-set-device">
<h3>Set device (GPU or CPU). Execute <code class="docutils literal notranslate"><span class="pre">set_device()</span></code><a class="headerlink" href="#set-device-gpu-or-cpu-execute-set-device" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set device (GPU or CPU). Execute `set_device()`</span>

<span class="c1"># Inform the user if the notebook uses GPU or CPU.</span>

<span class="k">def</span> <span class="nf">set_device</span><span class="p">():</span>
  <span class="sd">"""</span>
<span class="sd">  Set the device. CUDA if available, CPU otherwise</span>

<span class="sd">  Args:</span>
<span class="sd">    None</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>
  <span class="k">if</span> <span class="n">device</span> <span class="o">!=</span> <span class="s2">"cuda"</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"WARNING: For this notebook to perform best, "</span>
        <span class="s2">"if possible, in the menu under `Runtime` -&gt; "</span>
        <span class="s2">"`Change runtime type.`  select `GPU` "</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"GPU is enabled in this notebook."</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">device</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">DEVICE</span> <span class="o">=</span> <span class="n">set_device</span><span class="p">()</span>
<span class="n">SEED</span> <span class="o">=</span> <span class="mi">2021</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -&gt; `Change runtime type.`  select `GPU` 
Random seed 2021 has been set.
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-1-sequences-markov-chains-hmms">
<h1>Section 1: Sequences, Markov Chains &amp; HMMs<a class="headerlink" href="#section-1-sequences-markov-chains-hmms" title="Permalink to this headline">¶</a></h1>
<p><em>Time estimate: ~45mins</em></p>
<div class="section" id="video-1-sequences-markov-processes">
<h2>Video 1: Sequences &amp; Markov Processes<a class="headerlink" href="#video-1-sequences-markov-processes" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "4f1e6b2e45934affa1f1ae1767967a6e"}
</script></div>
</div>
<p>In this notebook we will be exploring the world of sequences - thinking of what kind of data can be thought of as sequences, and how these sequences can be represented as Markov Chains and Hidden Markov Models. These ideas and methods were an important part of natural language processing and language modelling, and serve as a useful way to ground ourselves before we dive into neural network methods.</p>
</div>
<div class="section" id="why-is-this-relevant-how-are-these-sequences-related-to-modern-recurrent-neural-networks">
<h2>Why is this relevant? How are these sequences related to modern recurrent neural networks?<a class="headerlink" href="#why-is-this-relevant-how-are-these-sequences-related-to-modern-recurrent-neural-networks" title="Permalink to this headline">¶</a></h2>
<p>Like we mentioned before, the notion of modelling sequences of data - in this particular case, <strong>language</strong>, is an ideal place to start. RNNs themselves were constructed keeping in mind sequences, and the ability to temporally model sequences is what inspired RNNs (and the family of LSTM, GRUs - we will see this in the next notebook).</p>
<p>Markov models and hidden markov models serve as an introduction to these concepts because they were some of the earliest ways to think about sequences. They do not capture a lot of the complexity that RNNs excel at, but are an useful way of thinking of sequences, probabilities, and how we can use these concepts to perform  tasks such as text generation, or classification - tasks that RNNs excel at today.</p>
<p>Think of this section as an introduction to thinking with sequences and text data, and as a historical introduction to the world of modelling sequential data.</p>
</div>
<div class="section" id="section-1-1-what-data-are-sequences">
<h2>Section 1.1: What data are sequences?<a class="headerlink" href="#section-1-1-what-data-are-sequences" title="Permalink to this headline">¶</a></h2>
<p>Native Sequences:</p>
<ul class="simple">
<li><p>Temporally occurring events (e.g., history, stock prices)</p></li>
<li><p>Temporally processed events (e.g., communication)</p></li>
<li><p>Topologically connected components (e.g., polymers, peptides)</p></li>
</ul>
<p>Synthetic Sequences:</p>
<ul class="simple">
<li><p>Anything processed as a sequence (e.g., scanned pixels in an image)</p></li>
</ul>
<p>Sequences can be represented as a Markov Process - since this notion of sequential data is intrinsically linked to RNNs, it is a good place for us to start, and natural language (text!) will be our sequence of choice.</p>
<p>We will be using the Brown corpus which comes loaded with NLTK, and using the entire corpus - this requires a lot of RAM for some of the methods, so we recommend using a smaller subset of categories if you do not have enough RAM.</p>
<p>We will be using some of the code from this <a class="reference external" href="https://www.kdnuggets.com/2019/11/markov-chains-train-text-generation.html">tutorial</a> and this <a class="reference external" href="https://github.com/StrikingLoo/ASOIAF-Markov/blob/master/ASOIAF.ipynb">Jupyter notebook</a></p>
<p>The first few cells of code all involve set-up; some of this code will be hidden because they are not necessary to understand the ideas of markov models, but the way data is setup can be vital to the way the model performs (something in common with neural network models!).</p>
<p>Let us start with loading our corpus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">category</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'editorial'</span><span class="p">,</span> <span class="s1">'fiction'</span><span class="p">,</span> <span class="s1">'government'</span><span class="p">,</span> <span class="s1">'news'</span><span class="p">,</span> <span class="s1">'religion'</span><span class="p">]</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="n">brown</span><span class="o">.</span><span class="n">sents</span><span class="p">(</span><span class="n">categories</span><span class="o">=</span><span class="n">category</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<p>Now that we have our sentences, let us look at some statistics to get an idea of what we are dealing with.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>
<span class="n">lengths</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<p>Find the 80-th percentile: the minimal length of such a sentence, which is longer than at least 80% of sentences in the <em>Brown corpus</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lengths</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">.8</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lengths</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>

</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</div>

</div>
<p>This gives us an idea of what our dataset looks like, along with some average lengths. This kind of quick data exploration can be very useful - we know how long different sequences are, and how we might want to collect these words.</p>
<p>Since we will be modeling words as sequences in sentences, let us first collect all the words in our corpus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus_words</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">:</span>
    <span class="k">if</span> <span class="s2">"''"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">word</span> <span class="ow">and</span> <span class="s2">"``"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">word</span><span class="p">:</span>
      <span class="n">corpus_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Corpus length: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">corpus_words</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Corpus length: 0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus_words</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[]
</pre></div>
</div>
</div>
</div>
<p>We’ll now get distinct (unique) words and create a matrix to represent all these words. This is necessary because we will be using this matrix to look at the probability of the words in sequences.</p>
<div class="section" id="creating-matrices-and-distinct-words">
<h3>Creating Matrices and Distinct Words<a class="headerlink" href="#creating-matrices-and-distinct-words" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Creating Matrices and Distinct Words</span>
<span class="n">distinct_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">corpus_words</span><span class="p">))</span>
<span class="n">word_idx_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">distinct_words</span><span class="p">)}</span>
<span class="n">distinct_words_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">corpus_words</span><span class="p">)))</span>
<span class="n">next_word_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">distinct_words_count</span><span class="p">,</span> <span class="n">distinct_words_count</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Number of distinct words: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">distinct_words_count</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of distinct words: 0
</pre></div>
</div>
</div>
</div>
<p>In the following lines of code we are populating the matrix that tracks the next word in a sentence.</p>
</div>
<div class="section" id="populating-matrix-that-tracks-next-word">
<h3>Populating Matrix that tracks next word<a class="headerlink" href="#populating-matrix-that-tracks-next-word" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Populating Matrix that tracks next word</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">corpus_words</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
  <span class="n">first_word_idx</span> <span class="o">=</span> <span class="n">word_idx_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
  <span class="n">next_word_idx</span> <span class="o">=</span> <span class="n">word_idx_dict</span><span class="p">[</span><span class="n">corpus_words</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]]</span>
  <span class="n">next_word_matrix</span><span class="p">[</span><span class="n">first_word_idx</span><span class="p">][</span><span class="n">next_word_idx</span><span class="p">]</span> <span class="o">+=</span><span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<p>Now we have the information ready to construct a markov chain. The next word matrix is crucial in this, as it allows us to go from one word in the sequence to the next. We will soon see how this is used.</p>
</div>
</div>
<div class="section" id="section-1-2-what-is-a-markov-chain-or-model">
<h2>Section 1.2: What is a Markov Chain or Model?<a class="headerlink" href="#section-1-2-what-is-a-markov-chain-or-model" title="Permalink to this headline">¶</a></h2>
<p>A Markov Chain (or Model) is a:</p>
<ul class="simple">
<li><p>stochastic model describing a sequence of possible events</p></li>
<li><p>the probability of each event depends only on the state attained in the previous event.</p></li>
<li><p>a countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC) [vs. a continuous-time process or CTMC].</p></li>
<li><p>The classic formal language model is a Markov Model</p></li>
</ul>
<p><em>Helpful explanations from <a class="reference external" href="https://ericmjl.github.io/essays-on-data-science/machine-learning/markov-models/#non-autoregressive-homoskedastic-emissions">eric mjl’s tutorial</a></em>!</p>
<p>The simplest Markov models assume that we have a <em>system</em> that contains a finite set of states,
and that the <em>system</em> transitions between these states with some probability at each time step <span class="math notranslate nohighlight">\(t\)</span>,
thus generating a sequence of states over time.
Let’s call these states <span class="math notranslate nohighlight">\(S\)</span>, where</p>
<div class="amsmath math notranslate nohighlight" id="equation-ff53fd2f-e385-4c9d-954a-e911e927551f">
<span class="eqno">(71)<a class="headerlink" href="#equation-ff53fd2f-e385-4c9d-954a-e911e927551f" title="Permalink to this equation">¶</a></span>\[\begin{equation}
S = \{s_1, s_2, ..., s_n\}
\end{equation}\]</div>
<p>To keep things simple, let’s start with three states:</p>
<div class="amsmath math notranslate nohighlight" id="equation-6e872e7d-1f8e-4b94-9523-0e1e7aae023c">
<span class="eqno">(72)<a class="headerlink" href="#equation-6e872e7d-1f8e-4b94-9523-0e1e7aae023c" title="Permalink to this equation">¶</a></span>\[\begin{equation}
S = \{s_1, s_2, s_3\}
\end{equation}\]</div>
<p>A Markov model generates a sequence of states, with one possible realization being:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ef9a207e-31e9-4475-ba48-655e2802e7b8">
<span class="eqno">(73)<a class="headerlink" href="#equation-ef9a207e-31e9-4475-ba48-655e2802e7b8" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\{s_1, s_1, s_1, s_3, s_3, s_3, s_2, s_2, s_3, s_3, s_3, s_3, s_1, ...\}
\end{equation}\]</div>
<p>And generically, we represent it as a sequence of states <span class="math notranslate nohighlight">\(x_t, x_{t+1}... x_{t+n}\)</span>.(We have chosen a different symbol to not confuse the “generic” state with the specific realization.) Graphically, a plain and simple Markov model looks like the following:</p>
<center><img src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/static/cell_chain.png" width="500"/></center><div class="section" id="modeling-transitions-between-states">
<h3>Modeling transitions between states<a class="headerlink" href="#modeling-transitions-between-states" title="Permalink to this headline">¶</a></h3>
<p>To know how a system transitions between states, we now need a <strong>transition matrix</strong>.</p>
<p>The transition matrix describes the probability of transitioning from one state to another (The probability of staying in the same state is semantically equivalent to transitioning to the same state).</p>
<p>By convention, transition matrix rows correspond to the state at time <span class="math notranslate nohighlight">\(t\)</span>,
while columns correspond to state at time <span class="math notranslate nohighlight">\(t+1\)</span>.
Hence, row probabilities sum to one, because the probability of transitioning to the next state depends on only the current state, and all possible states are known and enumerated.</p>
<p>Let’s call the transition matrix <span class="math notranslate nohighlight">\(P_{transition}\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-0bd7f78c-4aa0-436a-9abe-eda3e7494cdd">
<span class="eqno">(74)<a class="headerlink" href="#equation-0bd7f78c-4aa0-436a-9abe-eda3e7494cdd" title="Permalink to this equation">¶</a></span>\[\begin{equation}
P_{transition} = 
  \begin{pmatrix}
  p_{11} &amp; p_{12} &amp; p_{13} \\
  p_{21} &amp; p_{22} &amp; p_{23} \\
  p_{31} &amp; p_{32} &amp; p_{33} \\
  \end{pmatrix}
\end{equation}\]</div>
<p>Using the transition matrix, we can express different behaviors of the system. For example:</p>
<ol class="simple">
<li><p>by assigning larger probability mass to the diagonals, we can express that the system likes to stay in the current state;</p></li>
<li><p>by assigning larger probability mass to the off-diagonal, we can express that the system likes to transition out of its current state.</p></li>
</ol>
<p>In our case, this matrix is created by measuring how often one word appeared after another.</p>
<div class="section" id="function-for-most-likely-word">
<h4>Function for most likely word<a class="headerlink" href="#function-for-most-likely-word" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function for most likely word</span>
<span class="k">def</span> <span class="nf">most_likely_word_after</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
  <span class="c1"># We check for the word most likely</span>
  <span class="c1"># to occur using the matrix</span>
  <span class="n">most_likely</span> <span class="o">=</span> <span class="n">next_word_matrix</span><span class="p">[</span><span class="n">word_idx_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">distinct_words</span><span class="p">[</span><span class="n">most_likely</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Using our most likely word function, we can begin to create chains of words and create sequences. In the code below we create a naive chain that simply choses the most likely word.</p>
</div>
<div class="section" id="function-for-building-naive-chain">
<h4>Function for building Naive Chain<a class="headerlink" href="#function-for-building-naive-chain" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function for building Naive Chain</span>
<span class="k">def</span> <span class="nf">naive_chain</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">15</span><span class="p">):</span>
  <span class="n">current_word</span> <span class="o">=</span> <span class="n">word</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">word</span>
  <span class="c1"># We now build a naive chain by</span>
  <span class="c1"># picking up the most likely word</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">length</span><span class="p">):</span>
    <span class="n">sentence</span> <span class="o">+=</span> <span class="s1">' '</span>
    <span class="n">next_word</span> <span class="o">=</span> <span class="n">most_likely_word_after</span><span class="p">(</span><span class="n">current_word</span><span class="p">)</span>
    <span class="n">sentence</span> <span class="o">+=</span> <span class="n">next_word</span>
    <span class="n">current_word</span> <span class="o">=</span> <span class="n">next_word</span>
  <span class="k">return</span> <span class="n">sentence</span>
</pre></div>
</div>
</div>
</div>
<p>Let us now use this naive chain to see what comes up, using some simple words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">naive_chain</span><span class="p">(</span><span class="s1">'the'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">naive_chain</span><span class="p">(</span><span class="s1">'I'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">naive_chain</span><span class="p">(</span><span class="s1">'What'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">naive_chain</span><span class="p">(</span><span class="s1">'park'</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyError</span><span class="g g-Whitespace">                                  </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_72706</span><span class="o">/</span><span class="mf">2479173780.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="nb">print</span><span class="p">(</span><span class="n">naive_chain</span><span class="p">(</span><span class="s1">'the'</span><span class="p">))</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="nb">print</span><span class="p">(</span><span class="n">naive_chain</span><span class="p">(</span><span class="s1">'I'</span><span class="p">))</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="nb">print</span><span class="p">(</span><span class="n">naive_chain</span><span class="p">(</span><span class="s1">'What'</span><span class="p">))</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="nb">print</span><span class="p">(</span><span class="n">naive_chain</span><span class="p">(</span><span class="s1">'park'</span><span class="p">))</span>

<span class="nn">/tmp/ipykernel_72706/3521693624.py</span> in <span class="ni">naive_chain</span><span class="nt">(word, length)</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span>   <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">length</span><span class="p">):</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span>     <span class="n">sentence</span> <span class="o">+=</span> <span class="s1">' '</span>
<span class="ne">----&gt; </span><span class="mi">9</span>     <span class="n">next_word</span> <span class="o">=</span> <span class="n">most_likely_word_after</span><span class="p">(</span><span class="n">current_word</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">10</span>     <span class="n">sentence</span> <span class="o">+=</span> <span class="n">next_word</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span>     <span class="n">current_word</span> <span class="o">=</span> <span class="n">next_word</span>

<span class="nn">/tmp/ipykernel_72706/1366005415.py</span> in <span class="ni">most_likely_word_after</span><span class="nt">(word)</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span>   <span class="c1"># We check for the word most likely</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span>   <span class="c1"># to occur using the matrix</span>
<span class="ne">----&gt; </span><span class="mi">5</span>   <span class="n">most_likely</span> <span class="o">=</span> <span class="n">next_word_matrix</span><span class="p">[</span><span class="n">word_idx_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span>   <span class="k">return</span> <span class="n">distinct_words</span><span class="p">[</span><span class="n">most_likely</span><span class="p">]</span>

<span class="ne">KeyError</span>: 'the'
</pre></div>
</div>
</div>
</div>
<p>We notice that after the word <code class="docutils literal notranslate"><span class="pre">the</span></code>, <code class="docutils literal notranslate"><span class="pre">United</span> <span class="pre">States</span></code> comes up each time. All the other sequencies starting from other words also end up at <code class="docutils literal notranslate"><span class="pre">the</span></code> quite often. Since we use a <em>deterministic</em> markov chain model, its next state only depends on the previous one. Therefore, once the sequence comes to <code class="docutils literal notranslate"><span class="pre">the</span></code>, it inevitably continues the sequence with the <code class="docutils literal notranslate"><span class="pre">United</span> <span class="pre">States</span></code>.</p>
<p>We can now be a little more sophisticated, and return words in a sequence using a <em>weighted choice</em>, which randomly selects the next word from a set of words with some probability (weight).</p>
</div>
<div class="section" id="function-for-weighted-choice">
<h4>Function for weighted choice<a class="headerlink" href="#function-for-weighted-choice" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function for weighted choice</span>
<span class="k">def</span> <span class="nf">weighted_choice</span><span class="p">(</span><span class="n">objects</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Returns a random element from the sequence of 'objects',</span>
<span class="sd">  the likelihood of the objects is weighted according</span>
<span class="sd">  to the sequence of 'weights', i.e. percentages.</span>

<span class="sd">  Args:</span>
<span class="sd">    objects: list</span>
<span class="sd">      Sequence of objects</span>
<span class="sd">    weights: list</span>
<span class="sd">      Sequence of weights per object</span>

<span class="sd">  Returns:</span>
<span class="sd">    Random element from the sequence of 'objects',</span>
<span class="sd">    the likelihood of the objects is weighted according</span>
<span class="sd">    to the sequence of 'weights', i.e. percentages.</span>
<span class="sd">  """</span>

  <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
  <span class="n">sum_of_weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
  <span class="c1"># Standardization:</span>
  <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">sum_of_weights</span><span class="p">)</span>
  <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
      <span class="k">return</span> <span class="n">objects</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="function-for-sampling-next-word-with-weights">
<h4>Function for sampling next word with weights<a class="headerlink" href="#function-for-sampling-next-word-with-weights" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function for sampling next word with weights</span>
<span class="k">def</span> <span class="nf">sample_next_word_after</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Helper function to sample next word with weights based on present word</span>

<span class="sd">  Args:</span>
<span class="sd">    word: string</span>
<span class="sd">      Current word</span>
<span class="sd">    alpha: int</span>
<span class="sd">      Offset</span>

<span class="sd">  Returns:</span>
<span class="sd">    Next word</span>
<span class="sd">  """</span>
  <span class="n">next_word_vector</span> <span class="o">=</span> <span class="n">next_word_matrix</span><span class="p">[</span><span class="n">word_idx_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span> <span class="o">+</span> <span class="n">alpha</span>
  <span class="n">likelihoods</span> <span class="o">=</span> <span class="n">next_word_vector</span><span class="o">/</span><span class="n">next_word_vector</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">weighted_choice</span><span class="p">(</span><span class="n">distinct_words</span><span class="p">,</span> <span class="n">likelihoods</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_next_word_after</span><span class="p">(</span><span class="s1">'The'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyError</span><span class="g g-Whitespace">                                  </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_72706</span><span class="o">/</span><span class="mf">2448159836.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">sample_next_word_after</span><span class="p">(</span><span class="s1">'The'</span><span class="p">)</span>

<span class="nn">/tmp/ipykernel_72706/2107519818.py</span> in <span class="ni">sample_next_word_after</span><span class="nt">(word, alpha)</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span>     <span class="n">Next</span> <span class="n">word</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span>   <span class="s2">"""</span>
<span class="ne">---&gt; </span><span class="mi">15</span><span class="s2">   next_word_vector = next_word_matrix[word_idx_dict[word]] + alpha</span>
<span class="g g-Whitespace">     </span><span class="mi">16</span><span class="s2">   likelihoods = next_word_vector/next_word_vector.sum()</span>
<span class="g g-Whitespace">     </span><span class="mi">17</span><span class="s2">   return weighted_choice(distinct_words, likelihoods)</span>

<span class="ne">KeyError</span>: 'The'
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_next_word_after</span><span class="p">(</span><span class="s1">'The'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyError</span><span class="g g-Whitespace">                                  </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_72706</span><span class="o">/</span><span class="mf">2448159836.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">sample_next_word_after</span><span class="p">(</span><span class="s1">'The'</span><span class="p">)</span>

<span class="nn">/tmp/ipykernel_72706/2107519818.py</span> in <span class="ni">sample_next_word_after</span><span class="nt">(word, alpha)</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span>     <span class="n">Next</span> <span class="n">word</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span>   <span class="s2">"""</span>
<span class="ne">---&gt; </span><span class="mi">15</span><span class="s2">   next_word_vector = next_word_matrix[word_idx_dict[word]] + alpha</span>
<span class="g g-Whitespace">     </span><span class="mi">16</span><span class="s2">   likelihoods = next_word_vector/next_word_vector.sum()</span>
<span class="g g-Whitespace">     </span><span class="mi">17</span><span class="s2">   return weighted_choice(distinct_words, likelihoods)</span>

<span class="ne">KeyError</span>: 'The'
</pre></div>
</div>
</div>
</div>
<p>There! We don’t see the same word twice, because of the added randomisation (i.e., stochasticity). Our algorithm calculates how likely it is to find a certain word after a given word (<code class="docutils literal notranslate"><span class="pre">The</span></code> in this case) in the corpus, and then generates 1 sample of the next word with a matching probability.</p>
<p>In this example, we generated only one next word. Now, using this function, we’ll build a chain.</p>
</div>
<div class="section" id="function-for-a-stochastic-chain-using-weighted-choice">
<h4>Function for a stochastic chain using weighted choice<a class="headerlink" href="#function-for-a-stochastic-chain-using-weighted-choice" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function for a stochastic chain using weighted choice</span>
<span class="k">def</span> <span class="nf">stochastic_chain</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">15</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Helper function to build stochastic chain using</span>
<span class="sd">  weighted choices.</span>

<span class="sd">  Args:</span>
<span class="sd">    word: string</span>
<span class="sd">      Word</span>
<span class="sd">    length: int</span>
<span class="sd">      Length of sentence</span>

<span class="sd">  Returns:</span>
<span class="sd">    sentence: string</span>
<span class="sd">      Sentence built using stochastic chain</span>
<span class="sd">  """</span>
  <span class="n">current_word</span> <span class="o">=</span> <span class="n">word</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">word</span>

  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">length</span><span class="p">):</span>
    <span class="n">sentence</span> <span class="o">+=</span> <span class="s1">' '</span>
    <span class="n">next_word</span> <span class="o">=</span> <span class="n">sample_next_word_after</span><span class="p">(</span><span class="n">current_word</span><span class="p">)</span>
    <span class="n">sentence</span> <span class="o">+=</span> <span class="n">next_word</span>
    <span class="n">current_word</span> <span class="o">=</span> <span class="n">next_word</span>

  <span class="k">return</span> <span class="n">sentence</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_chain</span><span class="p">(</span><span class="s1">'Hospital'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyError</span><span class="g g-Whitespace">                                  </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_72706</span><span class="o">/</span><span class="mf">1630321602.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">stochastic_chain</span><span class="p">(</span><span class="s1">'Hospital'</span><span class="p">)</span>

<span class="nn">/tmp/ipykernel_72706/862387341.py</span> in <span class="ni">stochastic_chain</span><span class="nt">(word, length)</span>
<span class="g g-Whitespace">     </span><span class="mi">20</span>   <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">length</span><span class="p">):</span>
<span class="g g-Whitespace">     </span><span class="mi">21</span>     <span class="n">sentence</span> <span class="o">+=</span> <span class="s1">' '</span>
<span class="ne">---&gt; </span><span class="mi">22</span>     <span class="n">next_word</span> <span class="o">=</span> <span class="n">sample_next_word_after</span><span class="p">(</span><span class="n">current_word</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">23</span>     <span class="n">sentence</span> <span class="o">+=</span> <span class="n">next_word</span>
<span class="g g-Whitespace">     </span><span class="mi">24</span>     <span class="n">current_word</span> <span class="o">=</span> <span class="n">next_word</span>

<span class="nn">/tmp/ipykernel_72706/2107519818.py</span> in <span class="ni">sample_next_word_after</span><span class="nt">(word, alpha)</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span>     <span class="n">Next</span> <span class="n">word</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span>   <span class="s2">"""</span>
<span class="ne">---&gt; </span><span class="mi">15</span><span class="s2">   next_word_vector = next_word_matrix[word_idx_dict[word]] + alpha</span>
<span class="g g-Whitespace">     </span><span class="mi">16</span><span class="s2">   likelihoods = next_word_vector/next_word_vector.sum()</span>
<span class="g g-Whitespace">     </span><span class="mi">17</span><span class="s2">   return weighted_choice(distinct_words, likelihoods)</span>

<span class="ne">KeyError</span>: 'Hospital'
</pre></div>
</div>
</div>
</div>
<p>Neat - we can create stochastic chains for a single word. For a more effective language model, we would want to model sets of words - in the following cells, we create sets of words to predict a chain after a sequence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sequences_matrices</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
  <span class="c1"># @title Code to build sets of words for more realistic sequences</span>
  <span class="sd">"""</span>
<span class="sd">  Code to build sets of words for more realistic sequences</span>

<span class="sd">  Args:</span>
<span class="sd">    k: int</span>
<span class="sd">      Sequence length</span>

<span class="sd">  Returns:</span>
<span class="sd">    k_words_idx_dict: dict</span>
<span class="sd">      Dictionary of words and corresponding indices</span>
<span class="sd">    distinct_sets_of_k_words: list</span>
<span class="sd">      Set of k-word sets</span>
<span class="sd">    next_after_k_words_matrix: list</span>
<span class="sd">      Transition matrix</span>
<span class="sd">  """</span>
  <span class="n">sets_of_k_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">corpus_words</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">k</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">corpus_words</span><span class="p">[:</span><span class="o">-</span><span class="n">k</span><span class="p">])]</span>
  <span class="n">sets_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">sets_of_k_words</span><span class="p">)))</span>
  <span class="n">next_after_k_words_matrix</span> <span class="o">=</span> <span class="n">dok_matrix</span><span class="p">((</span><span class="n">sets_count</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">distinct_words</span><span class="p">)))</span>
  <span class="n">distinct_sets_of_k_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">sets_of_k_words</span><span class="p">))</span>
  <span class="n">k_words_idx_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">distinct_sets_of_k_words</span><span class="p">)}</span>
  <span class="n">distinct_k_words_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">sets_of_k_words</span><span class="p">)))</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">sets_of_k_words</span><span class="p">[:</span><span class="o">-</span><span class="n">k</span><span class="p">])):</span>
    <span class="n">word_sequence_idx</span> <span class="o">=</span> <span class="n">k_words_idx_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="n">next_word_idx</span> <span class="o">=</span> <span class="n">word_idx_dict</span><span class="p">[</span><span class="n">corpus_words</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">k</span><span class="p">]]</span>
    <span class="n">next_after_k_words_matrix</span><span class="p">[</span><span class="n">word_sequence_idx</span><span class="p">,</span> <span class="n">next_word_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="k">return</span> <span class="n">k_words_idx_dict</span><span class="p">,</span> <span class="n">distinct_sets_of_k_words</span><span class="p">,</span><span class="n">next_after_k_words_matrix</span>

<span class="n">k_words_idx_dict</span><span class="p">,</span> <span class="n">distinct_sets_of_k_words</span><span class="p">,</span> <span class="n">next_after_k_words_matrix</span> <span class="o">=</span> <span class="n">sequences_matrices</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<p>Let’s have a look at what that bit of code did.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">distinct_sets_of_k_words</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>

</div>
<p>Great! Now we are going to create a transition matrix for the sets of words.</p>
</div>
<div class="section" id="code-to-populate-matrix-of-sets-of-words">
<h4>Code to populate matrix of sets of words<a class="headerlink" href="#code-to-populate-matrix-of-sets-of-words" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Code to populate matrix of sets of words</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">distinct_sets_of_k_words</span><span class="p">[:</span><span class="o">-</span><span class="n">k</span><span class="p">])):</span>
  <span class="n">word_sequence_idx</span> <span class="o">=</span> <span class="n">k_words_idx_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
  <span class="n">next_word_idx</span> <span class="o">=</span> <span class="n">word_idx_dict</span><span class="p">[</span><span class="n">corpus_words</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">k</span><span class="p">]]</span>
  <span class="n">next_after_k_words_matrix</span><span class="p">[</span><span class="n">word_sequence_idx</span><span class="p">,</span> <span class="n">next_word_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>

</div>
<p>We now have what we need to build a stochastic chain over a <code class="docutils literal notranslate"><span class="pre">K</span></code> set of words.</p>
</div>
<div class="section" id="function-for-stochastic-chain-for-sets-of-words">
<h4>Function for Stochastic Chain for sets of words<a class="headerlink" href="#function-for-stochastic-chain-for-sets-of-words" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function for Stochastic Chain for sets of words</span>
<span class="k">def</span> <span class="nf">stochastic_chain_sequence</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">chain_length</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Function for stochastic Chain for sets of words</span>

<span class="sd">  Args:</span>
<span class="sd">    words: string</span>
<span class="sd">      Sentence</span>
<span class="sd">    chain_length: int</span>
<span class="sd">      Length of stochastic chain [default: 15]</span>
<span class="sd">    k: int</span>
<span class="sd">      Sequence length [default: 2]</span>

<span class="sd">  Returns:</span>
<span class="sd">    sentence: string</span>
<span class="sd">      Sentence generated using stochastic chain</span>
<span class="sd">  """</span>
  <span class="n">current_words</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">current_words</span><span class="p">)</span> <span class="o">!=</span> <span class="n">k</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Wrong number of words, expected </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">words</span>

  <span class="c1"># Pre-calculate seq embedding + transition matrix for a given k</span>
  <span class="n">matrices</span> <span class="o">=</span> <span class="n">sequences_matrices</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">chain_length</span><span class="p">):</span>
    <span class="n">sentence</span> <span class="o">+=</span> <span class="s1">' '</span>
    <span class="n">next_word</span> <span class="o">=</span> <span class="n">sample_next_word_after_sequence</span><span class="p">(</span><span class="n">matrices</span><span class="p">,</span><span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">current_words</span><span class="p">))</span>
    <span class="n">sentence</span> <span class="o">+=</span> <span class="n">next_word</span>
    <span class="n">current_words</span> <span class="o">=</span> <span class="n">current_words</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="p">[</span><span class="n">next_word</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">sentence</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="function-to-sample-next-word-in-sequence">
<h4>Function to sample next word in sequence<a class="headerlink" href="#function-to-sample-next-word-in-sequence" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function to sample next word in sequence</span>
<span class="k">def</span> <span class="nf">sample_next_word_after_sequence</span><span class="p">(</span><span class="n">matrices</span><span class="p">,</span>
                                    <span class="n">word_sequence</span><span class="p">,</span>
                                    <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="c1"># Unpack a tuple of matrices</span>
  <span class="sd">"""</span>
<span class="sd">  Function to sample next word in sequence</span>

<span class="sd">  Args:</span>
<span class="sd">    matrices: list</span>
<span class="sd">      Transition matrix</span>
<span class="sd">    word_sequence: list</span>
<span class="sd">      Word sequence</span>
<span class="sd">    alpha: int</span>
<span class="sd">      Offset</span>

<span class="sd">  Returns:</span>
<span class="sd">    Weighted choice of distinct words based on likelihoods</span>
<span class="sd">  """</span>
  <span class="n">k_words_idx_dict</span><span class="p">,</span><span class="n">distinct_sets_of_k_words</span><span class="p">,</span> <span class="n">next_after_k_words_matrix</span> <span class="o">=</span> <span class="n">matrices</span>

  <span class="n">next_word_vector</span> <span class="o">=</span> <span class="n">next_after_k_words_matrix</span><span class="p">[</span><span class="n">k_words_idx_dict</span><span class="p">[</span><span class="n">word_sequence</span><span class="p">]]</span> <span class="o">+</span> <span class="n">alpha</span>
  <span class="n">likelihoods</span> <span class="o">=</span> <span class="n">next_word_vector</span><span class="o">/</span><span class="n">next_word_vector</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">weighted_choice</span><span class="p">(</span><span class="n">distinct_words</span><span class="p">,</span> <span class="n">likelihoods</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_chain_sequence</span><span class="p">(</span><span class="s1">'Judges under the'</span><span class="p">,</span> <span class="n">chain_length</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<p>Great! This sentence was created using two of the techniques we recently saw - creating sets of words, and using a weighted average stochastic chain. Both of these methods contributed in making it a more meaningful sequence of words. Some of these notions are also captured by Recurrent Neural Networks!</p>
</div>
</div>
<div class="section" id="think-1-2-how-does-changing-parameters-affect-the-generated-sentences">
<h3>Think! 1.2: How does changing parameters affect the generated sentences?<a class="headerlink" href="#think-1-2-how-does-changing-parameters-affect-the-generated-sentences" title="Permalink to this headline">¶</a></h3>
<p>Try and use a set of words but using a naive chain, and try a stochastic chain with a low value of k (i.e., 2), and a higher value (i.e., 5). How do these different configurations change the quality of the sequences produced? Below you have sample code to try these out.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_chain_sequence</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">chain_length</span><span class="o">=...</span><span class="p">,</span> <span class="n">k</span><span class="o">=...</span><span class="p">)</span>
</pre></div>
</div>
<p>You should be able to use these matrices and the previous functions to be able to create the necessary configurations.</p>
<div class="section" id="student-response">
<h4>Student Response<a class="headerlink" href="#student-response" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Student Response</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">widgets</span>


<span class="n">text</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">Textarea</span><span class="p">(</span>
   <span class="n">value</span><span class="o">=</span><span class="s1">'Type your answer here and click on `Submit!`'</span><span class="p">,</span>
   <span class="n">placeholder</span><span class="o">=</span><span class="s1">'Type something'</span><span class="p">,</span>
   <span class="n">description</span><span class="o">=</span><span class="s1">''</span><span class="p">,</span>
   <span class="n">disabled</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="n">button</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">Button</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">"Submit!"</span><span class="p">)</span>

<span class="n">display</span><span class="p">(</span><span class="n">text</span><span class="p">,</span><span class="n">button</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">on_button_clicked</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
   <span class="n">atform</span><span class="o">.</span><span class="n">add_answer</span><span class="p">(</span><span class="s1">'q1'</span><span class="p">,</span> <span class="n">text</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
   <span class="nb">print</span><span class="p">(</span><span class="s2">"Submission successful!"</span><span class="p">)</span>


<span class="n">button</span><span class="o">.</span><span class="n">on_click</span><span class="p">(</span><span class="n">on_button_clicked</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "7dcb999c70294ef5b2a6c0900ae6ad17"}
</script><script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "94c7ef1ba650458981404171a9a3d5c6"}
</script></div>
</div>
</div>
</div>
</div>
<div class="section" id="section-1-3-what-is-a-hidden-markov-model">
<h2>Section 1.3: What is a Hidden Markov Model?<a class="headerlink" href="#section-1-3-what-is-a-hidden-markov-model" title="Permalink to this headline">¶</a></h2>
<p>A 1960s advance (by Leonard Baum and colleagues): Hidden Markov Models are:</p>
<ul class="simple">
<li><p>a Markov model in which the system modeled is assumed to be a Markov process/chain with unobservable (“hidden”) states.</p></li>
<li><p>HMM assumes there is another surrogate process whose behavior “depends” on the state–you learn about the state by observing the surrogate process.</p></li>
<li><p>HMMs have successfully been applied in fields where the goal is to recover a data sequence not immediately observable (but other data that depend on the sequence are).</p></li>
<li><p>The first dominant application: Speech and text processing (1970s)</p></li>
</ul>
<p>In this sub-section we will use the python library <a class="reference external" href="https://hmmlearn.readthedocs.io/en/latest/tutorial.html#training-hmm-parameters-and-inferring-the-hidden-states">hmmlearn</a>, which is part of the <em>scikit-learn</em> ecosystem. <a class="reference external" href="https://github.com/mfilej/nlg-with-hmmlearn">nlg-with-hmmlearn</a> offers useful code snippets to adapt <code class="docutils literal notranslate"><span class="pre">hmmlearn</span></code> for text data. Because we are using a package that offers many out of the box implementations for HMMs, we don’t have to worry about the states, transition matrices.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the data</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="n">brown</span><span class="o">.</span><span class="n">sents</span><span class="p">(</span><span class="n">categories</span><span class="o">=</span><span class="n">category</span><span class="p">)</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">]</span>
<span class="n">lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>
<span class="n">alphabet</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

<span class="c1"># Encode words</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">alphabet</span><span class="p">))</span>

<span class="c1"># Find word freqeuncies</span>
<span class="n">seq</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromiter</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span> <span class="c1"># Create 1D array</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="c1"># View input as at least 2D array</span>
<span class="n">fd</span> <span class="o">=</span> <span class="n">FreqDist</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="c1"># Returns frequency distribution of words.</span>
</pre></div>
</div>
</div>

</div>
<p>Now that we have our data setup, we can create our model. We use a multinomial HMM with 8 states, and can either do a random initialisation or use word frequences. We recommend trying both options!</p>
<div class="section" id="function-to-create-default-multinomial-hmm-model">
<h3>Function to create Default Multinomial HMM model<a class="headerlink" href="#function-to-create-default-multinomial-hmm-model" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function to create Default Multinomial HMM model</span>
<span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="n">num_states</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Function to create Default Multinomial HMM model</span>

<span class="sd">  Args:</span>
<span class="sd">    num_states: int</span>
<span class="sd">      Specifies number of states in HMM model</span>

<span class="sd">  Returns:</span>
<span class="sd">    model: HMM instance</span>
<span class="sd">      Default Multinomial HMM model</span>
<span class="sd">  """</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"Initial parameter estimation using built-in method"</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">MultinomialHMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">num_states</span><span class="p">,</span> <span class="n">init_params</span><span class="o">=</span><span class="s1">'ste'</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="function-to-create-default-multinomial-hmm-model-information-of-relative-frequencies-of-words">
<h3>Function to create Default Multinomial HMM model information of relative frequencies of words<a class="headerlink" href="#function-to-create-default-multinomial-hmm-model-information-of-relative-frequencies-of-words" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function to create Default Multinomial HMM model information of relative frequencies of words</span>
<span class="k">def</span> <span class="nf">frequencies</span><span class="p">(</span><span class="n">num_states</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Function to create Default Multinomial HMM</span>
<span class="sd">  model information of relative frequencies of words</span>

<span class="sd">  Args:</span>
<span class="sd">    num_states: int</span>
<span class="sd">      Specifies number of states in HMM model</span>

<span class="sd">  Returns:</span>
<span class="sd">    model: HMM instance</span>
<span class="sd">      Default Multinomial HMM model</span>
<span class="sd">      replete with relative frequencies of words</span>
<span class="sd">      and emission probabilities</span>
<span class="sd">  """</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"Initial parameter estimation using relative frequencies"</span><span class="p">)</span>

  <span class="n">frequencies</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromiter</span><span class="p">((</span><span class="n">fd</span><span class="o">.</span><span class="n">freq</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">alphabet</span><span class="p">))),</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
  <span class="n">emission_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">frequencies</span><span class="p">]</span><span class="o">*</span><span class="n">num_states</span><span class="p">)</span>

  <span class="n">model</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">MultinomialHMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">num_states</span><span class="p">,</span> <span class="n">init_params</span><span class="o">=</span><span class="s1">'st'</span><span class="p">)</span>
  <span class="n">model</span><span class="o">.</span><span class="n">emissionprob_</span> <span class="o">=</span> <span class="n">emission_prob</span>
  <span class="k">return</span> <span class="n">model</span>


<span class="nb">print</span><span class="p">(</span><span class="n">frequencies</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>

</div>
<p><strong>Note</strong>:</p>
<p>The following lines of code are commented out because they take a long time (~17 mins for default Brown corpus categories).</p>
<p>If you do not have that time, you can download the default model to try to generate text. You have to uncomment the appropriate lines.</p>
<p><strong>Note:</strong> Either you may want to uncomment Line 11 or Line 14, not both, as the output variable <code class="docutils literal notranslate"><span class="pre">model</span></code> will be overwritten.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Fitting a default multinomial HMM. This might take a while to run(~17 mins)</span>
<span class="k">def</span> <span class="nf">run_model</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">num_states</span><span class="p">):</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">(</span><span class="n">num_states</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">model</span>


<span class="n">num_states</span> <span class="o">=</span> <span class="mi">8</span>
<span class="c1">## Uncomment, if you have time!</span>
<span class="c1"># model = run_model(features, lengths, num_states)</span>

<span class="c1">## Another way to get a model is to use default frequencies when initialising the model</span>
<span class="c1"># model = frequencies(num_states)</span>
</pre></div>
</div>
</div>
</div>
<p>Alternatively, you could use a saved model. Here is a <a class="reference external" href="https://drive.google.com/file/d/1IymcmcO48V6q3x-6dhf7-OU5NByo5W2F/view?usp=sharing">link</a> to the default model, which you can download and then upload into Colab.</p>
<p>Execute this cell to download the saved model.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Execute this cell to download the saved model.</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">"https://osf.io/5k6cs/download"</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">'model_w2d3_t1.pkl'</span><span class="p">,</span> <span class="s1">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">fd</span><span class="p">:</span>
  <span class="n">fd</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>

<span class="c1"># Load the pickle file</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"model_w2d3_t1.pkl"</span><span class="p">,</span> <span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="function-to-generate-words-given-hmm-model">
<h3>Function to generate words given HMM model<a class="headerlink" href="#function-to-generate-words-given-hmm-model" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function to generate words given HMM model</span>
<span class="k">def</span> <span class="nf">generate_text</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">num_lines</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">random_len</span><span class="o">=</span><span class="mi">15</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Function to generate words from given HMM model</span>

<span class="sd">  Args:</span>
<span class="sd">    model: HMM instance</span>
<span class="sd">      Multinomial HMM Model</span>
<span class="sd">    num_lines: int</span>
<span class="sd">      Specifies number of lines [default: 5]</span>
<span class="sd">    random_len: int</span>
<span class="sd">      Specifies random sequence length [default: 15]</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="k">for</span> <span class="n">_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_lines</span><span class="p">):</span>
    <span class="n">set_seed</span><span class="p">(</span><span class="n">_i</span><span class="p">)</span>
    <span class="n">symbols</span><span class="p">,</span> <span class="n">_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">random_len</span><span class="p">)</span>

    <span class="c1"># To scale transformation to original representation</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">symbols</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">output</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">" "</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generate_text</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">num_lines</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_len</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<p>We see that a hidden markov model also does well in generating text. We encourage you to try out different initialisations and hyperparameters to see how the model does.</p>
</div>
<div class="section" id="bonus-exercise-1-3-transition-probabilities">
<h3>(Bonus) Exercise 1.3: Transition probabilities<a class="headerlink" href="#bonus-exercise-1-3-transition-probabilities" title="Permalink to this headline">¶</a></h3>
<p>We have seen how we can use sequences of text to form probability chains, as well as how we can use out of the box models to generate text. In this exercise, you will be using your own data to generate sequences using <code class="docutils literal notranslate"><span class="pre">hmmlearn</span></code> or any other implementation of a markov model. Explore the transition probabilities in your corpus and generate sentences. For example, one such exploration can be - How does using a model with the word frequencies compare to a default model?</p>
<p>Perform any one such comparison or exploration, and generate 3 sentences or 50 words using your model. You should be able to use all the existing functions defined for this exercise.</p>
<p><strong>Note:</strong> We suggest to do this exercise after the completion of both tutorials.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load your own dataset and create a model using the frequencies based HMM model!</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="useful-links-for-markov-models-and-hmm">
<h3>Useful links for Markov Models and HMM:<a class="headerlink" href="#useful-links-for-markov-models-and-hmm" title="Permalink to this headline">¶</a></h3>
<p>Here are some useful links if you wish to explore this topic further.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6">Markov Chain Text</a></p></li>
<li><p><a class="reference external" href="https://python.quantecon.org/finite_markov.html">Python QuantEcon: Finite Markov Chains with Finance</a></p></li>
<li><p><a class="reference external" href="https://ericmjl.github.io/essays-on-data-science/machine-learning/markov-models/">Markov Models from the ground up, with python</a></p></li>
<li><p><a class="reference external" href="https://github.com/nareshkumar66675/GenTex">GenTex</a></p></li>
<li><p><a class="reference external" href="https://hmmlearn.readthedocs.io/en/latest/tutorial.html">HMM learn</a></p></li>
</ul>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-2-word-embeddings">
<h1>Section 2: Word Embeddings<a class="headerlink" href="#section-2-word-embeddings" title="Permalink to this headline">¶</a></h1>
<p><em>Time estimate: ~60mins</em></p>
<div class="section" id="video-2-textual-dimension-reduction">
<h2>Video 2: Textual Dimension Reduction<a class="headerlink" href="#video-2-textual-dimension-reduction" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "f31d9393324b450e86f3928206940bec"}
</script></div>
</div>
<p>Words or subword units such as morphemes are the basic units that we use to express meaning  in language. The technique of mapping words to vectors of real numbers is known as word embedding.</p>
<p>Word2vec is based on theories of distributional semantics - words that appear around each other are more likely to mean similar things than words that do not appear around each other. Keeping this in mind, our job is to create a high dimensional space where these semantic relations are preserved. The innovation in word2vec is the realisation that we can use unlabelled, running text in sentences as inputs for a supervised learning algorithm–as a self-supervision task. It is supervised because we use the words in a sentence to serve as positive and negative examples. Let’s break this down:</p>
<p>… “use the kitchen knife to chop the vegetables”…</p>
<p><strong>C1   C2   C3   T   C4   C5   C6   C7</strong></p>
<p>Here, the target word is knife, and the context words are the ones in its immediate (6-word) window.
The first word2vec method we’ll see is called skipgram, where the task is to assign a probability for how likely it is that the context window appears around the target word. In the training process, positive examples are samples of words and their context words, and negative examples are created by sampling from pairs of words that do not appear nearby one another.</p>
<p>This method of implementing word2vec is called skipgram with negative sampling. So while the algorithm tries to better learn which context words are likely to appear around a target word, it ends up pushing the embedded representations for every word so that they are located optimally (e.g., with minimal semantic distortion). In this process of adjusting embedding values, the algorithm brings semantically similar words close together in the resulting high dimensional space, and dissimilar words far away.</p>
<p>Another word2vec training method, Continuous Bag of Words (CBOW), works in a similar fashion, and tries to predict the target word, given context. This is converse of skipgram, which tries to predict the context, given the target word. Skip-gram represents rare words and phrases well, often requiring more data for stable representations, while CBOW is several times faster to train than the skip-gram, but with slightly better accuracy for the frequent words in its prediction task. The popular gensim implementation of word2vec has both the methods included.</p>
</div>
<div class="section" id="section-2-1-creating-word-embeddings">
<h2>Section 2.1: Creating Word Embeddings<a class="headerlink" href="#section-2-1-creating-word-embeddings" title="Permalink to this headline">¶</a></h2>
<p>We will create embeddings for a subset of categories in <a class="reference external" href="https://www1.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html">Brown corpus</a>.  In order to achieve this task we will use <a class="reference external" href="https://radimrehurek.com/gensim/">gensim</a> library to create word2vec embeddings. Gensim’s word2vec expects a sequence of sentences as its input. Each sentence is a list of words.
Calling <code class="docutils literal notranslate"><span class="pre">Word2Vec(sentences,</span> <span class="pre">iter=1)</span></code> will run two passes over the sentences iterator (or, in general iter+1 passes). The first pass collects words and their frequencies to build an internal dictionary tree structure. The second and subsequent passes train the neural model.
<code class="docutils literal notranslate"><span class="pre">Word2vec</span></code> accepts several parameters that affect both training speed and quality.</p>
<p>One of them is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them:</p>
<p><code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">Word2Vec(sentences,</span> <span class="pre">min_count=10)</span>  <span class="pre">#</span> <span class="pre">default</span> <span class="pre">value</span> <span class="pre">is</span> <span class="pre">5</span></code></p>
<p>A reasonable value for min_count is between 0-100, depending on the size of your dataset.</p>
<p>Another parameter is the size of the NN layers, which correspond to the “degrees” of freedom the training algorithm has:</p>
<p><code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">Word2Vec(sentences,</span> <span class="pre">size=200)</span>  <span class="pre">#</span> <span class="pre">default</span> <span class="pre">value</span> <span class="pre">is</span> <span class="pre">100</span></code></p>
<p>Bigger size values require more training data, but can lead to better (more accurate) models. Reasonable values are in the tens to hundreds.</p>
<p>The last of the major parameters (full list <a class="reference external" href="https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec">here</a>) is for training parallelization, to speed up training:</p>
<p><code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">Word2Vec(sentences,</span> <span class="pre">workers=4)</span> <span class="pre">#</span> <span class="pre">default</span> <span class="pre">=</span> <span class="pre">1</span> <span class="pre">worker</span> <span class="pre">=</span> <span class="pre">no</span> <span class="pre">parallelization</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">category</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'editorial'</span><span class="p">,</span> <span class="s1">'fiction'</span><span class="p">,</span> <span class="s1">'government'</span><span class="p">,</span> <span class="s1">'mystery'</span><span class="p">,</span> <span class="s1">'news'</span><span class="p">,</span> <span class="s1">'religion'</span><span class="p">,</span>
            <span class="s1">'reviews'</span><span class="p">,</span> <span class="s1">'romance'</span><span class="p">,</span> <span class="s1">'science_fiction'</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_word2vec_model</span><span class="p">(</span><span class="n">category</span><span class="o">=</span><span class="s1">'news'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Helper function to create word2vec model</span>

<span class="sd">  Args:</span>
<span class="sd">    category: string</span>
<span class="sd">      Specifies category (editorial/fiction/government/mystery/news/religion/reviews/romance/science_fiction)</span>
<span class="sd">    size: int</span>
<span class="sd">      Specifies size [default: 50]</span>
<span class="sd">    min_count: int</span>
<span class="sd">      Minimum sentence length [default: 5]</span>
<span class="sd">    sg: int</span>
<span class="sd">      Skip gram length</span>

<span class="sd">  Returns:</span>
<span class="sd">    model: Word2Vec instance</span>
<span class="sd">      Word2Vec model</span>
<span class="sd">  """</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="n">brown</span><span class="o">.</span><span class="n">sents</span><span class="p">(</span><span class="n">categories</span><span class="o">=</span><span class="n">category</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="n">sg</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="n">min_count</span><span class="p">)</span>

  <span class="k">except</span> <span class="p">(</span><span class="ne">AttributeError</span><span class="p">,</span> <span class="ne">TypeError</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s1">'Input variable "category" should be a string or list,'</span>
      <span class="s1">'"size", "sg", "min_count" should be integers'</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">model</span>

<span class="k">def</span> <span class="nf">model_dictionary</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Helper function to build model dictionary</span>

<span class="sd">  Args:</span>
<span class="sd">    model:  Word2Vec instance</span>
<span class="sd">      Word2Vec model</span>

<span class="sd">  Returns:</span>
<span class="sd">    words: list</span>
<span class="sd">      Maps word to index position</span>
<span class="sd">  """</span>
  <span class="n">words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">words</span>

<span class="k">def</span> <span class="nf">get_embedding</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Helper function to get embedding</span>

<span class="sd">  Args:</span>
<span class="sd">    model:  Word2Vec instance</span>
<span class="sd">      Word2Vec model</span>
<span class="sd">    word: string</span>
<span class="sd">      Word for which embedding is to be extracted</span>

<span class="sd">  Returns:</span>
<span class="sd">    Word-index if word is in model_dictionary</span>
<span class="sd">    None otherwise</span>
<span class="sd">  """</span>
  <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">all_categories</span> <span class="o">=</span> <span class="n">brown</span><span class="o">.</span><span class="n">categories</span><span class="p">()</span>
</pre></div>
</div>
</div>

</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">all_categories</span>
</pre></div>
</div>
</div>

</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w2vmodel</span> <span class="o">=</span> <span class="n">create_word2vec_model</span><span class="p">(</span><span class="n">all_categories</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model_dictionary</span><span class="p">(</span><span class="n">w2vmodel</span><span class="p">))</span>
</pre></div>
</div>
</div>

</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">get_embedding</span><span class="p">(</span><span class="s1">'weather'</span><span class="p">,</span> <span class="n">w2vmodel</span><span class="p">))</span>
</pre></div>
</div>
</div>

</div>
</div>
<div class="section" id="section-2-2-visualizing-word-embedding">
<h2>Section 2.2: Visualizing Word Embedding<a class="headerlink" href="#section-2-2-visualizing-word-embedding" title="Permalink to this headline">¶</a></h2>
<p>We can now obtain the word embeddings for any word in the dictionary using word2vec. Let’s visualize these embeddings to get an intuition of what these embeddings mean. The word embeddings obtained from word2vec model are in high dimensional space. We will use <code class="docutils literal notranslate"><span class="pre">tSNE</span></code> (t-distributed stochastic neighbor embedding), a statistical method for dimensionality deduction that allow us to visualize high-dimensional data in a 2D or 3D space. Here, we will use <code class="docutils literal notranslate"><span class="pre">tSNE</span></code> from <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code></a> module to project our high dimensional embeddings in the 2D space (if you are not familiar with this method, think about <code class="docutils literal notranslate"><span class="pre">PCA</span></code>).</p>
<p>For each word in <code class="docutils literal notranslate"><span class="pre">keys</span></code>, we pick the top 10 similar words (using cosine similarity) and plot them.</p>
<p>What should be the arrangement of similar words?
What should be arrangement of the key clusters with respect to each other?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keys</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'voters'</span><span class="p">,</span> <span class="s1">'magic'</span><span class="p">,</span> <span class="s1">'love'</span><span class="p">,</span> <span class="s1">'God'</span><span class="p">,</span> <span class="s1">'evidence'</span><span class="p">,</span> <span class="s1">'administration'</span><span class="p">,</span> <span class="s1">'governments'</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_cluster_embeddings</span><span class="p">(</span><span class="n">keys</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Function to cluster embeddings</span>

<span class="sd">  Args:</span>
<span class="sd">    keys: list</span>
<span class="sd">      Keys from model_dictionary</span>

<span class="sd">  Returns:</span>
<span class="sd">    word_clusters: list</span>
<span class="sd">      Word clusters</span>
<span class="sd">    embedding_clusters: list</span>
<span class="sd">      Embeddings for words in clusters</span>
<span class="sd">  """</span>
  <span class="n">embedding_clusters</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">word_clusters</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># Find closest words and add them to cluster</span>
  <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">:</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">w2vmodel</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">'The word '</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="s1">'is not in the dictionary'</span><span class="p">)</span>
      <span class="k">continue</span>

    <span class="k">for</span> <span class="n">similar_word</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">w2vmodel</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
      <span class="n">words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">similar_word</span><span class="p">)</span>
      <span class="n">embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w2vmodel</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="n">similar_word</span><span class="p">])</span>
    <span class="n">embedding_clusters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
    <span class="n">word_clusters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

  <span class="c1"># Get embeddings for the words in clusers</span>
  <span class="n">embedding_clusters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">embedding_clusters</span><span class="p">)</span>
  <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">embedding_clusters</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">tsne_model_en_2d</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">perplexity</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">'pca'</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">3500</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
  <span class="n">embeddings_en_2d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tsne_model_en_2d</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embedding_clusters</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">)))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">embeddings_en_2d</span><span class="p">,</span> <span class="n">word_clusters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tsne_plot_similar_words</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">embedding_clusters</span><span class="p">,</span>
                            <span class="n">word_clusters</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Generates tSNE plot for words</span>

<span class="sd">  Args:</span>
<span class="sd">    a: int</span>
<span class="sd">      Offset</span>
<span class="sd">    filename: string</span>
<span class="sd">      Filename to save tSNE plot [default: None]</span>
<span class="sd">    title: string</span>
<span class="sd">      Specifies title of plot</span>
<span class="sd">    label: list</span>
<span class="sd">      List of labels</span>
<span class="sd">    word_clusters: list</span>
<span class="sd">      Word clusters</span>
<span class="sd">    embedding_clusters: list</span>
<span class="sd">      Embeddings for words in clusters</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
  <span class="n">colors</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">rainbow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)))</span>
  <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">words</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">embedding_clusters</span><span class="p">,</span> <span class="n">word_clusters</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span>
                   <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                   <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span>
                   <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                   <span class="n">textcoords</span><span class="o">=</span><span class="s1">'offset points'</span><span class="p">,</span>
                   <span class="n">ha</span><span class="o">=</span><span class="s1">'right'</span><span class="p">,</span>
                   <span class="n">va</span><span class="o">=</span><span class="s1">'bottom'</span><span class="p">,</span>
                   <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">"lower left"</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">filename</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">'png'</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">'tight'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embeddings_en_2d</span><span class="p">,</span> <span class="n">word_clusters</span> <span class="o">=</span> <span class="n">get_cluster_embeddings</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>
<span class="n">tsne_plot_similar_words</span><span class="p">(</span><span class="s1">'Similar words from Brown Corpus'</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">embeddings_en_2d</span><span class="p">,</span> <span class="n">word_clusters</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
</div>
<div class="section" id="section-2-3-exploring-meaning-with-word-embeddings">
<h2>Section 2.3: Exploring meaning with word embeddings<a class="headerlink" href="#section-2-3-exploring-meaning-with-word-embeddings" title="Permalink to this headline">¶</a></h2>
<p>While word2vec was the method that started it all, research has since boomed, and we now have more sophisticated ways to represent words. One such method is FastText, developed at Facebook AI research, which breaks words into sub-words: such a technique also allows us to create embedding representations for unseen words. In this section, we will explore how semantics and meaning are captured using embedidngs, after downloading a pre-trained FastText model. Downloading pre-trained models is a way for us to plug in word embeddings and explore them without training them ourselves.</p>
<div class="section" id="download-fasttext-english-embeddings-of-dimension-100">
<h3>Download FastText English Embeddings of Dimension 100<a class="headerlink" href="#download-fasttext-english-embeddings-of-dimension-100" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Download FastText English Embeddings of Dimension 100</span>
<span class="kn">import</span> <span class="nn">os</span><span class="o">,</span> <span class="nn">io</span><span class="o">,</span> <span class="nn">zipfile</span>
<span class="kn">from</span> <span class="nn">urllib.request</span> <span class="kn">import</span> <span class="n">urlopen</span>

<span class="n">zipurl</span> <span class="o">=</span> <span class="s1">'https://osf.io/w9sr7/download'</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Downloading and unzipping the file... Please wait."</span><span class="p">)</span>
<span class="k">with</span> <span class="n">urlopen</span><span class="p">(</span><span class="n">zipurl</span><span class="p">)</span> <span class="k">as</span> <span class="n">zipresp</span><span class="p">:</span>
  <span class="k">with</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">zipresp</span><span class="o">.</span><span class="n">read</span><span class="p">()))</span> <span class="k">as</span> <span class="n">zfile</span><span class="p">:</span>
    <span class="n">zfile</span><span class="o">.</span><span class="n">extractall</span><span class="p">(</span><span class="s1">'.'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Download completed!"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading and unzipping the file... Please wait.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Download completed!
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load 100 dimension FastText Vectors using FastText library</span>
<span class="n">ft_en_vectors</span> <span class="o">=</span> <span class="n">fasttext</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s1">'cc.en.100.bin'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Length of the embedding is: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_word_vector</span><span class="p">(</span><span class="s1">'king'</span><span class="p">))</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Embedding for the word King is: </span><span class="si">{</span><span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_word_vector</span><span class="p">(</span><span class="s1">'king'</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Length of the embedding is: 100
Embedding for the word King is: [-0.04045481 -0.10617249 -0.27222311  0.06879666  0.16408321  0.00276707
  0.27080125 -0.05805573 -0.31865698  0.03748008 -0.00254088  0.13805169
 -0.00182498 -0.08973497  0.00319015 -0.19619396 -0.09858181 -0.10103802
 -0.08279888  0.0082208   0.13119364 -0.15956607  0.17203182  0.0315701
 -0.25064597  0.06182072  0.03929246  0.05157393  0.03543638  0.13660161
  0.05473648  0.06072914 -0.04709269  0.17394426 -0.02101276 -0.11402624
 -0.24489872 -0.08576579 -0.00322696 -0.04509873 -0.00614253 -0.05772085
 -0.073414   -0.06718913 -0.06057961  0.10963406  0.1245006  -0.04819863
  0.11408057  0.11081408  0.06752145 -0.01689911 -0.01186301 -0.11716368
 -0.01287614  0.10639337 -0.04243141  0.01057278 -0.0230855  -0.04930984
  0.04717607  0.03696446  0.0015999  -0.02193867 -0.01331578  0.11102925
  0.1686794   0.05814958 -0.00296521 -0.04252011 -0.00352389  0.06267346
 -0.07747819 -0.08959802 -0.02445797 -0.08913022  0.13422231  0.1258949
 -0.01296814  0.0531218  -0.00541025 -0.16908626  0.06323182 -0.11510128
 -0.08352032 -0.07224389  0.01023453  0.08263734 -0.03859017 -0.00798539
 -0.01498295  0.05448429  0.02708506  0.00549948  0.14634523 -0.12550676
  0.04641578 -0.10164826  0.05370862  0.01217492]
</pre></div>
</div>
</div>
</div>
<p>Cosine similarity is used for similarities between words. Similarity is a scalar between 0 and 1.</p>
<p>Now find the 10 most similar words to “King”.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_nearest_neighbors</span><span class="p">(</span><span class="s2">"king"</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># Most similar by key</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(0.8168574571609497, 'prince'),
 (0.796097457408905, 'emperor'),
 (0.7907207608222961, 'kings'),
 (0.7655220627784729, 'lord'),
 (0.7435404062271118, 'king-'),
 (0.7394551634788513, 'chieftain'),
 (0.7307553291320801, 'tyrant'),
 (0.7226710319519043, 'conqueror'),
 (0.719561755657196, 'kingly'),
 (0.718187689781189, 'queen')]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="word-similarity">
<h3>Word Similarity<a class="headerlink" href="#word-similarity" title="Permalink to this headline">¶</a></h3>
<div class="section" id="video-3-semantic-measurements">
<h4>Video 3: Semantic Measurements<a class="headerlink" href="#video-3-semantic-measurements" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "d82974f823a147ce946f495f01136c28"}
</script></div>
</div>
<p>More on similarity between words. Let’s check how similar different pairs of word are. Feel free to play around.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">getSimilarity</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Defines similarity between set of words</span>

<span class="sd">  Args:</span>
<span class="sd">    word1: string</span>
<span class="sd">      Word 1</span>
<span class="sd">    word2: string</span>
<span class="sd">      Word 2</span>

<span class="sd">  Returns:</span>
<span class="sd">    Cosine similarity between word1 vector and word2 vector</span>
<span class="sd">  """</span>
  <span class="n">v1</span> <span class="o">=</span> <span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_word_vector</span><span class="p">(</span><span class="n">word1</span><span class="p">)</span>
  <span class="n">v2</span> <span class="o">=</span> <span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_word_vector</span><span class="p">(</span><span class="n">word2</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Similarity between the words King and Queen: "</span><span class="p">,</span> <span class="n">getSimilarity</span><span class="p">(</span><span class="s2">"king"</span><span class="p">,</span> <span class="s2">"queen"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Similarity between the words King and Knight: "</span><span class="p">,</span> <span class="n">getSimilarity</span><span class="p">(</span><span class="s2">"king"</span><span class="p">,</span> <span class="s2">"knight"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Similarity between the words King and Rock: "</span><span class="p">,</span> <span class="n">getSimilarity</span><span class="p">(</span><span class="s2">"king"</span><span class="p">,</span> <span class="s2">"rock"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Similarity between the words King and Twenty: "</span><span class="p">,</span> <span class="n">getSimilarity</span><span class="p">(</span><span class="s2">"king"</span><span class="p">,</span> <span class="s2">"twenty"</span><span class="p">))</span>

<span class="c1">## Try the same for two more pairs</span>
<span class="c1"># print("Similarity between the words ___ and ___: ", getSimilarity(...))</span>
<span class="c1"># print("Similarity between the words ___ and ___: ", getSimilarity(...))</span>

<span class="c1"># print("Similarity between the words ___ and ___: ", getSimilarity(...))</span>
<span class="c1"># print("Similarity between the words ___ and ___: ", getSimilarity(...))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Similarity between the words King and Queen:  0.71818775
Similarity between the words King and Knight:  0.6881009
Similarity between the words King and Rock:  0.28928387
Similarity between the words King and Twenty:  0.19655468
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="homonym-words-dagger">
<h3>Homonym Words<span class="math notranslate nohighlight">\(^\dagger\)</span><a class="headerlink" href="#homonym-words-dagger" title="Permalink to this headline">¶</a></h3>
<p>Find the similarity for homonym words with their different meanings. The first one has been implemented for you.</p>
<p><span class="math notranslate nohighlight">\(^\dagger\)</span>: Two or more words having the same spelling or pronunciation but different meanings and origins are called <em>homonyms</em>. E.g.,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#######################     Words with multiple meanings     ##########################</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Similarity between the words Cricket and Insect: "</span><span class="p">,</span> <span class="n">getSimilarity</span><span class="p">(</span><span class="s2">"cricket"</span><span class="p">,</span> <span class="s2">"insect"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Similarity between the words Cricket and Sport: "</span><span class="p">,</span> <span class="n">getSimilarity</span><span class="p">(</span><span class="s2">"cricket"</span><span class="p">,</span> <span class="s2">"sport"</span><span class="p">))</span>

<span class="c1">## Try the same for two more pairs</span>
<span class="c1"># print("Similarity between the words ___ and ___: ", getSimilarity(...))</span>
<span class="c1"># print("Similarity between the words ___ and ___: ", getSimilarity(...))</span>

<span class="c1"># print("Similarity between the words ___ and ___: ", getSimilarity(...))</span>
<span class="c1"># print("Similarity between the words ___ and ___: ", getSimilarity(...))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Similarity between the words Cricket and Insect:  0.40722153
Similarity between the words Cricket and Sport:  0.5812375
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="word-analogies">
<h3>Word Analogies<a class="headerlink" href="#word-analogies" title="Permalink to this headline">¶</a></h3>
<p>Embeddings can be used to find word analogies.
Let’s try it:</p>
<ol class="simple">
<li><p>Man : Woman  ::  King : _____</p></li>
<li><p>Germany: Berlin :: France : ______</p></li>
<li><p>Leaf : Tree  ::  Petal : _____</p></li>
<li><p>Hammer : Nail  ::  Comb : _____</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Use get_analogies() funnction. The words have to be in the order Positive, Negative, Positve</span>

<span class="c1"># Man : Woman  ::  King : _____</span>
<span class="c1"># Positive=(woman, king), Negative=(man)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_analogies</span><span class="p">(</span><span class="s2">"woman"</span><span class="p">,</span> <span class="s2">"man"</span><span class="p">,</span> <span class="s2">"king"</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Germany: Berlin :: France : ______</span>
<span class="c1"># Positive=(berlin, frannce), Negative=(germany)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_analogies</span><span class="p">(</span><span class="s2">"berlin"</span><span class="p">,</span> <span class="s2">"germany"</span><span class="p">,</span> <span class="s2">"france"</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Leaf : Tree  ::  Petal : _____</span>
<span class="c1"># Positive=(tree, petal), Negative=(leaf)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_analogies</span><span class="p">(</span><span class="s2">"tree"</span><span class="p">,</span> <span class="s2">"leaf"</span><span class="p">,</span> <span class="s2">"petal"</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Hammer : Nail  ::  Comb : _____</span>
<span class="c1"># Positive=(nail, comb), Negative=(hammer)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_analogies</span><span class="p">(</span><span class="s2">"nail"</span><span class="p">,</span> <span class="s2">"hammer"</span><span class="p">,</span> <span class="s2">"comb"</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(0.8162637948989868, 'queen')]
[(0.8568049669265747, 'paris')]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(0.7037209272384644, 'flower')]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(0.6908746361732483, 'hair')]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<p>But, does it always work?</p>
<ol class="simple">
<li><p>Poverty : Wealth  :: Sickness : _____</p></li>
<li><p>train : board :: horse : _____</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Poverty : Wealth  :: Sickness : _____</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_analogies</span><span class="p">(</span><span class="s2">"wealth"</span><span class="p">,</span> <span class="s2">"poverty"</span><span class="p">,</span> <span class="s2">"sickness"</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># train : board :: horse : _____</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_analogies</span><span class="p">(</span><span class="s2">"train"</span><span class="p">,</span> <span class="s2">"board"</span><span class="p">,</span> <span class="s2">"horse"</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(0.615874171257019, 'affliction')]
[(0.6799090504646301, 'horses')]
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-3-neural-net-with-word-embeddings">
<h1>Section 3: Neural Net with word embeddings<a class="headerlink" href="#section-3-neural-net-with-word-embeddings" title="Permalink to this headline">¶</a></h1>
<p><em>Time estimate: ~16mins</em></p>
<p>Let’s use the pretrained FastText embeddings to train a neural network on the IMDB dataset.</p>
<p>To recap, the data consists of reviews and sentiments attached to it. It is a binary classification task. As a simple preview of the upcoming neural networks, we are going to introduce neural net with word embeddings. We’ll see detailed networks in the next tutorial.</p>
<div class="section" id="coding-exercise-3-1-simple-feed-forward-net">
<h2>Coding Exercise 3.1: Simple Feed Forward Net<a class="headerlink" href="#coding-exercise-3-1-simple-feed-forward-net" title="Permalink to this headline">¶</a></h2>
<p>This will load 300 dim FastText embeddings. It will take around 2-3 minutes.</p>
<p>Define a vanilla neural network with linear layers. Then average the word embeddings to get an embedding for the entire review.
The neural net will have one hidden layer of size 128.</p>
<div class="section" id="download-embeddings-and-clear-old-variables-to-clean-memory">
<h3>Download embeddings and clear old variables to clean memory.<a class="headerlink" href="#download-embeddings-and-clear-old-variables-to-clean-memory" title="Permalink to this headline">¶</a></h3>
<div class="section" id="execute-this-cell">
<h4>Execute this cell!<a class="headerlink" href="#execute-this-cell" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Download embeddings and clear old variables to clean memory.</span>
<span class="c1"># @markdown #### Execute this cell!</span>
<span class="k">if</span> <span class="s1">'ft_en_vectors'</span> <span class="ow">in</span> <span class="nb">locals</span><span class="p">():</span>
  <span class="k">del</span> <span class="n">ft_en_vectors</span>
<span class="k">if</span> <span class="s1">'w2vmodel'</span> <span class="ow">in</span> <span class="nb">locals</span><span class="p">():</span>
  <span class="k">del</span> <span class="n">w2vmodel</span>

<span class="n">embedding_fasttext</span> <span class="o">=</span> <span class="n">FastText</span><span class="p">(</span><span class="s1">'simple'</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<p>Load the Dataset</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Load the Dataset</span>
<span class="n">TEXT</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">valid_iter</span><span class="p">,</span> <span class="n">test_iter</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">embedding_fasttext</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Neural Network with following structure:</span>
<span class="sd">  nn.Embedding(vocab_size, embedding_length)</span>
<span class="sd">  + nn.Parameter(word_embeddings, requires_grad=False) # Embedding Layer</span>
<span class="sd">  nn.Linear(embedding_length, hidden_size) # Fully connected layer #1</span>
<span class="sd">  nn.Linear(hidden_size, output_size) # Fully connected layer #2</span>
<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_length</span><span class="p">,</span>
               <span class="n">word_embeddings</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Initialize parameters of NeuralNet</span>

<span class="sd">    Args:</span>
<span class="sd">      output_size: int</span>
<span class="sd">        Size of final fully connected layer</span>
<span class="sd">      hidden_size: int</span>
<span class="sd">        Size of hidden/first fully connected layer</span>
<span class="sd">      vocab_size: int</span>
<span class="sd">        Size of vocabulary</span>
<span class="sd">      embedding_length: int</span>
<span class="sd">        Length of embedding</span>
<span class="sd">      word_embeddings: TEXT.vocab.vectors instance</span>
<span class="sd">        Word Embeddings</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing</span>
<span class="sd">    """</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">NeuralNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_length</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">word_embeddings</span><span class="p">,</span>
                                               <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_length</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>


  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Forward pass of NeuralNet</span>

<span class="sd">    Args:</span>
<span class="sd">      Inputs: list</span>
<span class="sd">        Text</span>

<span class="sd">    Returns:</span>
<span class="sd">      output: torch.tensor</span>
<span class="sd">        Outputs/Predictions</span>
<span class="sd">    """</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>  <span class="c1"># Convert text to embeddings</span>
    <span class="c1">####################################################################</span>
    <span class="c1"># Fill in missing code below (...)</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Fill in the Neural Net"</span><span class="p">)</span>
    <span class="c1">####################################################################</span>
    <span class="c1"># Average the word embeddings in a sentence</span>
    <span class="c1"># Use torch.nn.functional.avg_pool2d to compute the averages</span>
    <span class="n">pooled</span> <span class="o">=</span> <span class="o">...</span>

    <span class="c1"># Pass the embeddings through the neural net</span>
    <span class="c1"># A fully-connected layer</span>
    <span class="n">x</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1"># ReLU activation</span>
    <span class="n">x</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1"># Another fully-connected layer</span>
    <span class="n">x</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span>


<span class="c1"># Add event to airtable</span>
<span class="n">atform</span><span class="o">.</span><span class="n">add_event</span><span class="p">(</span><span class="s1">'Coding Exercise 3.1: Simple Feed Forward Net'</span><span class="p">)</span>

<span class="c1"># Uncomment to check your code</span>
<span class="c1"># nn_model = NeuralNet(2, 128, 100, 300, TEXT.vocab.vectors)</span>
<span class="c1"># print(nn_model)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D3_ModernRecurrentNeuralNetworks/solutions/W2D3_Tutorial1_Solution_ce00961b.py"><em>Click for solution</em></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">NeuralNet</span><span class="p">(</span>
  <span class="p">(</span><span class="n">word_embeddings</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
  <span class="p">(</span><span class="n">fc1</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">(</span><span class="n">fc2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="training-and-testing-functions">
<h3>Training and Testing Functions<a class="headerlink" href="#training-and-testing-functions" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Training and Testing Functions</span>

<span class="c1"># @markdown #### `train(model, device, train_iter, valid_iter, epochs, learning_rate)`</span>
<span class="c1"># @markdown #### `test(model, device, test_iter)`</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">valid_iter</span><span class="p">,</span>
          <span class="n">epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Training pass</span>

<span class="sd">  Args:</span>
<span class="sd">    model: nn.module</span>
<span class="sd">      NeuralNet instance</span>
<span class="sd">    device: string</span>
<span class="sd">      GPU if available, CPU otherwise</span>
<span class="sd">    epochs: int</span>
<span class="sd">      Number of epochs to train model for</span>
<span class="sd">    learning_rate: float</span>
<span class="sd">      Learning rate</span>
<span class="sd">    train_iter: BucketIterator</span>
<span class="sd">      Training iterator</span>
<span class="sd">    valid_iter: BucketIterator</span>
<span class="sd">      Validation iterator</span>

<span class="sd">  Returns:</span>
<span class="sd">    train_loss: list</span>
<span class="sd">      Log of training loss</span>
<span class="sd">    validation_loss: list</span>
<span class="sd">      Log of validation loss</span>
<span class="sd">    train_acc: list</span>
<span class="sd">      Log of training accuracy</span>
<span class="sd">    validation_acc: list</span>
<span class="sd">      Log of validation accuracy</span>
<span class="sd">  """</span>
  <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

  <span class="n">train_loss</span><span class="p">,</span> <span class="n">validation_loss</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
  <span class="n">train_acc</span><span class="p">,</span> <span class="n">validation_acc</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># train</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">correct</span><span class="p">,</span> <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_iter</span><span class="p">):</span>
      <span class="n">text</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">label</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
      <span class="n">text</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

      <span class="c1"># Add micro for coding training loop</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
      <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
      <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

      <span class="c1"># Get accuracy</span>

      <span class="c1"># To get predicted values for each row</span>
      <span class="c1"># across the entire batch</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

      <span class="n">total</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">train_iter</span><span class="p">))</span>
    <span class="n">train_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Epoch: </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1">, '</span>
          <span class="sa">f</span><span class="s1">'Training Loss: </span><span class="si">{</span><span class="n">running_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">train_iter</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">, '</span>
          <span class="sa">f</span><span class="s1">'Training Accuracy: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="si">:</span><span class="s1"> .2f</span><span class="si">}</span><span class="s1">%'</span><span class="p">)</span>

    <span class="c1"># Evaluate on validation data</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">correct</span><span class="p">,</span> <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">valid_iter</span><span class="p">):</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">label</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
        <span class="n">text</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Get accuracy</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">validation_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_iter</span><span class="p">))</span>
    <span class="n">validation_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="p">)</span>

    <span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s1">'Validation Loss: </span><span class="si">{</span><span class="n">running_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_iter</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">, '</span>
           <span class="sa">f</span><span class="s1">'Validation Accuracy: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="si">:</span><span class="s1"> .2f</span><span class="si">}</span><span class="s1">%'</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="n">validation_loss</span><span class="p">,</span> <span class="n">validation_acc</span>


<span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Test loop</span>

<span class="sd">  Args:</span>
<span class="sd">    model: nn.module</span>
<span class="sd">      NeuralNet instance</span>
<span class="sd">    device: string</span>
<span class="sd">      GPU if available,</span>
<span class="sd">    test_iter: BucketIterator</span>
<span class="sd">      Test iterator</span>

<span class="sd">  Returns:</span>
<span class="sd">    acc: float</span>
<span class="sd">      Test Accuracy</span>
<span class="sd">  """</span>
  <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
  <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_iter</span><span class="p">):</span>
      <span class="n">text</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">label</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
      <span class="n">text</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

      <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">total</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">acc</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>
    <span class="k">return</span> <span class="n">acc</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Model hyperparameters</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0003</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">embedding_length</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">vectors</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>

<span class="c1"># Model set-up</span>
<span class="n">nn_model</span> <span class="o">=</span> <span class="n">NeuralNet</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span>
                     <span class="n">hidden_size</span><span class="p">,</span>
                     <span class="n">vocab_size</span><span class="p">,</span>
                     <span class="n">embedding_length</span><span class="p">,</span>
                     <span class="n">word_embeddings</span><span class="p">)</span>
<span class="n">nn_model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">nn_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">522</span><span class="p">)</span>
<span class="n">nn_train_loss</span><span class="p">,</span> <span class="n">nn_train_acc</span><span class="p">,</span> <span class="n">nn_validation_loss</span><span class="p">,</span> <span class="n">nn_validation_acc</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">nn_model</span><span class="p">,</span>
                                                                           <span class="n">DEVICE</span><span class="p">,</span>
                                                                           <span class="n">train_iter</span><span class="p">,</span>
                                                                           <span class="n">valid_iter</span><span class="p">,</span>
                                                                           <span class="n">epochs</span><span class="p">,</span>
                                                                           <span class="n">learning_rate</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"--- Time taken to train = </span><span class="si">{</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">nn_start_time</span><span class="p">)</span><span class="si">}</span><span class="s2"> seconds ---"</span><span class="p">)</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">nn_model</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="se">\n\n</span><span class="s1">Test Accuracy: </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">}</span><span class="s1">%'</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot accuracy curves</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">plot_train_val</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span> <span class="n">nn_train_acc</span><span class="p">,</span> <span class="n">nn_validation_acc</span><span class="p">,</span>
               <span class="s1">'train accuracy'</span><span class="p">,</span> <span class="s1">'val accuracy'</span><span class="p">,</span>
               <span class="s1">'Neural Net on IMDB text classification'</span><span class="p">,</span> <span class="s1">'accuracy'</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="s1">'C0'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">plot_train_val</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span> <span class="n">nn_train_loss</span><span class="p">,</span>
               <span class="n">nn_validation_loss</span><span class="p">,</span>
               <span class="s1">'train loss'</span><span class="p">,</span> <span class="s1">'val loss'</span><span class="p">,</span>
               <span class="s1">''</span><span class="p">,</span>
               <span class="s1">'loss [a.u.]'</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="s1">'C0'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>

</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial, we explored two different concepts linked to sequences, and text in particular, that will be the conceptual foundation for Recurrent Neural Networks.</p>
<p>The first concept was that of sequences and probabilities. We saw how we can model language as sequences of text, and use this analogy to generate text. Such a setup is also used to classify text or identify parts of speech. We can either build chains manually using simple python and numerical computation, or use a package such as <code class="docutils literal notranslate"><span class="pre">hmmlearn</span></code> that allows us to train models a lot easier. These notions of sequences and probabilities (i.e, creating language models!) are key to the internals of a recurrent neural network as well.</p>
<p>The second concept is that of word embeddings, now a mainstay of natural language processing. By using a neural network to predict context of words, these neural networks learn internal representions of words that are a decent approximation of semantic meaning (i.e embeddings!). We saw how these embeddings can be visualised, as well as how they capture meaning. We finally saw how they can be integrated into neural networks to better classify text documents.</p>
<div class="section" id="airtable-submission-link">
<h2>Airtable Submission Link<a class="headerlink" href="#airtable-submission-link" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Airtable Submission Link</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span> <span class="k">as</span> <span class="n">IPydisplay</span>
<span class="n">IPydisplay</span><span class="o">.</span><span class="n">HTML</span><span class="p">(</span>
   <span class="sa">f</span><span class="s2">"""</span>
<span class="s2"> &lt;div&gt;</span>
<span class="s2">   &lt;a href= "</span><span class="si">{</span><span class="n">atform</span><span class="o">.</span><span class="n">url</span><span class="p">()</span><span class="si">}</span><span class="s2">" target="_blank"&gt;</span>
<span class="s2">   &lt;img src="https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/static/AirtableSubmissionButton.png?raw=1"</span>
<span class="s2"> alt="button link to Airtable" style="width:410px"&gt;&lt;/a&gt;</span>
<span class="s2">   &lt;/div&gt;"""</span> <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<div>
<a href="https://portal.neuromatchacademy.org/api/redirect/to/9c55f6cb-cdf9-4429-ac1c-ec44fe64c303?data=eyJmb3JtX2lkIjogImFwcG43VmRQUnNlU29NWEVHIiwgInRhYmxlX25hbWUiOiAiVzJEM19UMSIsICJhbnN3ZXJzIjoge30sICJldmVudHMiOiBbeyJldmVudCI6ICJpbml0IiwgInRzIjogMTY1Mjg5MTk4Ni43ODA1MzN9LCB7ImV2ZW50IjogIlZpZGVvIDE6IFNlcXVlbmNlcyAmIE1hcmtvdiBQcm9jZXNzZXMiLCAidHMiOiAxNjUyODkxOTg5LjYyNjczMzh9LCB7ImV2ZW50IjogIlZpZGVvIDI6IFRleHR1YWwgRGltZW5zaW9uIFJlZHVjdGlvbiIsICJ0cyI6IDE2NTI4OTE5OTEuNTM2MjQ3NX0sIHsiZXZlbnQiOiAiVmlkZW8gMzogU2VtYW50aWMgTWVhc3VyZW1lbnRzIiwgInRzIjogMTY1Mjg5MjAzMC44OTY2MTAzfSwgeyJldmVudCI6ICJDb2RpbmcgRXhlcmNpc2UgMy4xOiBTaW1wbGUgRmVlZCBGb3J3YXJkIE5ldCIsICJ0cyI6IDE2NTI4OTIwMzIuOTAwMzExfSwgeyJldmVudCI6ICJ1cmwgZ2VuZXJhdGVkIiwgInRzIjogMTY1Mjg5MjAzMy4wNTkyNTE1fV19" target="_blank">
<img alt="button link to Airtable" src="https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/static/AirtableSubmissionButton.png?raw=1" style="width:410px"/></a>
</div></div></div>
</div>
</div>
</div>
<script type="application/vnd.jupyter.widget-state+json">
{"state": {"380a57afe2d04e22a9a306f3c4c0d7ff": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7adb4726af604b0090a536fbebe0d5b3": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_380a57afe2d04e22a9a306f3c4c0d7ff", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://www.bilibili.com/video/BV1jg411774B\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<__main__.BiliVideo at 0x7f8a01771b50>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://player.bilibili.com/player.html?bvid=BV1jg411774B&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "}}]}}, "8221cbb72df3455bb1fad6749ff9653d": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d2818acb4cc448729a5c46ca6cc2d019": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_8221cbb72df3455bb1fad6749ff9653d", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://youtube.com/watch?v=ApkE7UFaJAQ\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f8a01775850>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://www.youtube.com/embed/ApkE7UFaJAQ?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ", "image/jpeg": "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\n"}}]}}, "93128fb8f96d4487baf29d146fa6574e": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "4f1e6b2e45934affa1f1ae1767967a6e": {"model_name": "TabModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TabModel", "_titles": {"0": "Youtube", "1": "Bilibili"}, "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TabView", "box_style": "", "children": ["IPY_MODEL_d2818acb4cc448729a5c46ca6cc2d019", "IPY_MODEL_7adb4726af604b0090a536fbebe0d5b3"], "layout": "IPY_MODEL_93128fb8f96d4487baf29d146fa6574e", "selected_index": 0}}, "604bce2bab664739b3ebc436ae92e10e": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "f9dbb7b5895b4d1a994652426ce6cff5": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "7dcb999c70294ef5b2a6c0900ae6ad17": {"model_name": "TextareaModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TextareaModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TextareaView", "continuous_update": true, "description": "", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_604bce2bab664739b3ebc436ae92e10e", "placeholder": "Type something", "rows": null, "style": "IPY_MODEL_f9dbb7b5895b4d1a994652426ce6cff5", "value": "Type your answer here and click on `Submit!`"}}, "5eb4e3edc5b94b5182c418a097fc2f06": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "9503550d58194e91b1f645a8e48ac709": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": null, "font_weight": ""}}, "94c7ef1ba650458981404171a9a3d5c6": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "Submit!", "disabled": false, "icon": "", "layout": "IPY_MODEL_5eb4e3edc5b94b5182c418a097fc2f06", "style": "IPY_MODEL_9503550d58194e91b1f645a8e48ac709", "tooltip": ""}}, "963b3abbaecd4c5197ef8b2de4568544": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "cf748f8e8c1d49e1a68d755dafce3e68": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_963b3abbaecd4c5197ef8b2de4568544", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://www.bilibili.com/video/BV1oM4y1P7Mn\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<__main__.BiliVideo at 0x7f8a01686fd0>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://player.bilibili.com/player.html?bvid=BV1oM4y1P7Mn&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "}}]}}, "27210f890dc04bc6b37436cb43b21164": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "dd7d830bf79d46b5a6eeb97708b1705b": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_27210f890dc04bc6b37436cb43b21164", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://youtube.com/watch?v=kweySXAZ1os\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f8a01775850>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://www.youtube.com/embed/kweySXAZ1os?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ", "image/jpeg": "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\n"}}]}}, "e6fcb392312648ff925b02991aa5a814": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "f31d9393324b450e86f3928206940bec": {"model_name": "TabModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TabModel", "_titles": {"0": "Youtube", "1": "Bilibili"}, "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TabView", "box_style": "", "children": ["IPY_MODEL_dd7d830bf79d46b5a6eeb97708b1705b", "IPY_MODEL_cf748f8e8c1d49e1a68d755dafce3e68"], "layout": "IPY_MODEL_e6fcb392312648ff925b02991aa5a814", "selected_index": 0}}, "a273d975078348f5a18763fba52407cf": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "6a02d06ecdb344b4aefb813e91e34dfb": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_a273d975078348f5a18763fba52407cf", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://www.bilibili.com/video/BV15w411R7SW\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<__main__.BiliVideo at 0x7f8a016b9ad0>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://player.bilibili.com/player.html?bvid=BV15w411R7SW&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "}}]}}, "b509b45f5e444038a17ff58e8638eef6": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "32335e192f69475bb9d8a297d273039a": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_b509b45f5e444038a17ff58e8638eef6", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://youtube.com/watch?v=Y45KIAOw4OY\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f8a01652c10>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://www.youtube.com/embed/Y45KIAOw4OY?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ", "image/jpeg": "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\n"}}]}}, "d1b82dd648b4419497b5bf228b13da3b": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d82974f823a147ce946f495f01136c28": {"model_name": "TabModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TabModel", "_titles": {"0": "Youtube", "1": "Bilibili"}, "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TabView", "box_style": "", "children": ["IPY_MODEL_32335e192f69475bb9d8a297d273039a", "IPY_MODEL_6a02d06ecdb344b4aefb813e91e34dfb"], "layout": "IPY_MODEL_d1b82dd648b4419497b5bf228b13da3b", "selected_index": 0}}}, "version_major": 2, "version_minor": 0}
</script>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tutorials/W2D3_ModernRecurrentNeuralNetworks/student"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</div>
</main>
<footer class="footer-article noprint">
<!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev" href="../chapter_title.html" id="prev-link" title="previous page">
<i class="fas fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Modern Recurrent Neural Networks</p>
</div>
</a>
<a class="right-next" href="W2D3_Tutorial2.html" id="next-link" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Tutorial 2: Modern RNNs and their variants</p>
</div>
<i class="fas fa-angle-right"></i>
</a>
</div>
</footer>
</div>
</div>
<div class="footer-content row">
<footer class="col footer"><p>
  
    By Neuromatch<br/>
  
      © Copyright 2021.<br/>
</p>
</footer>
</div>
</div>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
</body>
</html>