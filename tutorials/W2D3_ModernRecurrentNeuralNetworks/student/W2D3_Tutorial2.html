
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Tutorial 2: Modern RNNs and their variants — Neuromatch Academy: Deep Learning</title>
<!-- Loaded before other Sphinx assets -->
<link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/styles/sphinx-book-theme.css" rel="stylesheet" type="text/css">
<link href="../../../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/mystnb.css" rel="stylesheet" type="text/css">
<link href="../../../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf" rel="preload"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/togglebutton.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script src="../../../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
<script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
<script async="async" src="../../../_static/sphinx-thebe.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
<link href="../../../_static/nma-dl-logo-square-4xp.jpeg" rel="shortcut icon">
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
<link href="../../W2D4_AttentionAndTransformers/chapter_title.html" rel="next" title="Attention And Transformers"/>
<link href="W2D3_Tutorial1.html" rel="prev" title="Tutorial 1: Modeling sequencies and encoding text"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="None" name="docsearch:language"/>
<!-- Google Analytics -->
</link></link></link></link></link></link></head>
<body data-offset="60" data-spy="scroll" data-target="#bd-toc-nav">
<!-- Checkboxes to toggle the left sidebar -->
<input aria-label="Toggle navigation sidebar" class="sidebar-toggle" id="__navigation" name="__navigation" type="checkbox"/>
<label class="overlay overlay-navbar" for="__navigation">
<div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input aria-label="Toggle in-page Table of Contents" class="sidebar-toggle" id="__page-toc" name="__page-toc" type="checkbox"/>
<label class="overlay overlay-pagetoc" for="__page-toc">
<div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
<div class="bd-sidebar__content">
<div class="bd-sidebar__top"><div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
<!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
<img alt="logo" class="logo" src="../../../_static/nma-dl-logo-square-4xp.jpeg"/>
<h1 class="site-logo" id="site-title">Neuromatch Academy: Deep Learning</h1>
</a>
</div><form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search this book..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." type="search"/>
</form><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<ul class="nav bd-sidenav bd-sidenav__home-link">
<li class="toctree-l1">
<a class="reference internal" href="../../intro.html">
                    Introduction
                </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Schedule/schedule_intro.html">
   Schedule
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/daily_schedules.html">
     General schedule
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/shared_calendars.html">
     Shared calendars
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/timezone_widget.html">
     Timezone widget
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../TechnicalHelp/tech_intro.html">
   Technical Help
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox">
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../TechnicalHelp/Jupyterbook.html">
     Using jupyterbook
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox">
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_colab.html">
       Using Google Colab
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_kaggle.html">
       Using Kaggle
      </a>
</li>
</ul>
</input></li>
<li class="toctree-l2">
<a class="reference internal" href="../../TechnicalHelp/Discord.html">
     Using Discord
    </a>
</li>
</ul>
</input></li>
</ul>
<p class="caption">
<span class="caption-text">
  The Basics
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D1_BasicsAndPytorch/chapter_title.html">
   Basics And Pytorch (W1D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_BasicsAndPytorch/student/W1D1_Tutorial1.html">
     Tutorial 1: PyTorch
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/chapter_title.html">
   Linear Deep Learning (W1D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial1.html">
     Tutorial 1: Gradient Descent and AutoGrad
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial2.html">
     Tutorial 2: Learning Hyperparameters
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial3.html">
     Tutorial 3: Deep linear neural networks
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/chapter_title.html">
   Multi Layer Perceptrons (W1D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial1.html">
     Tutorial 1: Biological vs. Artificial Neural Networks
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial2.html">
     Tutorial 2: Deep MLPs
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D4_Optimization/chapter_title.html">
   Optimization (W1D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_Optimization/student/W1D4_Tutorial1.html">
     Tutorial 1: Optimization techniques
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D5_Regularization/chapter_title.html">
   Regularization (W1D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
<label for="toctree-checkbox-8">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_Regularization/student/W1D5_Tutorial1.html">
     Tutorial 1: Regularization techniques part 1
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_Regularization/student/W1D5_Tutorial2.html">
     Tutorial 2: Regularization techniques part 2
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/TheBasics.html">
   Deep Learning: The Basics Wrap-up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Doing More With Fewer Parameters
 </span>
</p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/chapter_title.html">
   Convnets And Recurrent Neural Networks (W2D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
<label for="toctree-checkbox-9">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial1.html">
     Tutorial 1: Introduction to CNNs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial2.html">
     Tutorial 2: Introduction to RNNs
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D2_ModernConvnets/chapter_title.html">
   Modern Convnets (W2D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
<label for="toctree-checkbox-10">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_ModernConvnets/student/W2D2_Tutorial1.html">
     Tutorial 1: Learn how to use modern convnets
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_ModernConvnets/student/W2D2_Tutorial2.html">
     (Bonus) Tutorial 2: Facial recognition using modern convnets
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 current active has-children">
<a class="reference internal" href="../chapter_title.html">
   Modern Recurrent Neural Networks (W2D3)
  </a>
<input checked="" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
<label for="toctree-checkbox-11">
<i class="fas fa-chevron-down">
</i>
</label>
<ul class="current">
<li class="toctree-l2">
<a class="reference internal" href="W2D3_Tutorial1.html">
     Tutorial 1: Modeling sequencies and encoding text
    </a>
</li>
<li class="toctree-l2 current active">
<a class="current reference internal" href="#">
     Tutorial 2: Modern RNNs and their variants
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D4_AttentionAndTransformers/chapter_title.html">
   Attention And Transformers (W2D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
<label for="toctree-checkbox-12">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_AttentionAndTransformers/student/W2D4_Tutorial1.html">
     Tutorial 1: Learn how to work with Transformers
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D5_GenerativeModels/chapter_title.html">
   Generative Models (W2D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
<label for="toctree-checkbox-13">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial1.html">
     Tutorial 1: Variational Autoencoders (VAEs)
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial2.html">
     Tutorial 2: Introduction to GANs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial3.html">
     Tutorial 3: Conditional GANs and Implications of GAN Technology
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial4.html">
     (Bonus) Tutorial 4: Deploying Neural Networks on the Web
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/DoingMoreWithFewerParameters.html">
   Deep Learning: Doing more with fewer parameters Wrap-up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Advanced Topics
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D1_UnsupervisedAndSelfSupervisedLearning/chapter_title.html">
   Unsupervised And Self Supervised Learning (W3D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
<label for="toctree-checkbox-14">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_UnsupervisedAndSelfSupervisedLearning/student/W3D1_Tutorial1.html">
     Tutorial 1: Un/Self-supervised learning methods
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D2_BasicReinforcementLearning/chapter_title.html">
   Basic Reinforcement Learning (W3D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
<label for="toctree-checkbox-15">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_BasicReinforcementLearning/student/W3D2_Tutorial1.html">
     Tutorial 1: Learning to Predict
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D3_ReinforcementLearningForGames/chapter_title.html">
   Reinforcement Learning For Games (W3D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
<label for="toctree-checkbox-16">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_ReinforcementLearningForGames/student/W3D3_Tutorial1.html">
     Tutorial 1: Learn to play games with RL
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D4_ContinualLearning/chapter_title.html">
   Continual Learning (W3D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
<label for="toctree-checkbox-17">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ContinualLearning/student/W3D4_Tutorial1.html">
     Tutorial 1: Introduction to Continual Learning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ContinualLearning/student/W3D4_Tutorial2.html">
     Tutorial 2: Out-of-distribution (OOD) Learning
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/AdvancedTopics.html">
   Deep Learning: Advanced Topics Wrap-up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Project Booklet
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/README.html">
   Introduction to projects
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_guidance.html">
   Daily guide for projects
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/modelingsteps/intro.html">
   Modeling Step-by-Step Guide
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
<label for="toctree-checkbox-18">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_1through2_DL.html">
     Modeling Steps 1 - 2
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_3through4_DL.html">
     Modeling Steps 3 - 4
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_5through6_DL.html">
     Modeling Steps 5 - 6
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_7through9_DL.html">
     Modeling Steps 7 - 9
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_10_DL.html">
     Modeling Steps 10
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionDataProjectDL.html">
     Example Data Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionModelingProjectDL.html">
     Example Model Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/Example_Deep_Learning_Project.html">
     Example Deep Learning Project
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/projects_overview.html">
   Project Templates
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
<label for="toctree-checkbox-19">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/ComputerVision/README.html">
     Computer Vision
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
<label for="toctree-checkbox-20">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/em_synapses.html">
       Knowledge Extraction from a Convolutional Neural Network
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/spectrogram_analysis.html">
       Music classification and generation with spectrograms
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/screws.html">
       Something Screwy - image recognition, detection, and classification of screws
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/image_alignment.html">
       Image Alignment
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/data_augmentation.html">
       Data Augmentation in image classification models
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/transfer_learning.html">
       Transfer Learning
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/ReinforcementLearning/README.html">
     Reinforcement Learning
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
<label for="toctree-checkbox-21">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/robolympics.html">
       NMA Robolympics: Controlling robots using reinforcement learning
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/lunar_lander.html">
       Performance Analysis of DQN Algorithm on the Lunar Lander task
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/human_rl.html">
       Using RL to Model Cognitive Tasks
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/README.html">
     Natural Language Processing
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
<label for="toctree-checkbox-22">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/sentiment_analysis.html">
       Twitter Sentiment Analysis
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/machine_translation.html">
       Machine Translation
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/Neuroscience/README.html">
     Neuroscience
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
<label for="toctree-checkbox-23">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/pose_estimation.html">
       Animal Pose Estimation
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/cellular_segmentation.html">
       Segmentation and Denoising
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/algonauts_videos.html">
       Load algonauts videos
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/blurry_vision.html">
       Vision with Lost Glasses: Modelling how the brain deals with noisy input
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/finetuning_fmri.html">
       Moving beyond Labels: Finetuning CNNs on BOLD response
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/neuro_seq_to_seq.html">
       Focus on what matters: inferring low-dimensional dynamics from neural recordings
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/datasets_and_models.html">
   Models and Data sets
  </a>
</li>
</ul>
</div>
</nav></div>
<div class="bd-sidebar__bottom">
<!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
</div>
</div>
<div id="rtd-footer-container"></div>
</div>
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
<div class="header-article row sticky-top noprint">
<div class="col py-1 d-flex header-article-main">
<div class="header-article__left">
<label class="headerbtn" data-placement="right" data-toggle="tooltip" for="__navigation" title="Toggle navigation">
<span class="headerbtn__icon-container">
<i class="fas fa-bars"></i>
</span>
</label>
</div>
<div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
<button aria-label="Launch interactive content" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-rocket"></i>
</button>
<div class="menu-dropdown__content">
<ul>
</ul>
</div>
</div>
<button class="headerbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="headerbtn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<div class="menu-dropdown menu-dropdown-repository-buttons">
<button aria-label="Source repositories" class="headerbtn menu-dropdown__trigger">
<i class="fab fa-github"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="https://github.com/NeuromatchAcademy/course-content-dl" title="Source repository">
<span class="headerbtn__icon-container">
<i class="fab fa-github"></i>
</span>
<span class="headerbtn__text-container">repository</span>
</a>
</li>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="https://github.com/NeuromatchAcademy/course-content-dl/issues/new?title=Issue%20on%20page%20%2Ftutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial2.html&amp;body=Your%20issue%20content%20here." title="Open an issue">
<span class="headerbtn__icon-container">
<i class="fas fa-lightbulb"></i>
</span>
<span class="headerbtn__text-container">open issue</span>
</a>
</li>
</ul>
</div>
</div>
<div class="menu-dropdown menu-dropdown-download-buttons">
<button aria-label="Download this page" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-download"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="../../../_sources/tutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial2.ipynb" title="Download source file">
<span class="headerbtn__icon-container">
<i class="fas fa-file"></i>
</span>
<span class="headerbtn__text-container">.ipynb</span>
</a>
</li>
<li>
<button class="headerbtn" data-placement="left" data-toggle="tooltip" onclick="printPdf(this)" title="Print to PDF">
<span class="headerbtn__icon-container">
<i class="fas fa-file-pdf"></i>
</span>
<span class="headerbtn__text-container">.pdf</span>
</button>
</li>
</ul>
</div>
</div>
<label class="headerbtn headerbtn-page-toc" for="__page-toc">
<span class="headerbtn__icon-container">
<i class="fas fa-list"></i>
</span>
</label>
</div>
</div>
<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
    </div>
<nav aria-label="Page" id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 2: Modern RNNs and their variants
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial objectives
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-slides">
     Tutorial slides
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#install-dependecies">
     Install dependecies
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure Settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#download-the-dataset">
     Download the dataset
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
     Helper functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-random-seed">
     Set random seed
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-device-gpu-or-cpu-execute-set-device">
     Set device (GPU or CPU). Execute
     <code class="docutils literal notranslate">
<span class="pre">
       set_device()
      </span>
</code>
</a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-recurrent-neural-networks-rnns">
   Section 1: Recurrent Neural Networks (RNNs)
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-recurrent-neural-networks">
     Video 1: Recurrent Neural Networks
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-1-load-and-view-of-the-dataset">
     Section 1.1: Load and View of the dataset
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-1-1-vanilla-rnn">
       Coding Exercise 1.1: Vanilla RNN
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-2-train-and-test-the-network">
     Section 1.2: Train and test the network
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#change-the-input-length">
       Change the input length
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#re-run-the-network">
       Re-run the network
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-3-architectures">
     Section 1.3: Architectures
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-bidirectional-rnns">
       Video 2: Bidirectional RNNs
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-4-vanishing-and-exploding-gradients">
     Section 1.4: Vanishing and Exploding Gradients
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-lstm-gru-and-memory-cell">
   Section 2: LSTM, GRU and Memory Cell
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-3-lstm-gru-the-memory-cells">
     Video 3: LSTM, GRU &amp; The Memory Cells
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-1-architecture">
     Section 2.1: Architecture
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-1-implementing-lstm">
       Coding Exercise 2.1: Implementing LSTM
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-gated-recurrent-units-gru">
     Section 2.2: Gated Recurrent Units (GRU)
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-2-bilstm">
       Coding Exercise 2.2: BiLSTM
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-sequence-to-sequence-seq2seq-encoder-decoder-networks">
   Section 3: Sequence to Sequence (Seq2Seq) &amp; Encoder/ Decoder Networks
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-4-seq2seq-encoder-decoder-nets">
     Video 4: Seq2Seq &amp; Encoder-Decoder Nets
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-3-encoder">
     Coding Exercise 3: Encoder
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-1-decoder">
     Section 3.1: Decoder
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-2-loss-function">
     Section 3.2: Loss Function
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#training">
       Training
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#prediction">
       Prediction
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#evaluation-of-predicted-sequences">
       Evaluation of Predicted Sequences
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-4-ethical-aspects">
   Section 4: Ethical aspects
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-5-ethics-of-representation-and-generation">
     Video 5: Ethics of Representation and Generation
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#summary">
   Summary
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-6-beyond-sequence">
     Video 6: Beyond Sequence
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#airtable-submission-link">
     Airtable Submission Link
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-attention">
   Bonus: Attention
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-7-attention-mechanisms">
     Video 7: Attention mechanisms
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#queries-keys-and-values">
     Queries, Keys, and Values
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-coding-exercise-attention-for-text-classification">
       Bonus Coding Exercise: Attention for Text Classification
      </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class="article row">
<div class="col pl-md-3 pl-lg-5 content-container">
<!-- Table of contents that is only displayed when printing the page -->
<div class="onlyprint" id="jb-print-docs-body">
<h1>Tutorial 2: Modern RNNs and their variants</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 2: Modern RNNs and their variants
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial objectives
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-slides">
     Tutorial slides
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#install-dependecies">
     Install dependecies
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure Settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#download-the-dataset">
     Download the dataset
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
     Helper functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-random-seed">
     Set random seed
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-device-gpu-or-cpu-execute-set-device">
     Set device (GPU or CPU). Execute
     <code class="docutils literal notranslate">
<span class="pre">
       set_device()
      </span>
</code>
</a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-recurrent-neural-networks-rnns">
   Section 1: Recurrent Neural Networks (RNNs)
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-recurrent-neural-networks">
     Video 1: Recurrent Neural Networks
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-1-load-and-view-of-the-dataset">
     Section 1.1: Load and View of the dataset
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-1-1-vanilla-rnn">
       Coding Exercise 1.1: Vanilla RNN
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-2-train-and-test-the-network">
     Section 1.2: Train and test the network
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#change-the-input-length">
       Change the input length
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#re-run-the-network">
       Re-run the network
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-3-architectures">
     Section 1.3: Architectures
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-bidirectional-rnns">
       Video 2: Bidirectional RNNs
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-4-vanishing-and-exploding-gradients">
     Section 1.4: Vanishing and Exploding Gradients
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-lstm-gru-and-memory-cell">
   Section 2: LSTM, GRU and Memory Cell
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-3-lstm-gru-the-memory-cells">
     Video 3: LSTM, GRU &amp; The Memory Cells
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-1-architecture">
     Section 2.1: Architecture
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-1-implementing-lstm">
       Coding Exercise 2.1: Implementing LSTM
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-gated-recurrent-units-gru">
     Section 2.2: Gated Recurrent Units (GRU)
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-2-bilstm">
       Coding Exercise 2.2: BiLSTM
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-sequence-to-sequence-seq2seq-encoder-decoder-networks">
   Section 3: Sequence to Sequence (Seq2Seq) &amp; Encoder/ Decoder Networks
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-4-seq2seq-encoder-decoder-nets">
     Video 4: Seq2Seq &amp; Encoder-Decoder Nets
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-3-encoder">
     Coding Exercise 3: Encoder
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-1-decoder">
     Section 3.1: Decoder
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-2-loss-function">
     Section 3.2: Loss Function
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#training">
       Training
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#prediction">
       Prediction
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#evaluation-of-predicted-sequences">
       Evaluation of Predicted Sequences
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-4-ethical-aspects">
   Section 4: Ethical aspects
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-5-ethics-of-representation-and-generation">
     Video 5: Ethics of Representation and Generation
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#summary">
   Summary
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-6-beyond-sequence">
     Video 6: Beyond Sequence
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#airtable-submission-link">
     Airtable Submission Link
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-attention">
   Bonus: Attention
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-7-attention-mechanisms">
     Video 7: Attention mechanisms
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#queries-keys-and-values">
     Queries, Keys, and Values
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-coding-exercise-attention-for-text-classification">
       Bonus Coding Exercise: Attention for Text Classification
      </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
</div>
<main id="main-content" role="main">
<div>
<p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial2.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a>   <a href="https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial2.ipynb" target="_blank"><img alt="Open in Kaggle" src="https://kaggle.com/static/images/open-in-kaggle.svg"/></a></p>
<div class="section" id="tutorial-2-modern-rnns-and-their-variants">
<h1>Tutorial 2: Modern RNNs and their variants<a class="headerlink" href="#tutorial-2-modern-rnns-and-their-variants" title="Permalink to this headline">¶</a></h1>
<p><strong>Week 2, Day 3: Modern RNNs</strong></p>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators:</strong> Bhargav Srinivasa Desikan, Anis Zahedifard, James Evans</p>
<p><strong>Content reviewers:</strong> Lily Cheng, Melvin Selim Atay, Ezekiel Williams, Kelson Shilling-Scrivo</p>
<p><strong>Content editors:</strong> Gagana B, Spiros Chavlis</p>
<p><strong>Production editors:</strong> Roberto Guidotti, Gagana B, Spiros Chavlis</p>
<p><strong>Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs</strong></p>
<p align="center"><img src="https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True"/></p></div>
<hr class="docutils"/>
<div class="section" id="tutorial-objectives">
<h1>Tutorial objectives<a class="headerlink" href="#tutorial-objectives" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial you will learn about:</p>
<ol class="simple">
<li><p>Modern Recurrent Neural Networks and their use</p></li>
<li><p>Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU) and the memory cell</p></li>
<li><p>Sequence to Sequence and Encoder-Decoder Networks</p></li>
<li><p>Models of Attention for text classification</p></li>
</ol>
<div class="section" id="tutorial-slides">
<h2>Tutorial slides<a class="headerlink" href="#tutorial-slides" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<iframe allowfullscreen="" frameborder="0" height="480" src="https://mfr.ca-1.osf.io/render?url=https://osf.io/n23hy/?direct%26mode=render%26action=download%26mode=render" width="854"></iframe>
</div></div>
</div>
<p>These are the slides for the videos in this tutorials. If you want to locally download the slides, click <a class="reference external" href="https://osf.io/n23hy/download">here</a>.</p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<p>We will use the IMDB dataset, which consists of a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. We will use torchtext to download the dataset and prepare it for training, validation and testing. Our goal is to build a model that performs binary classification between positive and negative movie reviews.</p>
<p>We use <code class="docutils literal notranslate"><span class="pre">fix_length</span></code> argument to pad sentences of length less than <code class="docutils literal notranslate"><span class="pre">sentence_length</span></code> or truncate sentences of length greater than <code class="docutils literal notranslate"><span class="pre">sentence_length</span></code>.</p>
<div class="section" id="install-dependecies">
<h2>Install dependecies<a class="headerlink" href="#install-dependecies" title="Permalink to this headline">¶</a></h2>
<p>There may be <em>errors</em> and/or <em>warnings</em> reported during the installation. However, they are to be ignored.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Install dependecies</span>

<span class="c1"># @markdown There may be *errors* and/or *warnings* reported during the installation. However, they are to be ignored.</span>
<span class="c1"># !pip install torch==1.8.0 torchtext==0.9.0 torchaudio==0.8.0 torchvision==0.9.0 --quiet</span>
<span class="o">!</span>pip install <span class="nv">torch</span><span class="o">==</span><span class="m">1</span>.8.0+cu111 <span class="nv">torchvision</span><span class="o">==</span><span class="m">0</span>.9.0+cu111 <span class="nv">torchaudio</span><span class="o">==</span><span class="m">0</span>.8.0 <span class="nv">torchtext</span><span class="o">==</span><span class="m">0</span>.9.0 -f https://download.pytorch.org/whl/torch_stable.html --quiet

<span class="o">!</span>pip install nltk --quiet
<span class="o">!</span>pip install <span class="nv">d2l</span><span class="o">==</span><span class="m">0</span>.16.5 --quiet

<span class="o">!</span>pip install git+https://github.com/NeuromatchAcademy/evaltools --quiet
<span class="kn">from</span> <span class="nn">evaltools.airtable</span> <span class="kn">import</span> <span class="n">AirtableForm</span>

<span class="n">atform</span> <span class="o">=</span> <span class="n">AirtableForm</span><span class="p">(</span><span class="s1">'appn7VdPRseSoMXEG'</span><span class="p">,</span><span class="s1">'W2D3_T2'</span><span class="p">,</span><span class="s1">'https://portal.neuromatchacademy.org/api/redirect/to/3412a777-eb0e-4312-9254-eec266f0bee4'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">collections</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="kn">from</span> <span class="nn">torchtext.legacy</span> <span class="kn">import</span> <span class="n">data</span><span class="p">,</span> <span class="n">datasets</span>

<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">d2l</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="figure-settings">
<h2>Figure Settings<a class="headerlink" href="#figure-settings" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Figure Settings</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="download-the-dataset">
<h2>Download the dataset<a class="headerlink" href="#download-the-dataset" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Download the dataset</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">'punkt'</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">'averaged_perceptron_tagger'</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">'brown'</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">'webtext'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="helper-functions">
<h2>Helper functions<a class="headerlink" href="#helper-functions" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Helper functions</span>

<span class="k">def</span> <span class="nf">plot_train_val</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">train_label</span><span class="p">,</span>
                   <span class="n">val_label</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">y_label</span><span class="p">,</span>
                   <span class="n">color</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Plots training/validation performance per epoch</span>

<span class="sd">  Args:</span>
<span class="sd">    x: np.ndarray</span>
<span class="sd">      Input data</span>
<span class="sd">    train: list</span>
<span class="sd">      Training data performance</span>
<span class="sd">    val: list</span>
<span class="sd">      Validation data performance</span>
<span class="sd">    train_label: string</span>
<span class="sd">      Train Label [specifies training criterion]</span>
<span class="sd">    color: string</span>
<span class="sd">      Specifies color of plot</span>
<span class="sd">    val_label: string</span>
<span class="sd">      Validation Label [specifies validation criterion]</span>
<span class="sd">    title: string</span>
<span class="sd">      Specifies title of plot</span>
<span class="sd">    y_label: string</span>
<span class="sd">      Specifies performance criterion</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">train_label</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">val_label</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'lower right'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'epoch'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">y_label</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">count_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Helper function to count parameters</span>

<span class="sd">  Args:</span>
<span class="sd">    model: nn.module</span>
<span class="sd">      NeuralNet instance</span>

<span class="sd">  Returns:</span>
<span class="sd">    parameters: int</span>
<span class="sd">      Number of parameters in model</span>
<span class="sd">  """</span>
  <span class="n">parameters</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">parameters</span>

<span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Helper function to initialize weights</span>

<span class="sd">  Args:</span>
<span class="sd">    m: nn.module</span>
<span class="sd">      Type of layer</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="ow">in</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">):</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="n">sentence_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">2021</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Dataset Loader</span>

<span class="sd">  Args:</span>
<span class="sd">    sentence_length: int</span>
<span class="sd">      Length of sentence</span>
<span class="sd">    seed: int</span>
<span class="sd">      Set seed for reproducibility</span>
<span class="sd">    batch_size: int</span>
<span class="sd">      Batch size</span>

<span class="sd">  Returns:</span>
<span class="sd">    TEXT: Field instance</span>
<span class="sd">      Text</span>
<span class="sd">    vocab_size: int</span>
<span class="sd">      Specifies size of TEXT</span>
<span class="sd">    train_iter: BucketIterator</span>
<span class="sd">      Training iterator</span>
<span class="sd">    valid_iter: BucketIterator</span>
<span class="sd">      Validation iterator</span>
<span class="sd">    test_iter: BucketIterator</span>
<span class="sd">      Test iterator</span>
<span class="sd">  """</span>
  <span class="n">TEXT</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">sequential</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">tokenize</span><span class="o">=</span><span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">,</span>
                    <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">include_lengths</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">fix_length</span><span class="o">=</span><span class="n">sentence_length</span><span class="p">)</span>
  <span class="n">LABEL</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">LabelField</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

  <span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">IMDB</span><span class="o">.</span><span class="n">splits</span><span class="p">(</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">LABEL</span><span class="p">)</span>

  <span class="c1"># If no specific vector embeddings are specified,</span>
  <span class="c1"># Torchtext initializes random vector embeddings</span>
  <span class="c1"># which would get updated during training through backpropagation.</span>
  <span class="n">TEXT</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
  <span class="n">LABEL</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>

  <span class="n">train_data</span><span class="p">,</span> <span class="n">valid_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">split_ratio</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
                                            <span class="n">random_state</span><span class="o">=</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">))</span>
  <span class="n">train_iter</span><span class="p">,</span> <span class="n">valid_iter</span><span class="p">,</span> <span class="n">test_iter</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">BucketIterator</span><span class="o">.</span><span class="n">splits</span><span class="p">((</span><span class="n">train_data</span><span class="p">,</span> <span class="n">valid_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">),</span>
                                                                  <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sort_key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">text</span><span class="p">),</span>
                                                                  <span class="n">repeat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Data loading is completed. Sentence length: </span><span class="si">{</span><span class="n">sentence_length</span><span class="si">}</span><span class="s2">, "</span>
        <span class="sa">f</span><span class="s2">"Batch size: </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">, and seed: </span><span class="si">{</span><span class="n">seed</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">TEXT</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">valid_iter</span><span class="p">,</span> <span class="n">test_iter</span>


<span class="k">def</span> <span class="nf">text_from_dict</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Helper function to extract text from dictionary</span>

<span class="sd">  Args:</span>
<span class="sd">    dictionary: dict</span>
<span class="sd">      Dictionary of words and corresponding indices</span>
<span class="sd">    arr: list</span>
<span class="sd">      Sequence of words</span>

<span class="sd">  Returns:</span>
<span class="sd">    text: list</span>
<span class="sd">      Log of keys from dictionary</span>
<span class="sd">  """</span>
  <span class="n">text</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">arr</span><span class="p">:</span>
    <span class="n">text</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dictionary</span><span class="p">[</span><span class="n">element</span><span class="p">])</span>
  <span class="k">return</span> <span class="n">text</span>

<span class="k">def</span> <span class="nf">view_data</span><span class="p">(</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Helper function to view data</span>

<span class="sd">  Args:</span>
<span class="sd">    TEXT: Field instance</span>
<span class="sd">      Text</span>
<span class="sd">    train_iter: BucketIterator</span>
<span class="sd">      Training iterator</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_iter</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">label</span>

    <span class="k">for</span> <span class="n">itr</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">30</span><span class="p">):</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">'Review: '</span><span class="p">,</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text_from_dict</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="n">itr</span><span class="p">],</span> <span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">itos</span><span class="p">)))</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">'Label: '</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">target</span><span class="p">[</span><span class="n">itr</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()),</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'[0: Negative Review, 1: Positive Review]'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">idx</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
      <span class="k">break</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">valid_iter</span><span class="p">,</span>
          <span class="n">epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Training function</span>

<span class="sd">  Args:</span>
<span class="sd">    model: nn.module</span>
<span class="sd">      NeuralNet instance</span>
<span class="sd">    device: string</span>
<span class="sd">      GPU if available, CPU otherwise</span>
<span class="sd">    epochs: int</span>
<span class="sd">      Number of epochs to train model for</span>
<span class="sd">    learning_rate: float</span>
<span class="sd">      Learning rate</span>
<span class="sd">    train_iter: BucketIterator</span>
<span class="sd">      Training iterator</span>
<span class="sd">    valid_iter: BucketIterator</span>
<span class="sd">      Validation iterator</span>

<span class="sd">  Returns:</span>
<span class="sd">    train_loss: list</span>
<span class="sd">      Log of training loss</span>
<span class="sd">    validation_loss: list</span>
<span class="sd">      Log of validation loss</span>
<span class="sd">    train_acc: list</span>
<span class="sd">      Log of training accuracy</span>
<span class="sd">    validation_acc: list</span>
<span class="sd">      Log of validation accuracy</span>
<span class="sd">  """</span>
  <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

  <span class="n">train_loss</span><span class="p">,</span> <span class="n">validation_loss</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
  <span class="n">train_acc</span><span class="p">,</span> <span class="n">validation_acc</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># Train</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">correct</span><span class="p">,</span> <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_iter</span><span class="p">):</span>
      <span class="n">text</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">label</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
      <span class="n">text</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

      <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

      <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
      <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
      <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

      <span class="c1"># Get accuracy</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">total</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">train_iter</span><span class="p">))</span>
    <span class="n">train_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Epoch: </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1">, '</span>
          <span class="sa">f</span><span class="s1">'Training Loss: </span><span class="si">{</span><span class="n">running_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">train_iter</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">, '</span>
          <span class="sa">f</span><span class="s1">'Training Accuracy: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="si">:</span><span class="s1"> .2f</span><span class="si">}</span><span class="s1">%'</span><span class="p">)</span>

    <span class="c1"># Evaluate on validation data</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">correct</span><span class="p">,</span> <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">valid_iter</span><span class="p">):</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">label</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
        <span class="n">text</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># get accuracy</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">validation_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_iter</span><span class="p">))</span>
    <span class="n">validation_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="p">)</span>

    <span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s1">'Validation Loss: </span><span class="si">{</span><span class="n">running_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_iter</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">, '</span>
           <span class="sa">f</span><span class="s1">'Validation Accuracy: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="si">:</span><span class="s1"> .2f</span><span class="si">}</span><span class="s1">%'</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="n">validation_loss</span><span class="p">,</span> <span class="n">validation_acc</span>


<span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Testing function</span>

<span class="sd">  Args:</span>
<span class="sd">    model: nn.module</span>
<span class="sd">      NeuralNet instance</span>
<span class="sd">    device: string</span>
<span class="sd">      GPU if available,</span>
<span class="sd">    test_iter: BucketIterator</span>
<span class="sd">      Test iterator</span>

<span class="sd">  Returns:</span>
<span class="sd">    acc: float</span>
<span class="sd">      Test Accuracy</span>
<span class="sd">  """</span>
  <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
  <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_iter</span><span class="p">):</span>
      <span class="n">text</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">label</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
      <span class="n">text</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

      <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">total</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">acc</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>
    <span class="k">return</span> <span class="n">acc</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="set-random-seed">
<h2>Set random seed<a class="headerlink" href="#set-random-seed" title="Permalink to this headline">¶</a></h2>
<p>Executing <code class="docutils literal notranslate"><span class="pre">set_seed(seed=seed)</span></code> you are setting the seed</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set random seed</span>

<span class="c1"># @markdown Executing `set_seed(seed=seed)` you are setting the seed</span>

<span class="c1"># For DL its critical to set the random seed so that students can have a</span>
<span class="c1"># baseline to compare their results to expected results.</span>
<span class="c1"># Read more here: https://pytorch.org/docs/stable/notes/randomness.html</span>

<span class="c1"># Call `set_seed` function in the exercises to ensure reproducibility.</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Function that controls randomness.</span>
<span class="sd">  NumPy and random modules must be imported.</span>

<span class="sd">  Args:</span>
<span class="sd">    seed : Integer</span>
<span class="sd">      A non-negative integer that defines the random state. Default is `None`.</span>
<span class="sd">    seed_torch : Boolean</span>
<span class="sd">      If `True` sets the random seed for pytorch tensors, so pytorch module</span>
<span class="sd">      must be imported. Default is `True`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>
  <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">32</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">seed_torch</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Random seed </span><span class="si">{</span><span class="n">seed</span><span class="si">}</span><span class="s1"> has been set.'</span><span class="p">)</span>

<span class="c1"># In case that `DataLoader` is used</span>
<span class="k">def</span> <span class="nf">seed_worker</span><span class="p">(</span><span class="n">worker_id</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  DataLoader will reseed workers following randomness in</span>
<span class="sd">  multi-process data loading algorithm.</span>

<span class="sd">  Args:</span>
<span class="sd">    worker_id: integer</span>
<span class="sd">      ID of subprocess to seed. 0 means that</span>
<span class="sd">      the data will be loaded in the main process</span>
<span class="sd">      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">worker_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">initial_seed</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="set-device-gpu-or-cpu-execute-set-device">
<h2>Set device (GPU or CPU). Execute <code class="docutils literal notranslate"><span class="pre">set_device()</span></code><a class="headerlink" href="#set-device-gpu-or-cpu-execute-set-device" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set device (GPU or CPU). Execute `set_device()`</span>
<span class="c1"># Inform the user if the notebook uses GPU or CPU.</span>

<span class="k">def</span> <span class="nf">set_device</span><span class="p">():</span>
  <span class="sd">"""</span>
<span class="sd">  Set the device. CUDA if available, CPU otherwise</span>

<span class="sd">  Args:</span>
<span class="sd">    None</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>
  <span class="k">if</span> <span class="n">device</span> <span class="o">!=</span> <span class="s2">"cuda"</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"WARNING: For this notebook to perform best, "</span>
        <span class="s2">"if possible, in the menu under `Runtime` -&gt; "</span>
        <span class="s2">"`Change runtime type.`  select `GPU` "</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"GPU is enabled in this notebook."</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">device</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">DEVICE</span> <span class="o">=</span> <span class="n">set_device</span><span class="p">()</span>
<span class="n">SEED</span> <span class="o">=</span> <span class="mi">2021</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -&gt; `Change runtime type.`  select `GPU` 
Random seed 2021 has been set.
</pre></div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-1-recurrent-neural-networks-rnns">
<h1>Section 1: Recurrent Neural Networks (RNNs)<a class="headerlink" href="#section-1-recurrent-neural-networks-rnns" title="Permalink to this headline">¶</a></h1>
<p><em>Time estimate: ~27mins</em></p>
<div class="section" id="video-1-recurrent-neural-networks">
<h2>Video 1: Recurrent Neural Networks<a class="headerlink" href="#video-1-recurrent-neural-networks" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "c6d960a8cbae4b3d8fe1a3945b536c4e"}
</script></div>
</div>
<p>Recurrent neural networks, or RNNs , are a family of neural networks for processing sequential data. Just as a convolutional network is specialized for processing a grid of values X such as an image, a recurrent neural network is specialized for processing a sequence of values. RNNs prove useful in many scenarios where other deep learning models are not effective.</p>
<ul class="simple">
<li><p>Not all problems can be converted into one with fixed length inputs and outputs.</p></li>
<li><p>The deep learning models we have seen so far pick samples randomly. This might not be the best strategy for a task of understanding meaning from a piece of text. Words in a text occur in a sequence and therefore cannot be permuted randomly to get the meaning.</p></li>
</ul>
<p>The following provides more data than the video (but can be skipped for now). For more detail, see the sources, the <a class="reference external" href="https://www.deeplearningbook.org/contents/rnn.html">deep learning book</a>, and <a class="reference external" href="https://d2l.ai/chapter_recurrent-neural-networks/rnn.html">d2l.ai</a></p>
<p>When the recurrent network is trained to perform a task that requires predicting the future from the past, the network typically learns to use a hidden state at time step <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(H_t\)</span> as a kind of lossy summary of the task-relevant aspects of the past sequence of inputs up to <span class="math notranslate nohighlight">\(t\)</span>. This summary is in general necessarily lossy, since it maps an arbitrary length sequence <span class="math notranslate nohighlight">\((X_t, X_{t-1}, X_{t-2}, \dots, X_{2}, X_{1})\)</span> to a ﬁxed length vector <span class="math notranslate nohighlight">\(H_t\)</span>.</p>
<p>We can represent the unfolded recurrence after <span class="math notranslate nohighlight">\(t\)</span> steps with a function <span class="math notranslate nohighlight">\(G_t\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-05f524c5-7043-4604-9630-ac577958effe">
<span class="eqno">(75)<a class="headerlink" href="#equation-05f524c5-7043-4604-9630-ac577958effe" title="Permalink to this equation">¶</a></span>\[\begin{align}
H_t &amp;= G_t(X_t, X_{t-1}, X_{t-2}, \dots, X_{2}, X_{1}) \\
&amp;= f(H_{t−1}, X_{t}; \theta)
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta\)</span> denotes the model parameters, i.e., weights and biases.</p>
<figure>
<img src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/static/rnn-2.gif" width="700"/>
<figcaption>
  Source <a href="https://blog.floydhub.com/">blog.floydhub.com</a>
</figcaption>
</figure>
<p>The function <span class="math notranslate nohighlight">\(g_t\)</span> takes the whole past sequence <span class="math notranslate nohighlight">\((X_t, X_{t-1}, X_{t-2}, \dots , X_{2}, X_{1})\)</span> as input and produces the current state, but the unfolded recurrent structure allows us to factorize <span class="math notranslate nohighlight">\(g_t\)</span> into repeated application of a function f. The unfolding process thus introduces two major advantages:</p>
<ul class="simple">
<li><p>Regardless of the sequence length, the learned model always has the same input size, because it is speciﬁed in terms of transition from one state to another state, rather than speciﬁed in terms of a variable-length history of states.</p></li>
<li><p>It is possible to use the same transition function <span class="math notranslate nohighlight">\(f\)</span> with the same parameters at every time step.</p></li>
</ul>
<p>We will now formally write down the equations of a recurrent unit.</p>
<p>Assume that we have a minibatch of inputs <span class="math notranslate nohighlight">\(X_t \in R^{n \times d}\)</span> at time step <span class="math notranslate nohighlight">\(t\)</span>. In other words, for a minibatch of <span class="math notranslate nohighlight">\(n\)</span> sequence examples, each row of <span class="math notranslate nohighlight">\(X_t\)</span> corresponds to one example at time step <span class="math notranslate nohighlight">\(t\)</span> from the sequence. Next, we denote by <span class="math notranslate nohighlight">\(H_t \in R^{n \times h}\)</span> the hidden variable of time step <span class="math notranslate nohighlight">\(t\)</span>. Unlike the MLP, here we save the hidden variable <span class="math notranslate nohighlight">\(H_{t-1}\)</span> from the previous time step and introduce a new weight parameter <span class="math notranslate nohighlight">\(W_{hh} \in R^{h \times h}\)</span> to describe how to use the hidden variable of the previous time step in the current time step. Specifically, the calculation of the hidden variable of the current time step is determined by the input of the current time step together with the hidden variable of the previous time step:</p>
<div class="amsmath math notranslate nohighlight" id="equation-6cb977d1-435a-4a9a-8282-1a25e6a78ccd">
<span class="eqno">(76)<a class="headerlink" href="#equation-6cb977d1-435a-4a9a-8282-1a25e6a78ccd" title="Permalink to this equation">¶</a></span>\[\begin{equation}
H_t = \phi(X_t W_{xh} + H_{t-1}W_{hh} + b_h)
\end{equation}\]</div>
<p>For time step <span class="math notranslate nohighlight">\(t\)</span>, the output of the output layer is similar to the computation in the MLP:</p>
<div class="amsmath math notranslate nohighlight" id="equation-2e251a08-4038-4f5c-9847-c91dae0ff045">
<span class="eqno">(77)<a class="headerlink" href="#equation-2e251a08-4038-4f5c-9847-c91dae0ff045" title="Permalink to this equation">¶</a></span>\[\begin{equation}
O_t = H_t W_{hq} + b_q
\end{equation}\]</div>
<p>Parameters of the RNN include the weights <span class="math notranslate nohighlight">\(W_{xh} \in R^{d \times h}, W_{hh} \in R^{h \times h}\)</span> , and the bias <span class="math notranslate nohighlight">\(b_h \in R^{1 \times h}\)</span> of the hidden layer, together with the weights <span class="math notranslate nohighlight">\(W_{hq} \in R^{h \times  q}\)</span> and the bias <span class="math notranslate nohighlight">\(b_q \in R^{1 \times q}\)</span> of the output layer. It is worth mentioning that even at different time steps, RNNs always use these model parameters. Therefore, the parameterization cost of an RNN does not grow as the number of time steps increases.</p>
<figure>
<img align="center" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/static/rnn.svg" width="700"/>
<figcaption>
  Source <a href="https://d2l.ai/">d2l.ai</a>
</figcaption>
</figure></div>
<div class="section" id="section-1-1-load-and-view-of-the-dataset">
<h2>Section 1.1: Load and View of the dataset<a class="headerlink" href="#section-1-1-load-and-view-of-the-dataset" title="Permalink to this headline">¶</a></h2>
<p>Let us first load the dataset using the helper function <code class="docutils literal notranslate"><span class="pre">load_data</span></code>, which takes three arguments; the <code class="docutils literal notranslate"><span class="pre">sentence_length</span></code>, <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, and the <code class="docutils literal notranslate"><span class="pre">seed</span></code>. The default values are 50, 32, and 522, respectively. Execute the cell below to load the data.</p>
<p>Dataset Loading with default params</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Dataset Loading with default params</span>
<span class="n">TEXT</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">valid_iter</span><span class="p">,</span> <span class="n">test_iter</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data loading is completed. Sentence length: 50, Batch size: 32, and seed: 2021
</pre></div>
</div>
</div>
</div>
<p>Now, let’s view the data!</p>
<p>Visualize dataset</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Visualize dataset</span>
<span class="n">view_data</span><span class="p">(</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Review:  i first saw this film about 11 years ago when my former college accounting professor recommended it to me . i was amazed that a movie from 1968 could so coherently and hilariously portray computer crime . maggie smith is delightful and ustinov plays the `` retro hacker '' perfectly
Label:  1 

Review:  i 've just returned from a showing of `` my left foot '' at our public library . what an emotional experience -- i feel drained and uplifted. &lt; br / &gt; &lt; br / &gt; it 's the story of christy brown , irish writer and painter , and
Label:  1 

Review:  michael cacoyannis has had a relatively long career but has surprisingly few credits to his name , including some real duds such as the unfunny cold war satire the day the fish came out . iphigenia , however , is a highlight . adapted by cacoyannis from the play by
Label:  1 

Review:  i did n't expect much when i saw this at the palm springs film fest this weekend . it was an alternate choice when two other films were sold out . still , i held out hope . it sounded a bit much like `` bride and prejudice '' (
Label:  0 

Review:  i think i 've seen worse films , so i 'm giving this a 3 , but it 's a struggle to remember what they could have been ! ! possibly xtro ( nasty &amp; dull ) or possibly creep ( just plain dull ) , but it is a
Label:  0 

[0: Negative Review, 1: Positive Review]
</pre></div>
</div>
</div>
</div>
<div class="section" id="coding-exercise-1-1-vanilla-rnn">
<h3>Coding Exercise 1.1: Vanilla RNN<a class="headerlink" href="#coding-exercise-1-1-vanilla-rnn" title="Permalink to this headline">¶</a></h3>
<p>Now it’s your turn to write a Vanilla RNN using PyTorch.</p>
<ul class="simple">
<li><p>Once again we will use <code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code>. You are given the <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> which is the size of the dictionary of embeddings, and the <code class="docutils literal notranslate"><span class="pre">embed_size</span></code> which is the size of each embedding vector.</p></li>
<li><p>Add 2 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.RNN.html">RNN</a> layers. This would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results.</p></li>
<li><p>Determine the size of inputs and outputs to the fully-connected layer.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">VanillaRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Vanilla RNN with following structure:</span>
<span class="sd">  Embedding of size vocab_size * embed_size # Embedding Layer</span>
<span class="sd">  RNN of size embed_size * hidden_size * self.n_layers # RNN Layer</span>
<span class="sd">  Linear of size self.n_layers*hidden_size * output_size # Fully connected layer</span>
<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span>
               <span class="n">device</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Initialize parameters of VanillaRNN</span>

<span class="sd">    Args:</span>
<span class="sd">      layers: int</span>
<span class="sd">        Number of layers</span>
<span class="sd">      output_size: int</span>
<span class="sd">        Size of final fully connected layer</span>
<span class="sd">      hidden_size: int</span>
<span class="sd">        Size of hidden layer</span>
<span class="sd">      vocab_size: int</span>
<span class="sd">        Size of vocabulary</span>
<span class="sd">      device: string</span>
<span class="sd">        GPU if available, CPU otherwise</span>
<span class="sd">      embed_size: int</span>
<span class="sd">        Size of embedding</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing</span>
<span class="sd">    """</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">VanillaRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="o">=</span> <span class="n">layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="c1">####################################################################</span>
    <span class="c1"># Fill in missing code below (...),</span>
    <span class="c1"># then remove or comment the line below to test your function</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Define the Vanilla RNN components"</span><span class="p">)</span>
    <span class="c1">####################################################################</span>
    <span class="c1"># Define the embedding</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1"># Define the RNN layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1"># Define the fully connected layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Forward pass of VanillaRNN</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: torch.tensor</span>
<span class="sd">        Input features</span>

<span class="sd">    Returns:</span>
<span class="sd">      logits: torch.tensor</span>
<span class="sd">        Output of final fully connected layer</span>
<span class="sd">    """</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">h_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">h_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h_0</span><span class="p">)</span>
    <span class="n">h_n</span> <span class="o">=</span> <span class="n">h_n</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="c1"># Reshape the data and create a copy of the tensor such that the</span>
    <span class="c1"># order of its elements in memory is the same as if it had been created</span>
    <span class="c1"># from scratch with the same data. Without contiguous it may raise an error</span>
    <span class="c1"># RuntimeError: input is not contiguous;</span>
    <span class="c1"># Note that this is necessary as permute may return a non-contiguous tensor</span>
    <span class="n">h_n</span> <span class="o">=</span> <span class="n">h_n</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">h_n</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">h_n</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">h_n</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">h_n</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logits</span>


<span class="c1"># Add event to airtable</span>
<span class="n">atform</span><span class="o">.</span><span class="n">add_event</span><span class="p">(</span><span class="s1">'Coding Exercise 1.1: Vanilla RNN'</span><span class="p">)</span>

<span class="c1">## Uncomment to test VanillaRNN class</span>
<span class="c1"># sampleRNN = VanillaRNN(2, 10, 50, 1000, 300, DEVICE)</span>
<span class="c1"># print(sampleRNN)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D3_ModernRecurrentNeuralNetworks/solutions/W2D3_Tutorial2_Solution_5f8fdeab.py"><em>Click for solution</em></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">VanillaRNN</span><span class="p">(</span>
  <span class="p">(</span><span class="n">embeddings</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
  <span class="p">(</span><span class="n">rnn</span><span class="p">):</span> <span class="n">RNN</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
  <span class="p">(</span><span class="n">fc</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="section-1-2-train-and-test-the-network">
<h2>Section 1.2: Train and test the network<a class="headerlink" href="#section-1-2-train-and-test-the-network" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Model hyperparamters</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0002</span>
<span class="n">layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># 100</span>
<span class="n">embedding_length</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>


<span class="c1"># Initialize model, training and testing</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">vanilla_rnn_model</span> <span class="o">=</span> <span class="n">VanillaRNN</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span>
                               <span class="n">embedding_length</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">)</span>
<span class="n">vanilla_rnn_model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">vanilla_rnn_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">vanilla_train_loss</span><span class="p">,</span> <span class="n">vanilla_train_acc</span><span class="p">,</span> <span class="n">vanilla_validation_loss</span><span class="p">,</span> <span class="n">vanilla_validation_acc</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">vanilla_rnn_model</span><span class="p">,</span>
                                                                                               <span class="n">DEVICE</span><span class="p">,</span>
                                                                                               <span class="n">train_iter</span><span class="p">,</span>
                                                                                               <span class="n">valid_iter</span><span class="p">,</span>
                                                                                               <span class="n">epochs</span><span class="p">,</span>
                                                                                               <span class="n">learning_rate</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"--- Time taken to train = </span><span class="si">%s</span><span class="s2"> seconds ---"</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">vanilla_rnn_start_time</span><span class="p">))</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">vanilla_rnn_model</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Test Accuracy: </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">}</span><span class="s1"> with len=50</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># Number of model parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Number of parameters = </span><span class="si">{</span><span class="n">count_parameters</span><span class="p">(</span><span class="n">vanilla_rnn_model</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<p>Now, let’s plot the accuracies!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot accuracy curves</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">plot_train_val</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span> <span class="n">vanilla_train_acc</span><span class="p">,</span> <span class="n">vanilla_validation_acc</span><span class="p">,</span>
               <span class="s1">'train accuracy'</span><span class="p">,</span> <span class="s1">'val accuracy'</span><span class="p">,</span>
               <span class="s1">'Vanilla RNN on IMDB text classification'</span><span class="p">,</span> <span class="s1">'accuracy'</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="s1">'C0'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">plot_train_val</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span> <span class="n">vanilla_train_loss</span><span class="p">,</span>
               <span class="n">vanilla_validation_loss</span><span class="p">,</span>
               <span class="s1">'train loss'</span><span class="p">,</span> <span class="s1">'val loss'</span><span class="p">,</span>
               <span class="s1">'Vanilla RNN on IMDB text classification'</span><span class="p">,</span>
               <span class="s1">'loss [a.u.]'</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="s1">'C0'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>

</div>
<div class="section" id="change-the-input-length">
<h3>Change the input length<a class="headerlink" href="#change-the-input-length" title="Permalink to this headline">¶</a></h3>
<p>Now let’s increase the <code class="docutils literal notranslate"><span class="pre">sentence_length</span></code> to see how RNN performs when long reviews are allowed..</p>
<p>Load dataset with <code class="docutils literal notranslate"><span class="pre">sentence_length=200</span></code></p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Load dataset with `sentence_length=200`</span>
<span class="n">TEXT_long</span><span class="p">,</span> <span class="n">vocab_size_long</span><span class="p">,</span> <span class="n">train_iter_long</span><span class="p">,</span> <span class="n">valid_iter_long</span><span class="p">,</span> <span class="n">test_iter_long</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">sentence_length</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data loading is completed. Sentence length: 200, Batch size: 32, and seed: 2021
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="re-run-the-network">
<h3>Re-run the network<a class="headerlink" href="#re-run-the-network" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Model hyperparamters</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0002</span>
<span class="n">layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># 100</span>
<span class="n">embedding_length</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Initialize model, training, testing</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">vanilla_rnn_model_long</span> <span class="o">=</span> <span class="n">VanillaRNN</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span>
                                    <span class="n">vocab_size_long</span><span class="p">,</span> <span class="n">embedding_length</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">)</span>
<span class="n">vanilla_rnn_model_long</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">vanilla_rnn_start_time_long</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">vanilla_train_loss_long</span><span class="p">,</span> <span class="n">vanilla_train_acc_long</span><span class="p">,</span> <span class="n">vanilla_validation_loss_long</span><span class="p">,</span> <span class="n">vanilla_validation_acc_long</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">vanilla_rnn_model_long</span><span class="p">,</span>
                                                                                                                   <span class="n">DEVICE</span><span class="p">,</span>
                                                                                                                   <span class="n">train_iter_long</span><span class="p">,</span>
                                                                                                                   <span class="n">valid_iter_long</span><span class="p">,</span>
                                                                                                                   <span class="n">epochs</span><span class="p">,</span>
                                                                                                                   <span class="n">learning_rate</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"--- Time taken to train = </span><span class="si">%s</span><span class="s2"> seconds ---"</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">vanilla_rnn_start_time_long</span><span class="p">))</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">vanilla_rnn_model_long</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">test_iter_long</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Test Accuracy: </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">}</span><span class="s1"> with len=200</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># Number of parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="se">\n</span><span class="s1">Number of parameters = </span><span class="si">{</span><span class="n">count_parameters</span><span class="p">(</span><span class="n">vanilla_rnn_model_long</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compare accuracies of model trained on different sentence lengths</span>
<span class="n">plot_train_val</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span> <span class="n">vanilla_train_acc</span><span class="p">,</span>
               <span class="n">vanilla_validation_acc</span><span class="p">,</span>
               <span class="s1">'train accuracy, len=50'</span><span class="p">,</span> <span class="s1">'val accuracy, len=50'</span><span class="p">,</span>
               <span class="s1">''</span><span class="p">,</span> <span class="s1">'accuracy'</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="s1">'C0'</span><span class="p">)</span>
<span class="n">plot_train_val</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span> <span class="n">vanilla_train_acc_long</span><span class="p">,</span>
               <span class="n">vanilla_validation_acc_long</span><span class="p">,</span>
               <span class="s1">'train accuracy, len=200'</span><span class="p">,</span> <span class="s1">'val accuracy, len=200'</span><span class="p">,</span>
               <span class="s1">'Training and Validation Accuracy for Sentence Lengths 50 and 200'</span><span class="p">,</span>
               <span class="s1">'accuracy'</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="s1">'C1'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>

</div>
</div>
</div>
<div class="section" id="section-1-3-architectures">
<h2>Section 1.3: Architectures<a class="headerlink" href="#section-1-3-architectures" title="Permalink to this headline">¶</a></h2>
<div class="section" id="video-2-bidirectional-rnns">
<h3>Video 2: Bidirectional RNNs<a class="headerlink" href="#video-2-bidirectional-rnns" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "71ae824511164c7cb9642bb2f9bcb5bb"}
</script></div>
</div>
<p>RNN models are mostly used in the fields of natural language processing and speech recognition. Below are types of RNNs. Depending on which outputs we use, RNN can be used for variety of tasks. The text classification problem we solved was an instance of the many to one architecture. Write down the applications of other architectures.</p>
<figure>
<img src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/static/karpathy.jpeg" width="800"/>
<figcaption>
  Source <a href="https://blog.floydhub.com/">blog.floydhub.com</a>
</figcaption>
</figure></div>
</div>
<div class="section" id="section-1-4-vanishing-and-exploding-gradients">
<h2>Section 1.4: Vanishing and Exploding Gradients<a class="headerlink" href="#section-1-4-vanishing-and-exploding-gradients" title="Permalink to this headline">¶</a></h2>
<p>For an RNN to learn via backprop through time on a loss calculated at time <span class="math notranslate nohighlight">\(T\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}_T\)</span>, with respect to an input <span class="math notranslate nohighlight">\(t\)</span> time steps in the past, the RNN weights must be updated based on how they contributed to the hidden state at this past time step. This contribution is learned through the term <span class="math notranslate nohighlight">\(\frac{\partial h_{-t}}{\partial W}\)</span>, in the gradient of the loss, <span class="math notranslate nohighlight">\(\frac{\partial\mathcal{L}_T}{\partial W}\)</span>.</p>
<p>However, because one has to backpropagate error through <span class="math notranslate nohighlight">\(t-1\)</span> hidden states, <span class="math notranslate nohighlight">\(\frac{\partial h_{-t}}{\partial W}\)</span> is multiplied by <span class="math notranslate nohighlight">\(\prod_{i=0}^{t-1} \frac{\partial{h_i}}{\partial{h_{i-1}}}\)</span> in the expression for <span class="math notranslate nohighlight">\(\frac{\partial\mathcal{L}_T}{\partial W}\)</span>, which are summarized mathematically:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a33b9799-3314-438e-86a8-e5824db32add">
<span class="eqno">(78)<a class="headerlink" href="#equation-a33b9799-3314-438e-86a8-e5824db32add" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\frac{\partial{\mathcal{L}_T}}{\partial{W}} \propto \frac{\partial h_t }{ \partial W} + \sum_{k=0}^{t-1} \left( \prod_{i=k+1}^{t} \frac{\partial{h_i}}{\partial{h_{i-1}}} \right) \frac{\partial{h_k}}{\partial{W}}
\end{equation}\]</div>
<p>The product term leads to two common problems during the backpropagation of time-series data:</p>
<ul class="simple">
<li><p><em>Vanishing gradients</em>, <em>if</em> <span class="math notranslate nohighlight">\( \left| \left| \frac{\partial{h_i}}{\partial{h_{i-1}}} \right| \right|_2 &lt; 1\)</span></p></li>
<li><p><em>Exploding gradients</em>, <em>if</em> <span class="math notranslate nohighlight">\( \left| \left| \frac{\partial{h_i}}{\partial{h_{i-1}}} \right| \right|_2 &gt; 1\)</span></p></li>
</ul>
<p>Given a sufficiently long sequence, the gradients get multiplied by the weight matrix at every time step. If the weight matrix contains very small values, then the norm of gradients will become smaller and smaller exponentially, the so-called <strong>vanishing gradient</strong> problem. On the other hand, if we have a weight matrix with very large values, the gradients will increase exponentially, leading to the <strong>exploding gradients</strong> problem: where the weights diverge at the update step.</p>
<p>An example that has the vanishing gradient problem:</p>
<p>The input is the characters from a <em>C</em> Program. The system will tell whether it is a syntactically correct program. A syntactically correct program should have a valid number of braces and parentheses. Thus, the network should remember how many open parentheses and braces there are to check, and whether we have closed them all. The network has to store such information in hidden states like a counter. However, because of vanishing gradients, it will fail to preserve such information in a long program.</p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-2-lstm-gru-and-memory-cell">
<h1>Section 2: LSTM, GRU and Memory Cell<a class="headerlink" href="#section-2-lstm-gru-and-memory-cell" title="Permalink to this headline">¶</a></h1>
<p><em>Time estimate: ~28mins</em></p>
<div class="section" id="video-3-lstm-gru-the-memory-cells">
<h2>Video 3: LSTM, GRU &amp; The Memory Cells<a class="headerlink" href="#video-3-lstm-gru-the-memory-cells" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "5536a83f548441e8b38f3af37e90780e"}
</script></div>
</div>
</div>
<div class="section" id="section-2-1-architecture">
<h2>Section 2.1: Architecture<a class="headerlink" href="#section-2-1-architecture" title="Permalink to this headline">¶</a></h2>
<p>The core idea behind an LSTM is the cell state <span class="math notranslate nohighlight">\(C_t\)</span> that runs along all the LSTM units in a layer, and gets updated along the way. These updates are possible through “gates”. Gates are made out of a sigmoid neural net layer and a pointwise multiplication operation.</p>
<p>Each LSTM unit performs the following distinct steps using the input <span class="math notranslate nohighlight">\(X_t\)</span>, current cell state <span class="math notranslate nohighlight">\(C_t\)</span> and previous hidden state <span class="math notranslate nohighlight">\(H_{t-1}\)</span>:</p>
<ul class="simple">
<li><p>Forget Gate: <em>Should I throw away information from this cell?</em></p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-5f282f9a-631d-4b1c-8a58-1b36c71c1886">
<span class="eqno">(79)<a class="headerlink" href="#equation-5f282f9a-631d-4b1c-8a58-1b36c71c1886" title="Permalink to this equation">¶</a></span>\[\begin{equation}
F_t = \sigma (W_f \cdot [H_{t-1}, X_t] + b_f)
\end{equation}\]</div>
<ul>
<li><p>Input Gate:</p>
<ul>
<li><p><em>Should I add new values to this cell?</em></p>
<div class="amsmath math notranslate nohighlight" id="equation-c44fdedb-1633-4d20-9c6f-664c81183ece">
<span class="eqno">(80)<a class="headerlink" href="#equation-c44fdedb-1633-4d20-9c6f-664c81183ece" title="Permalink to this equation">¶</a></span>\[\begin{equation}
      I_t = \sigma (W_i \cdot [H_{t-1}, X_t] + b_i)
      \end{equation}\]</div>
</li>
<li><p><em>What new candidate values should I store?</em></p>
<div class="amsmath math notranslate nohighlight" id="equation-fb0d47d2-a896-48d8-a55f-89096094b613">
<span class="eqno">(81)<a class="headerlink" href="#equation-fb0d47d2-a896-48d8-a55f-89096094b613" title="Permalink to this equation">¶</a></span>\[\begin{equation}
      \tilde{C}_t = tanh (W_C \cdot [H_{t-1}, X_t] + b_C)
      \end{equation}\]</div>
</li>
</ul>
</li>
<li><p>Update cell state: <em>Forget things from the past and add new things from the candidates</em></p>
<div class="amsmath math notranslate nohighlight" id="equation-b0796917-e132-4bad-8d9a-194eac557be4">
<span class="eqno">(82)<a class="headerlink" href="#equation-b0796917-e132-4bad-8d9a-194eac557be4" title="Permalink to this equation">¶</a></span>\[\begin{equation}
  C_t = (F_t \cdot C_{t-1}) + (I_t \cdot \tilde{C}_t)
  \end{equation}\]</div>
</li>
<li><p>Output Gate:</p>
<ul>
<li><p><em>What information should I output?</em></p>
<div class="amsmath math notranslate nohighlight" id="equation-698ede2a-f29f-4de2-bff6-1de6e1a57362">
<span class="eqno">(83)<a class="headerlink" href="#equation-698ede2a-f29f-4de2-bff6-1de6e1a57362" title="Permalink to this equation">¶</a></span>\[\begin{equation}
      O_t = \sigma (W_o \cdot [H_{t-1}, X_t] + b_o)
      \end{equation}\]</div>
</li>
<li><p><em>How much of the cell state should I store in the hidden state?</em></p>
<div class="amsmath math notranslate nohighlight" id="equation-83ba4662-430b-4638-a9b5-c3e01363ecdc">
<span class="eqno">(84)<a class="headerlink" href="#equation-83ba4662-430b-4638-a9b5-c3e01363ecdc" title="Permalink to this equation">¶</a></span>\[\begin{equation}
      H_t = O_t \cdot tanh(C_t)
      \end{equation}\]</div>
</li>
</ul>
</li>
</ul>
<p>The architecture can be summarized by the diagram below:</p>
<center>
<figure>
<img src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/static/lstm-1.svg" width="700"/>
<figcaption>
  Source <a href="https://d2l.ai/">d2l.ai</a>
</figcaption>
</figure>
</center><div class="section" id="coding-exercise-2-1-implementing-lstm">
<h3>Coding Exercise 2.1: Implementing LSTM<a class="headerlink" href="#coding-exercise-2-1-implementing-lstm" title="Permalink to this headline">¶</a></h3>
<p>It is now your turn to build an LSTM network in PyTorch. Feel free to refer to the documentation <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM">here</a>.</p>
<ul class="simple">
<li><p>Once again we will use <code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code>. You are given the <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> and the <code class="docutils literal notranslate"><span class="pre">embed_size</span></code>.</p></li>
<li><p>Add the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM"><code class="docutils literal notranslate"><span class="pre">LSTM</span></code></a> layers.</p></li>
<li><p>Define a dropout layer of 0.5.</p></li>
<li><p>Determine the size of inputs and outputs to the fully-connected layer.</p></li>
<li><p>Pay special attention to the shapes of your inputs and outputs as you write the forward function.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LSTM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  LSTM (Long Short Term Memory) with following structure</span>
<span class="sd">  Embedding layer of size vocab_size * embed_size</span>
<span class="sd">  Dropout layer with dropout_probability of 0.5</span>
<span class="sd">  LSTM layer of size embed_size * hidden_size * num_layers</span>
<span class="sd">  Fully connected layer of n_layers*hidden_size * output_size</span>
<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span>
               <span class="n">device</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Initialize parameters of LSTM</span>

<span class="sd">    Args:</span>
<span class="sd">      layers: int</span>
<span class="sd">        Number of layers</span>
<span class="sd">      output_size: int</span>
<span class="sd">        Size of final fully connected layer</span>
<span class="sd">      hidden_size: int</span>
<span class="sd">        Size of hidden layer</span>
<span class="sd">      vocab_size: int</span>
<span class="sd">        Size of vocabulary</span>
<span class="sd">      device: string</span>
<span class="sd">        GPU if available, CPU otherwise</span>
<span class="sd">      embed_size: int</span>
<span class="sd">        Size of embedding</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing</span>
<span class="sd">    """</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">LSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="c1">####################################################################</span>
    <span class="c1"># Fill in missing code below (...),</span>
    <span class="c1"># then remove or comment the line below to test your function</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"LSTM Init"</span><span class="p">)</span>
    <span class="c1">####################################################################</span>
    <span class="c1"># Define the word embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1"># Define the dropout layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1"># Define the lstm layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1"># Define the fully-connected layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="o">...</span>


  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_sentences</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Forward pass of LSTM</span>
<span class="sd">    Hint: Make sure the shapes of your tensors match the requirement</span>

<span class="sd">    Args:</span>
<span class="sd">      input_sentences: torch.tensor</span>
<span class="sd">        Input Sentences</span>

<span class="sd">    Returns:</span>
<span class="sd">      logits: torch.tensor</span>
<span class="sd">        Output of final fully connected layer</span>
<span class="sd">    """</span>
    <span class="c1">####################################################################</span>
    <span class="c1"># Fill in missing code below (...),</span>
    <span class="c1"># then remove or comment the line below to test your function</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"LSTM Forward"</span><span class="p">)</span>
    <span class="c1">####################################################################</span>
    <span class="c1"># Embeddings</span>
    <span class="c1"># `input` shape: (`num_steps`, `batch_size`, `num_hiddens`)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="o">...</span>

    <span class="n">hidden</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
              <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
    <span class="c1"># Dropout for regularization</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="c1"># LSTM</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="o">...</span>

    <span class="n">h_n</span> <span class="o">=</span> <span class="n">hidden</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">h_n</span> <span class="o">=</span> <span class="n">h_n</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">h_n</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">h_n</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logits</span>


<span class="c1"># Add event to airtable</span>
<span class="n">atform</span><span class="o">.</span><span class="n">add_event</span><span class="p">(</span><span class="s1">'Coding Exercise 2.1: Implementing LSTM'</span><span class="p">)</span>

<span class="c1">## Uncomment to run</span>
<span class="c1"># sampleLSTM = LSTM(3, 10, 100, 1000, 300, DEVICE)</span>
<span class="c1"># print(sampleLSTM)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D3_ModernRecurrentNeuralNetworks/solutions/W2D3_Tutorial2_Solution_3a998b43.py"><em>Click for solution</em></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">LSTM</span><span class="p">(</span>
  <span class="p">(</span><span class="n">word_embeddings</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
  <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="p">(</span><span class="n">lstm</span><span class="p">):</span> <span class="n">LSTM</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
  <span class="p">(</span><span class="n">fc</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Hyperparameters</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0003</span>
<span class="n">layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">embedding_length</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Model, training, testing</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">lstm_model</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span>
                  <span class="n">embedding_length</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">)</span>
<span class="n">lstm_model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">lstm_train_loss</span><span class="p">,</span> <span class="n">lstm_train_acc</span><span class="p">,</span> <span class="n">lstm_validation_loss</span><span class="p">,</span> <span class="n">lstm_validation_acc</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">lstm_model</span><span class="p">,</span>
                                                                                   <span class="n">DEVICE</span><span class="p">,</span>
                                                                                   <span class="n">train_iter</span><span class="p">,</span>
                                                                                   <span class="n">valid_iter</span><span class="p">,</span>
                                                                                   <span class="n">epochs</span><span class="p">,</span>
                                                                                   <span class="n">learning_rate</span><span class="p">)</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">lstm_model</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="se">\n\n</span><span class="s1">Test Accuracy: </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">}</span><span class="s1"> of the LSTM model</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># Plotting accuracy curve</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">plot_train_val</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span> <span class="n">lstm_train_acc</span><span class="p">,</span> <span class="n">lstm_validation_acc</span><span class="p">,</span>
               <span class="s1">'train accuracy'</span><span class="p">,</span>
               <span class="s1">'val accuracy'</span><span class="p">,</span>
               <span class="s1">'LSTM on IMDB text classification'</span><span class="p">,</span>
               <span class="s1">'accuracy'</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="s1">'C0'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">plot_train_val</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span> <span class="n">lstm_train_loss</span><span class="p">,</span> <span class="n">lstm_validation_loss</span><span class="p">,</span>
               <span class="s1">'train loss'</span><span class="p">,</span>
               <span class="s1">'val loss'</span><span class="p">,</span>
               <span class="s1">''</span><span class="p">,</span>
               <span class="s1">'loss'</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="s1">'C0'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>

</div>
</div>
</div>
<div class="section" id="section-2-2-gated-recurrent-units-gru">
<h2>Section 2.2: Gated Recurrent Units (GRU)<a class="headerlink" href="#section-2-2-gated-recurrent-units-gru" title="Permalink to this headline">¶</a></h2>
<p>The GRU architecture looks very similar to the LSTM, and is often used as an alternative to the traditional LSTM. It also contains some variations that reduce it’s complexity. For example, it combines the forget and input gates into a single “update gate”; it contains a “hidden state” but not a “cell state”. In the next section we will be using GRUs as the choice of recurrent unit in our models, but you can always swap out the GRU for an LSTM later on (make sure that you take care of input and output dimensions in this case). Here is a description of the parts of the GRU:</p>
<ul>
<li><p>Reset Gate: <em>How much of the previous hidden state should I remember?</em></p>
<div class="amsmath math notranslate nohighlight" id="equation-7afc9faf-3028-43e5-87ad-b2baa1112f83">
<span class="eqno">(85)<a class="headerlink" href="#equation-7afc9faf-3028-43e5-87ad-b2baa1112f83" title="Permalink to this equation">¶</a></span>\[\begin{equation}
  R_t = \sigma (W_r \cdot [H_{t-1}, X_t])
  \end{equation}\]</div>
</li>
<li><p>Update Gate:</p>
<ul>
<li><p><em>How much of the new state is different from the old state?</em></p>
<div class="amsmath math notranslate nohighlight" id="equation-1eaeee33-c3ce-46f6-905d-7b3a191a93de">
<span class="eqno">(86)<a class="headerlink" href="#equation-1eaeee33-c3ce-46f6-905d-7b3a191a93de" title="Permalink to this equation">¶</a></span>\[\begin{equation}
      Z_t = \sigma (W_z \cdot [H_{t-1}, X_t])
      \end{equation}\]</div>
</li>
<li><p><em>What new candidate values should I store?</em></p>
<div class="amsmath math notranslate nohighlight" id="equation-022a91d5-ec6d-4605-b6ad-1ea6bc1f6af0">
<span class="eqno">(87)<a class="headerlink" href="#equation-022a91d5-ec6d-4605-b6ad-1ea6bc1f6af0" title="Permalink to this equation">¶</a></span>\[\begin{equation}
      \tilde{H}_t = tanh (W \cdot [R_t \cdot H_{t-1}, X_t])
      \end{equation}\]</div>
</li>
</ul>
</li>
<li><p>Update hidden state: <em>Deciding how much of the old hidden state to keep and discard</em></p>
<div class="amsmath math notranslate nohighlight" id="equation-449471a2-a1bb-4342-b13f-6db18b8fef3e">
<span class="eqno">(88)<a class="headerlink" href="#equation-449471a2-a1bb-4342-b13f-6db18b8fef3e" title="Permalink to this equation">¶</a></span>\[\begin{equation}
  H_t = ((1-Z_t) \cdot H_{t-1} ) + (Z_t \cdot \tilde{H}_t)
  \end{equation}\]</div>
</li>
</ul>
<p>Here is what the architecture looks like:</p>
<center>
<figure>
<img src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/static/gru-3.svg" width="700"/>
<figcaption>
  Source <a href="https://d2l.ai/">d2l.ai</a>
</figcaption>
</figure>
</center><div class="section" id="coding-exercise-2-2-bilstm">
<h3>Coding Exercise 2.2: BiLSTM<a class="headerlink" href="#coding-exercise-2-2-bilstm" title="Permalink to this headline">¶</a></h3>
<p>Let’s apply the knowledge to write a bi-LSTM using PyTorch.</p>
<ul class="simple">
<li><p>Use an Embedding layer</p></li>
<li><p>Dropout of 0.5</p></li>
<li><p>Add 2 LSTM layers</p></li>
<li><p>Linear layer</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">biLSTM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Bidirectional LSTM with following structure</span>
<span class="sd">  Embedding layer of size vocab_size * embed_size</span>
<span class="sd">  Dropout layer with dropout_probability of 0.5</span>
<span class="sd">  biLSTM layer of size embed_size * hidden_size * num_layers</span>
<span class="sd">  Fully connected layer of n_layers*hidden_size * output_size</span>
<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span>
               <span class="n">device</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Initialize parameters of biLSTM</span>

<span class="sd">    Args:</span>
<span class="sd">      output_size: int</span>
<span class="sd">        Size of final fully connected layer</span>
<span class="sd">      hidden_size: int</span>
<span class="sd">        Size of hidden layer</span>
<span class="sd">      vocab_size: int</span>
<span class="sd">        Size of vocabulary</span>
<span class="sd">      device: string</span>
<span class="sd">        GPU if available, CPU otherwise</span>
<span class="sd">      embed_size: int</span>
<span class="sd">        Size of embedding</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing</span>
<span class="sd">    """</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">biLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="c1">####################################################################</span>
    <span class="c1"># Fill in missing code below (...)</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"biLSTM"</span><span class="p">)</span>
    <span class="c1">####################################################################</span>
    <span class="c1"># Define the word embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1"># Define the dropout layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1"># Define the bilstm layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bilstm</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1"># Define the fully-connected layer; 4 = 2*2: 2 for stacking and 2 for bidirectionality</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_sentences</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Forward pass of biLSTM</span>

<span class="sd">    Args:</span>
<span class="sd">      input_sentences: torch.tensor</span>
<span class="sd">        Input Sentences</span>

<span class="sd">    Returns:</span>
<span class="sd">      logits: torch.tensor</span>
<span class="sd">        Output of final fully connected layer</span>
<span class="sd">    """</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span><span class="p">(</span><span class="n">input_sentences</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
              <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

    <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bilstm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>

    <span class="n">h_n</span> <span class="o">=</span> <span class="n">hidden</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">h_n</span> <span class="o">=</span> <span class="n">h_n</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">h_n</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">h_n</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logits</span>


<span class="c1"># Add event to airtable</span>
<span class="n">atform</span><span class="o">.</span><span class="n">add_event</span><span class="p">(</span><span class="s1">'Coding Exercise 2.2: BiLSTM'</span><span class="p">)</span>

<span class="c1">## Uncomment to run</span>
<span class="c1"># sampleBiLSTM = biLSTM(10, 100, 1000, 300, DEVICE)</span>
<span class="c1"># print(sampleBiLSTM)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D3_ModernRecurrentNeuralNetworks/solutions/W2D3_Tutorial2_Solution_edf9ab89.py"><em>Click for solution</em></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">biLSTM</span><span class="p">(</span>
  <span class="p">(</span><span class="n">word_embeddings</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
  <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="p">(</span><span class="n">bilstm</span><span class="p">):</span> <span class="n">LSTM</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">(</span><span class="n">fc</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Hyperparameters</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0003</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">embedding_length</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Model, training, testing</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">bilstm_model</span> <span class="o">=</span> <span class="n">biLSTM</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span>
                      <span class="n">embedding_length</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">)</span>
<span class="n">bilstm_model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">bilstm_train_loss</span><span class="p">,</span> <span class="n">bilstm_train_acc</span><span class="p">,</span> <span class="n">bilstm_validation_loss</span><span class="p">,</span> <span class="n">bilstm_validation_acc</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">bilstm_model</span><span class="p">,</span>
                                                                                           <span class="n">DEVICE</span><span class="p">,</span>
                                                                                           <span class="n">train_iter</span><span class="p">,</span>
                                                                                           <span class="n">valid_iter</span><span class="p">,</span>
                                                                                           <span class="n">epochs</span><span class="p">,</span>
                                                                                           <span class="n">learning_rate</span><span class="p">)</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">bilstm_model</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Test Accuracy: </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">}</span><span class="s1"> of the biLSTM model</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># Plotting accuracy curve</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">plot_train_val</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span> <span class="n">bilstm_train_acc</span><span class="p">,</span> <span class="n">bilstm_validation_acc</span><span class="p">,</span>
               <span class="s1">'train accuracy'</span><span class="p">,</span>
               <span class="s1">'val accuracy'</span><span class="p">,</span>
               <span class="s1">'biLSTM on IMDB text classification'</span><span class="p">,</span>
               <span class="s1">'accuracy'</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="s1">'C1'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">plot_train_val</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span> <span class="n">bilstm_train_loss</span><span class="p">,</span> <span class="n">bilstm_validation_loss</span><span class="p">,</span>
               <span class="s1">'train loss'</span><span class="p">,</span>
               <span class="s1">'val loss'</span><span class="p">,</span>
               <span class="s1">''</span><span class="p">,</span>
               <span class="s1">'loss'</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="s1">'C1'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>

</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compare accuracies of LSTM and biLSTM</span>
<span class="n">plot_train_val</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span> <span class="n">lstm_train_acc</span><span class="p">,</span>
               <span class="n">lstm_validation_acc</span><span class="p">,</span>
               <span class="s1">'train accuracy LSTM'</span><span class="p">,</span> <span class="s1">'val accuracy LSTM'</span><span class="p">,</span>
               <span class="s1">''</span><span class="p">,</span> <span class="s1">'accuracy'</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="s1">'C0'</span><span class="p">)</span>
<span class="n">plot_train_val</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span> <span class="n">bilstm_train_acc</span><span class="p">,</span>
               <span class="n">bilstm_validation_acc</span><span class="p">,</span>
               <span class="s1">'train accuracy biLSTM'</span><span class="p">,</span> <span class="s1">'val accuracy biLSTM'</span><span class="p">,</span>
               <span class="s1">'Training and Validation Accuracy for LSTM and biLSTM models'</span><span class="p">,</span>
               <span class="s1">'accuracy'</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="s1">'C1'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>

</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-3-sequence-to-sequence-seq2seq-encoder-decoder-networks">
<h1>Section 3: Sequence to Sequence (Seq2Seq) &amp; Encoder/ Decoder Networks<a class="headerlink" href="#section-3-sequence-to-sequence-seq2seq-encoder-decoder-networks" title="Permalink to this headline">¶</a></h1>
<p><em>Time estimate: ~15mins</em></p>
<div class="section" id="video-4-seq2seq-encoder-decoder-nets">
<h2>Video 4: Seq2Seq &amp; Encoder-Decoder Nets<a class="headerlink" href="#video-4-seq2seq-encoder-decoder-nets" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "2cf1394d54744506b9985fbcf1fab91e"}
</script></div>
</div>
<p>Sources: <a class="reference external" href="https://d2l.ai/chapter_recurrent-modern/encoder-decoder.html">d2l.ai on encoders</a>; <a class="reference external" href="https://d2l.ai/chapter_recurrent-modern/seq2seq.html">d2l.ai on seq2seq</a>; <a class="reference external" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Jalammar’s blog</a></p>
<p>Sequence-to-sequence models take in a sequence of items (words, characters, etc) as input and produces another sequence of items as output. The most
simple seq2seq models are composed of two parts: the encoder, the context (“state” in the figure) and the decoder. The encoder and decoder usually consist of recurrent units that we’ve seen before (RNNs, GRUs or LSTMs). A high-level schematic of the architecture is as follows:</p>
<center>
<figure>
<img src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/static/seq2seq-predict.svg" width="700"/>
<figcaption>
  Source <a href="https://d2l.ai/">d2l.ai</a>
</figcaption>
</figure>
</center>
<p>The encoder’s recurrent unit processes the input one item at a time. Once the entire sequence is processed, the final hidden state vector produced is known as a context vector. The size of the context vector is defined while setting up the model, and is equal to the number of hidden states used in the encoder RNN. The encoder then passes the context to the decoder. The decoder’s recurrent unit uses the context to produce the items for the output sequence one by one.</p>
<p>One of the most popular applications of seq2seq models is “machine translation”: the task of taking in a sentence in one language (the source) and producing its translation in another language (the target); with words in both languages being the sequence units. This is a supervised learning task, and requires the dataset to have “parallel sentences”; i.e., each sentence in the source language must be labelled with its translation in the target language.</p>
<p><a class="reference external" href="https://i.imgur.com/HJ6t8up.mp4">Here is an intuitive visualization for understanding seq2seq models for machine translation from English to French</a>.</p>
<p>Since the vocabulary of an entire language is very large, training such models to give meaningful performance requires significant time and resources. In this section, we will train a seq2seq model to perform machine translation from English to <a class="reference external" href="https://en.wikipedia.org/wiki/Pig_Latin">Pig-Latin</a>. We will modify the task to perform character-level machine translation, so that vocabulary size does not grow exponentially.</p>
</div>
<div class="section" id="coding-exercise-3-encoder">
<h2>Coding Exercise 3: Encoder<a class="headerlink" href="#coding-exercise-3-encoder" title="Permalink to this headline">¶</a></h2>
<p>Let us consider a sequence example (<code class="docutils literal notranslate"><span class="pre">batch_size=1</span></code>). Suppose that the input sequence is <span class="math notranslate nohighlight">\(x_1, \ldots, x_T\)</span>, such that <span class="math notranslate nohighlight">\(x_t\)</span> is the <span class="math notranslate nohighlight">\(t^{\mathrm{th}}\)</span> token in the input text sequence. At time step <span class="math notranslate nohighlight">\(t\)</span>, the RNN transforms the input feature vector <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> for <span class="math notranslate nohighlight">\(x_t\)</span> and the hidden state <span class="math notranslate nohighlight">\(\mathbf{h} _{t-1}\)</span> from the previous time step into the current hidden state <span class="math notranslate nohighlight">\(\mathbf{h}_t\)</span>.</p>
<p>We can use a function <span class="math notranslate nohighlight">\(f\)</span> to express the transformation of the RNN’s recurrent layer:</p>
<div class="amsmath math notranslate nohighlight" id="equation-0521613a-9037-4f8b-884d-07a1f741314c">
<span class="eqno">(89)<a class="headerlink" href="#equation-0521613a-9037-4f8b-884d-07a1f741314c" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\mathbf{h}_t = f(\mathbf{x}_t, \mathbf{h}_{t-1})
\end{equation}\]</div>
<p>In general, the encoder transforms the hidden states at all the time steps into the context variable through a customized function <span class="math notranslate nohighlight">\(q\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-283edfcb-817a-4a3a-928b-57bd7d5773c9">
<span class="eqno">(90)<a class="headerlink" href="#equation-283edfcb-817a-4a3a-928b-57bd7d5773c9" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\mathbf{c} =  q(\mathbf{h}_1, \ldots, \mathbf{h}_T)
\end{equation}\]</div>
<p>For example, when choosing <span class="math notranslate nohighlight">\(q(\mathbf{h}_1, \ldots, \mathbf{h}_T) = \mathbf{h}_T\)</span>  the context variable is just the hidden state <span class="math notranslate nohighlight">\(\mathbf{h}_T\)</span> of the input sequence at the final time step.</p>
<p>So far we have used a unidirectional RNN to design the encoder, where a hidden state only depends on the input subsequence at and before the time step of the hidden state. We can also construct encoders using bidirectional RNNs. In this case, a hidden state depends on the subsequence before and after the time step (including the input at the current time step), which encodes the information of the entire sequence.</p>
<p>Now let us implement the RNN encoder. Note that we use an <em>embedding layer</em>
to obtain the feature vector for each token in the input sequence. The weight of an embedding layer is a matrix whose number of rows is equal to the size of the input vocabulary (<code class="docutils literal notranslate"><span class="pre">vocab_size</span></code>) and the number of columns equals to the feature vector’s dimension (<code class="docutils literal notranslate"><span class="pre">embed_size</span></code>). For any input token index <span class="math notranslate nohighlight">\(i\)</span>,
the embedding layer fetches the <span class="math notranslate nohighlight">\(i^{\mathrm{th}}\)</span> row (starting from 0) of the weight matrix to return its feature vector. Here we choose a multilayer GRU to implement the encoder.</p>
<p>The returned variables of recurrent layers have been completely explained at <a class="reference external" href="https://www.d2l.ai/chapter_recurrent-neural-networks/rnn-concise.html#sec-rnn-concise">this link</a>. Let us still use a concrete example to illustrate the above encoder implementation. Below we instantiate a two-layer GRU encoder whose number of hidden units is 16. Given a minibatch of sequence inputs <span class="math notranslate nohighlight">\(X\)</span> (<code class="docutils literal notranslate"><span class="pre">batch_size=4</span></code>, <code class="docutils literal notranslate"><span class="pre">number_of_time_steps=7</span></code>), the hidden states of the last layer at all the time steps (<code class="docutils literal notranslate"><span class="pre">output</span></code> returned by the encoder’s recurrent layers) are a tensor of shape (number of time steps, batch size, number of hidden units).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Seq2SeqEncoder</span><span class="p">(</span><span class="n">d2l</span><span class="o">.</span><span class="n">Encoder</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  RNN encoder for sequence to sequence learning.</span>
<span class="sd">  RNN has the following structure:</span>
<span class="sd">  Embedding layer with size vocab_size * embed_size</span>
<span class="sd">  RNN layer with size embed_size * num_hiddens * num_layers + dropout</span>
<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span>
                <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Initialize parameters of Seq2SeqEncoder</span>

<span class="sd">    Args:</span>
<span class="sd">      num_layers: int</span>
<span class="sd">        Number of layers in GRU/RNN</span>
<span class="sd">      num_hiddens: int</span>
<span class="sd">        Size of hidden layer</span>
<span class="sd">      vocab_size: int</span>
<span class="sd">        Size of vocabulary</span>
<span class="sd">      embed_size: int</span>
<span class="sd">        Size of embedding</span>
<span class="sd">      dropout: int</span>
<span class="sd">        Dropout [default: 0]</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing</span>
<span class="sd">    """</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Seq2SeqEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="c1">####################################################################</span>
    <span class="c1"># Fill in missing code below (...),</span>
    <span class="c1"># then remove or comment the line below to test your function</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Encoder Unit"</span><span class="p">)</span>
    <span class="c1">####################################################################</span>
    <span class="c1"># Embedding layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1"># Here you're going to implement a GRU as the RNN unit</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Forward pass of Seq2SeqEncoder</span>

<span class="sd">    Args:</span>
<span class="sd">      X: torch.tensor</span>
<span class="sd">        Input features</span>

<span class="sd">    Returns:</span>
<span class="sd">      output: torch.tensor</span>
<span class="sd">        Output with shape (`num_steps`, `batch_size`, `num_hiddens`)</span>
<span class="sd">      state: torch.tensor</span>
<span class="sd">        State with shape (`num_layers`, `batch_size`, `num_hiddens`)</span>
<span class="sd">    """</span>
    <span class="c1"># The output `X` shape: (`batch_size`, `num_steps`, `embed_size`)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="c1"># In RNN models, the first axis corresponds to time steps</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="c1">####################################################################</span>
    <span class="c1"># Fill in missing code below (...),</span>
    <span class="c1"># then remove or comment the line below to test your function</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Forward pass"</span><span class="p">)</span>
    <span class="c1">####################################################################</span>
    <span class="c1"># When state is not mentioned, it defaults to zeros, the output should be a RNN function of X!</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1"># `output` shape: (`num_steps`, `batch_size`, `num_hiddens`)</span>
    <span class="c1"># `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)</span>

    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">state</span>


<span class="c1"># Add event to airtable</span>
<span class="n">atform</span><span class="o">.</span><span class="n">add_event</span><span class="p">(</span><span class="s1">'Coding Exercise 3: Encoder'</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="c1">## uncomment the lines below.</span>
<span class="c1"># encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)</span>
<span class="c1"># encoder.eval()</span>
<span class="c1"># output, state = encoder(X)</span>
<span class="c1"># print(output.shape)</span>
<span class="c1"># print(state.shape)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D3_ModernRecurrentNeuralNetworks/solutions/W2D3_Tutorial2_Solution_f7a7735b.py"><em>Click for solution</em></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="section" id="section-3-1-decoder">
<h2>Section 3.1: Decoder<a class="headerlink" href="#section-3-1-decoder" title="Permalink to this headline">¶</a></h2>
<p>As we just mentioned, the context variable <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> of the encoder’s output encodes the entire input sequence <span class="math notranslate nohighlight">\(x_1, \ldots, x_T\)</span>. Given the output sequence <span class="math notranslate nohighlight">\(y_1, y_2, \ldots, y_{T'}\)</span> from the training dataset, for each time step <span class="math notranslate nohighlight">\(t'\)</span>
(the symbol differs from the time step <span class="math notranslate nohighlight">\(t\)</span> of input sequences or encoders),
the probability of the decoder output <span class="math notranslate nohighlight">\(y_{t'}\)</span> is conditional on the previous output subsequence <span class="math notranslate nohighlight">\(y_1, \ldots, y_{t'-1}\)</span> and the context variable <span class="math notranslate nohighlight">\(\mathbf{c}\)</span>, i.e., <span class="math notranslate nohighlight">\(P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c})\)</span>.</p>
<p>To model this conditional probability on sequences, we can use another RNN as the decoder. At any time step <span class="math notranslate nohighlight">\(t^\prime\)</span> on the output sequence, the RNN takes the output <span class="math notranslate nohighlight">\(y_{t^\prime-1}\)</span> from the previous time step and the context variable <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> as its input, then transforms them and the previous hidden state <span class="math notranslate nohighlight">\(\mathbf{s}_{t^\prime-1}\)</span> into the hidden state <span class="math notranslate nohighlight">\(\mathbf{s}_{t^\prime}\)</span> at the current time step.</p>
<p>As a result, we can use a function <span class="math notranslate nohighlight">\(g\)</span> to express the transformation of the decoder’s hidden layer:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b4026329-f527-46f8-b24b-eec1554a8804">
<span class="eqno">(91)<a class="headerlink" href="#equation-b4026329-f527-46f8-b24b-eec1554a8804" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\mathbf{s}_{t^\prime} = g(y_{t^\prime-1}, \mathbf{c}, \mathbf{s}_{t^\prime-1})
\end{equation}\]</div>
<p>After obtaining the hidden state of the decoder, we can use an output layer and the softmax operation to compute the conditional probability distribution
<span class="math notranslate nohighlight">\(P(y_{t^\prime} \mid y_1, \ldots, y_{t^\prime-1}, \mathbf{c})\)</span> for the output at time step <span class="math notranslate nohighlight">\(t^\prime\)</span>.</p>
<p>Following <code class="docutils literal notranslate"><span class="pre">fig_seq2seq</span></code>, when implementing the decoder as follows, we directly use the hidden state at the final time step of the encoder to initialize the hidden state of the decoder.</p>
<p>This requires that the RNN encoder and the RNN decoder have the same number of layers and hidden units. To further incorporate the encoded input sequence information, the context variable is concatenated with the decoder input at all the time steps. To predict the probability distribution of the output token,
a fully-connected layer is used to transform the hidden state at the final layer of the RNN decoder.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Seq2SeqDecoder</span><span class="p">(</span><span class="n">d2l</span><span class="o">.</span><span class="n">Decoder</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  RNN decoder for sequence to sequence learning.</span>
<span class="sd">  Seq2SeqDecoder has the following structure:</span>
<span class="sd">  nn.Embedding(vocab_size, embed_size) # Embedding Layer</span>
<span class="sd">  nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout) # RNN Layer</span>
<span class="sd">  nn.Linear(num_hiddens, vocab_size) # Fully connected layer</span>
<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span>
                <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Initialize parameters of Seq2SeqDecoder</span>

<span class="sd">    Args:</span>
<span class="sd">      num_layers: int</span>
<span class="sd">        Number of layers in GRU/RNN</span>
<span class="sd">      num_hiddens: int</span>
<span class="sd">        Size of hidden layer</span>
<span class="sd">      vocab_size: int</span>
<span class="sd">        Size of vocabulary</span>
<span class="sd">      embed_size: int</span>
<span class="sd">        Size of embedding</span>
<span class="sd">      dropout: int</span>
<span class="sd">        Dropout [default: 0]</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing</span>
<span class="sd">    """</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Seq2SeqDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">embed_size</span> <span class="o">+</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span>
                      <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_hiddens</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">init_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Initialise Seq2SeqDecoder state</span>

<span class="sd">    Args:</span>
<span class="sd">      enc_outputs: Seq2SeqEncoder instance</span>
<span class="sd">        Output of the Seq2SeqEncoder</span>

<span class="sd">    Returns:</span>
<span class="sd">      Init state of Seq2SeqDecoder as enc_outputs</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">enc_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Forward pass of Seq2SeqDecoder</span>

<span class="sd">    Args:</span>
<span class="sd">      X: torch.tensor</span>
<span class="sd">        Input features</span>
<span class="sd">      state: Seq2SeqEncoder instance</span>
<span class="sd">        Output of the Seq2SeqEncoder</span>

<span class="sd">    Returns:</span>
<span class="sd">      output: torch.tensor</span>
<span class="sd">        Output with shape (`batch_size`, `num_steps`, `vocab_size`)</span>
<span class="sd">      state: torch.tensor</span>
<span class="sd">        State with shape (`num_layers`, `batch_size`, `num_hiddens`)</span>
<span class="sd">    """</span>
    <span class="c1"># The output `X` shape: (`num_steps`, `batch_size`, `embed_size`)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="c1"># Broadcast `context` so it has the same `num_steps` as `X`</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">X_and_context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">context</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">X_and_context</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">output</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="c1"># `output` shape: (`batch_size`, `num_steps`, `vocab_size`)</span>
    <span class="c1"># `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">state</span>
</pre></div>
</div>
</div>
</div>
<p>To illustrate the implemented decoder,
below we instantiate it with the same hyperparameters from the aforementioned encoder.
As we can see, the output shape of the decoder becomes (batch size, number of time steps, vocabulary size),
where the last dimension of the tensor stores the predicted token distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Seq2SeqDecoder</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
                         <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">init_state</span><span class="p">(</span><span class="n">encoder</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="n">output</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
<span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">),</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>

</div>
</div>
<div class="section" id="section-3-2-loss-function">
<h2>Section 3.2: Loss Function<a class="headerlink" href="#section-3-2-loss-function" title="Permalink to this headline">¶</a></h2>
<p>At each time step, the decoder predicts a probability distribution for the output tokens. Similar to language modeling, we can apply softmax to obtain the distribution and calculate the cross-entropy loss for optimization. Recall that the special padding tokens are appended to the end of sequences so sequences of varying lengths can be efficiently loaded in minibatches of the same shape.
However, prediction of padding tokens should be excluded from loss calculations.</p>
<p>To this end, we can use the following <code class="docutils literal notranslate"><span class="pre">sequence_mask</span></code> function to mask irrelevant entries with zero values so later multiplication of any irrelevant prediction with zero equals to zero. For example, if the valid length of two sequences excluding padding tokens (i.e., pads each sequence to the same length usually matching the longest sequence) are one and two, respectively, the remaining entries after the first one and the first two entries are cleared to zeros.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sequence_mask</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">valid_len</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Mask irrelevant entries in sequences.</span>

<span class="sd">  Args:</span>
<span class="sd">    X: torch.tensor</span>
<span class="sd">      Unmasked sequence as input</span>
<span class="sd">    valid_len: torch.tensor</span>
<span class="sd">      Valid Length</span>
<span class="sd">    value: int</span>
<span class="sd">      Mask valur</span>

<span class="sd">  Returns:</span>
<span class="sd">    X: torch.tensor</span>
<span class="sd">      Output post masking</span>
<span class="sd">  """</span>
  <span class="n">maxlen</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">((</span><span class="n">maxlen</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                      <span class="n">device</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">device</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&lt;</span> <span class="n">valid_len</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
  <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
  <span class="k">return</span> <span class="n">X</span>


<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sequence_mask</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sequence_mask</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">value</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1, 0, 0],
        [4, 5, 0]])
tensor([[[ 1.,  1.,  1.,  1.],
         [-1., -1., -1., -1.],
         [-1., -1., -1., -1.]],

        [[ 1.,  1.,  1.,  1.],
         [ 1.,  1.,  1.,  1.],
         [-1., -1., -1., -1.]]])
</pre></div>
</div>
</div>
</div>
<p>Now we can extend the softmax cross-entropy loss
to allow the masking of irrelevant predictions.
Initially,
masks for all the predicted tokens are set to one.
Once the valid length is given,
the mask corresponding to any padding token
will be cleared to zero.
In the end,
the loss for all the tokens
will be multiplied by the mask to filter out
irrelevant predictions of padding tokens in the loss.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MaskedSoftmaxCELoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  The softmax cross-entropy loss with masks.</span>
<span class="sd">  """</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">valid_len</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Forward pass of MaskedSoftmaxCELoss</span>

<span class="sd">    Args:</span>
<span class="sd">      pred: torch.tensor</span>
<span class="sd">        Predictions of shape: (`batch_size`, `num_steps`, `vocab_size`)</span>
<span class="sd">      label: torch.tensor</span>
<span class="sd">        Label of shape: (`batch_size`, `num_steps`)</span>
<span class="sd">      valid_len: torch.tensor</span>
<span class="sd">        Valid Length of shape (`batch_size`,)</span>

<span class="sd">    Returns:</span>
<span class="sd">      weighted_loss: float</span>
<span class="sd">        Weighted Loss</span>
<span class="sd">    """</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">sequence_mask</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">valid_len</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="s1">'none'</span>
    <span class="n">unweighted_loss</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">MaskedSoftmaxCELoss</span><span class="p">,</span>
                            <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">weighted_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">unweighted_loss</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">weighted_loss</span>


<span class="n">loss</span> <span class="o">=</span> <span class="n">MaskedSoftmaxCELoss</span><span class="p">()</span>
<span class="n">loss</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
     <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">),</span>
     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([2.3026, 1.1513, 0.0000])
</pre></div>
</div>
</div>
</div>
<p>In the following training loop,
we concatenate the special beginning-of-sequence token
and the original output sequence excluding the final token as
the input to the decoder.
This is called <em>teacher forcing</em> because
the original output sequence (token labels) is fed into the decoder.
Alternatively,
we could also feed the <em>predicted</em> token
from the previous time step
as the current input to the decoder.</p>
<div class="section" id="training">
<h3>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Training</span>
<span class="k">def</span> <span class="nf">train_seq2seq</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">tgt_vocab</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Train a model for sequence to sequence.</span>
<span class="sd">  """</span>

  <span class="k">def</span> <span class="nf">xavier_init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Function to initialise weights</span>

<span class="sd">    Args:</span>
<span class="sd">      m: nn.module</span>
<span class="sd">        Type of layer</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
      <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">_flat_weights_names</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">"weight"</span> <span class="ow">in</span> <span class="n">param</span><span class="p">:</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">param</span><span class="p">])</span>


  <span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">xavier_init_weights</span><span class="p">)</span>
  <span class="n">net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">MaskedSoftmaxCELoss</span><span class="p">()</span>
  <span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
  <span class="n">animator</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Animator</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">'epoch'</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">'loss'</span><span class="p">,</span>
                          <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">])</span>
  <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">timer</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Timer</span><span class="p">()</span>
    <span class="n">metric</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Accumulator</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Sum of training loss, no. of tokens</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_iter</span><span class="p">:</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
      <span class="n">X</span><span class="p">,</span> <span class="n">X_valid_len</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Y_valid_len</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
      <span class="n">bos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">tgt_vocab</span><span class="p">[</span><span class="s1">'&lt;bos&gt;'</span><span class="p">]]</span> <span class="o">*</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                          <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">dec_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">bos</span><span class="p">,</span> <span class="n">Y</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Teacher forcing</span>
      <span class="n">Y_hat</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dec_input</span><span class="p">,</span> <span class="n">X_valid_len</span><span class="p">)</span>
      <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">Y_hat</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Y_valid_len</span><span class="p">)</span>
      <span class="n">l</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Make the loss scalar for `backward`</span>
      <span class="n">d2l</span><span class="o">.</span><span class="n">grad_clipping</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">num_tokens</span> <span class="o">=</span> <span class="n">Y_valid_len</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
      <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
          <span class="n">metric</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">num_tokens</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">animator</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">metric</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">1</span><span class="p">],))</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'loss </span><span class="si">{</span><span class="n">metric</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">metric</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">timer</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1"> '</span>
        <span class="sa">f</span><span class="s1">'tokens/sec on </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can create and train an RNN encoder-decoder model
for sequence to sequence learning on the machine translation dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embed_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.1</span>
<span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span>
<span class="n">lr</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">=</span> <span class="mf">0.005</span><span class="p">,</span> <span class="mi">300</span>

<span class="n">train_iter</span><span class="p">,</span> <span class="n">src_vocab</span><span class="p">,</span> <span class="n">tgt_vocab</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">load_data_nmt</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">)</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">Seq2SeqEncoder</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">src_vocab</span><span class="p">),</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span>
                         <span class="n">dropout</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">Seq2SeqDecoder</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tgt_vocab</span><span class="p">),</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span>
                         <span class="n">dropout</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">EncoderDecoder</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">)</span>
<span class="n">train_seq2seq</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">tgt_vocab</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<p>To predict the output sequence token by token, at each decoder time step
the predicted token from the previous time step is fed into the decoder as an input.</p>
<p>Similar to training, at the initial time step the beginning-of-sequence (“&lt;bos&gt;”) token is fed into the decoder. This prediction process is illustrated in <code class="docutils literal notranslate"><span class="pre">seq2seq</span></code> figure. When the end-of-sequence (“&lt;eos&gt;”) token is predicted, the prediction of the output sequence is complete.</p>
<figure>
<img src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/static/seq2seq-predict.svg"/>
<figcaption>
  Source <a href="https://d2l.ai/">d2l.ai</a>
</figcaption>
</figure></div>
<div class="section" id="prediction">
<h3>Prediction<a class="headerlink" href="#prediction" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Prediction</span>
<span class="k">def</span> <span class="nf">predict_seq2seq</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">src_sentence</span><span class="p">,</span> <span class="n">src_vocab</span><span class="p">,</span>
                    <span class="n">tgt_vocab</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span>
                    <span class="n">device</span><span class="p">,</span> <span class="n">save_attention_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Predict for sequence to sequence.</span>

<span class="sd">  Args:</span>
<span class="sd">    net: nn.module</span>
<span class="sd">      Instance of model</span>
<span class="sd">    src_sentence: string</span>
<span class="sd">      Source Sentence</span>
<span class="sd">    src_vocab: dict</span>
<span class="sd">      Source vocabulary</span>
<span class="sd">    tgt_vocab: dict</span>
<span class="sd">      Target vocabulary</span>
<span class="sd">    num_steps: int</span>
<span class="sd">      Number of steps</span>
<span class="sd">    save_attention_weights: boolean</span>
<span class="sd">      If true, save attention weights</span>
<span class="sd">    device: string</span>
<span class="sd">      If available, GPU/CUDA. CPU otherwise.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Sequence predicted using tokenized target vocabulary</span>
<span class="sd">    obtained through attention weights</span>
<span class="sd">  """</span>
  <span class="c1"># Set `net` to eval mode for inference</span>
  <span class="n">net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
  <span class="n">src_tokens</span> <span class="o">=</span> <span class="n">src_vocab</span><span class="p">[</span><span class="n">src_sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span>
      <span class="n">src_vocab</span><span class="p">[</span><span class="s1">'&lt;eos&gt;'</span><span class="p">]]</span>
  <span class="n">enc_valid_len</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">src_tokens</span><span class="p">)],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
  <span class="n">src_tokens</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">truncate_pad</span><span class="p">(</span><span class="n">src_tokens</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">src_vocab</span><span class="p">[</span><span class="s1">'&lt;pad&gt;'</span><span class="p">])</span>

  <span class="c1"># Add the batch axis</span>
  <span class="n">enc_X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">src_tokens</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">enc_outputs</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">enc_X</span><span class="p">,</span> <span class="n">enc_valid_len</span><span class="p">)</span>
  <span class="n">dec_state</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">init_state</span><span class="p">(</span><span class="n">enc_outputs</span><span class="p">,</span> <span class="n">enc_valid_len</span><span class="p">)</span>

  <span class="c1"># Add the batch axis</span>
  <span class="n">dec_X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">tgt_vocab</span><span class="p">[</span><span class="s1">'&lt;bos&gt;'</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
      <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">output_seq</span><span class="p">,</span> <span class="n">attention_weight_seq</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
    <span class="n">Y</span><span class="p">,</span> <span class="n">dec_state</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">dec_X</span><span class="p">,</span> <span class="n">dec_state</span><span class="p">)</span>

    <span class="c1"># We use the token with the highest prediction likelihood as the input</span>
    <span class="c1"># of the decoder at the next time step</span>
    <span class="n">dec_X</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">dec_X</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="c1"># Save attention weights (to be covered later)</span>
    <span class="k">if</span> <span class="n">save_attention_weights</span><span class="p">:</span>
        <span class="n">attention_weight_seq</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">attention_weights</span><span class="p">)</span>

    <span class="c1"># Once the end-of-sequence token is predicted, the generation of the</span>
    <span class="c1"># output sequence is complete</span>
    <span class="k">if</span> <span class="n">pred</span> <span class="o">==</span> <span class="n">tgt_vocab</span><span class="p">[</span><span class="s1">'&lt;eos&gt;'</span><span class="p">]:</span>
        <span class="k">break</span>
    <span class="n">output_seq</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
  <span class="k">return</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tgt_vocab</span><span class="o">.</span><span class="n">to_tokens</span><span class="p">(</span><span class="n">output_seq</span><span class="p">)),</span> <span class="n">attention_weight_seq</span>
</pre></div>
</div>
</div>
</div>
<p>We can evaluate a predicted sequence by comparing it with the label sequence (the ground-truth). BLEU (Bilingual Evaluation Understudy), though originally proposed for evaluating machine translation results in <a class="reference external" href="https://dl.acm.org/doi/10.3115/1073083.1073135">Papieni et al., 2002</a>, has been extensively used in measuring the quality of output sequences for different applications.</p>
<p>In principle, for any <span class="math notranslate nohighlight">\(n\)</span>-grams in the predicted sequence, BLEU evaluates whether this <span class="math notranslate nohighlight">\(n\)</span>-grams appears in the label sequence.</p>
<p>Denote by <span class="math notranslate nohighlight">\(p_n\)</span> the precision of <span class="math notranslate nohighlight">\(n\)</span>-grams, which is the ratio of the number of matched <span class="math notranslate nohighlight">\(n\)</span>-grams in the predicted and label sequences to the number of <span class="math notranslate nohighlight">\(n\)</span>-grams in the predicted sequence.
To explain, given a label sequence <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, <span class="math notranslate nohighlight">\(C\)</span>, <span class="math notranslate nohighlight">\(D\)</span>, <span class="math notranslate nohighlight">\(E\)</span>, <span class="math notranslate nohighlight">\(F\)</span>, and a predicted sequence <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, <span class="math notranslate nohighlight">\(C\)</span>, <span class="math notranslate nohighlight">\(D\)</span>, we have <span class="math notranslate nohighlight">\(p_1 = 4/5\)</span>,  <span class="math notranslate nohighlight">\(p_2 = 3/4\)</span>, <span class="math notranslate nohighlight">\(p_3 = 1/3\)</span>, and <span class="math notranslate nohighlight">\(p_4 = 0\)</span>.</p>
<p>Besides, let <span class="math notranslate nohighlight">\(\mathrm{len}_{\text{label}}\)</span> and <span class="math notranslate nohighlight">\(\mathrm{len}_{\text{pred}}\)</span>
be the numbers of tokens in the label sequence and the predicted sequence, respectively.</p>
<p>Then, BLEU is defined as</p>
<div class="amsmath math notranslate nohighlight" id="equation-110432c3-c2ba-4581-b5f9-7bc83172a249">
<span class="eqno">(92)<a class="headerlink" href="#equation-110432c3-c2ba-4581-b5f9-7bc83172a249" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\exp\left(\min\left(0, 1 - \frac{\mathrm{len}_{\text{label}}}{\mathrm{len}_{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n},
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(k\)</span> is the longest <span class="math notranslate nohighlight">\(n\)</span>-grams for matching.</p>
<p>Based on the definition of BLEU in the above equation, whenever the predicted sequence is the same as the label sequence, BLEU is 1.</p>
<p>Moreover, since matching longer <span class="math notranslate nohighlight">\(n\)</span>-grams is more difficult, BLEU assigns a greater weight to a longer <span class="math notranslate nohighlight">\(n\)</span>-gram precision. Specifically, when <span class="math notranslate nohighlight">\(p_n\)</span> is fixed, <span class="math notranslate nohighlight">\(p_n^{1/2^n}\)</span> increases as <span class="math notranslate nohighlight">\(n\)</span> grows (the original paper uses <span class="math notranslate nohighlight">\(p_n^{1/n}\)</span>).</p>
<p>Furthermore, since predicting shorter sequences tends to obtain a higher <span class="math notranslate nohighlight">\(p_n\)</span> value, the coefficient before the multiplication term in the above equation
penalizes shorter predicted sequences.</p>
<p>For example, when <span class="math notranslate nohighlight">\(k=2\)</span>, given the label sequence <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, <span class="math notranslate nohighlight">\(C\)</span>, <span class="math notranslate nohighlight">\(D\)</span>, <span class="math notranslate nohighlight">\(E\)</span>, <span class="math notranslate nohighlight">\(F\)</span> and the predicted sequence <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, although <span class="math notranslate nohighlight">\(p_1 = p_2 = 1\)</span>, the penalty factor <span class="math notranslate nohighlight">\(\exp(1-6/2) \approx 0.14\)</span> lowers the BLEU.</p>
<p>We implement the BLEU measure as follows.</p>
</div>
<div class="section" id="evaluation-of-predicted-sequences">
<h3>Evaluation of Predicted Sequences<a class="headerlink" href="#evaluation-of-predicted-sequences" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Evaluation of Predicted Sequences</span>
<span class="k">def</span> <span class="nf">bleu</span><span class="p">(</span><span class="n">pred_seq</span><span class="p">,</span> <span class="n">label_seq</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Compute the BLEU Score</span>

<span class="sd">  Args:</span>
<span class="sd">    pred_seq: string</span>
<span class="sd">      Predicted Sequence</span>
<span class="sd">    label_seq: string</span>
<span class="sd">      Ground truth</span>
<span class="sd">    k: int</span>
<span class="sd">      Number of iterations</span>

<span class="sd">  Returns:</span>
<span class="sd">    score: float</span>
<span class="sd">      BLEU score</span>
<span class="sd">      The score between 0 and 1, indicates how</span>
<span class="sd">      similar the predicted and reference statements are.</span>
<span class="sd">  """</span>
  <span class="n">pred_tokens</span><span class="p">,</span> <span class="n">label_tokens</span> <span class="o">=</span> <span class="n">pred_seq</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">),</span> <span class="n">label_seq</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">)</span>
  <span class="n">len_pred</span><span class="p">,</span> <span class="n">len_label</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred_tokens</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">label_tokens</span><span class="p">)</span>
  <span class="n">score</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">len_label</span> <span class="o">/</span> <span class="n">len_pred</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">num_matches</span><span class="p">,</span> <span class="n">label_subs</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">len_label</span> <span class="o">-</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
      <span class="n">label_subs</span><span class="p">[</span><span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">label_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">n</span><span class="p">])]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">len_pred</span> <span class="o">-</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">label_subs</span><span class="p">[</span><span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pred_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">n</span><span class="p">])]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">num_matches</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">label_subs</span><span class="p">[</span><span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pred_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">n</span><span class="p">])]</span> <span class="o">-=</span> <span class="mi">1</span>
    <span class="n">score</span> <span class="o">*=</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">num_matches</span> <span class="o">/</span> <span class="p">(</span><span class="n">len_pred</span> <span class="o">-</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">score</span>
</pre></div>
</div>
</div>
</div>
<p>In the end,
we use the trained RNN encoder-decoder
to translate a few English sentences into French
and compute the BLEU of the results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">engs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'go .'</span><span class="p">,</span> <span class="s2">"i lost ."</span><span class="p">,</span> <span class="s1">'he</span><span class="se">\'</span><span class="s1">s calm .'</span><span class="p">,</span> <span class="s1">'i</span><span class="se">\'</span><span class="s1">m home .'</span><span class="p">]</span>
<span class="c1"># fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']</span>
<span class="n">fras</span> <span class="o">=</span> <span class="p">[</span> <span class="s1">'je suis chez moi .'</span><span class="p">,</span> <span class="s1">'j</span><span class="se">\'</span><span class="s1">ai perdu .'</span><span class="p">,</span><span class="s1">'va !'</span><span class="p">,</span> <span class="s1">'il est calme .'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">eng</span><span class="p">,</span> <span class="n">fra</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">engs</span><span class="p">,</span> <span class="n">fras</span><span class="p">):</span>
  <span class="n">translation</span><span class="p">,</span> <span class="n">attention_weight_seq</span> <span class="o">=</span> <span class="n">predict_seq2seq</span><span class="p">(</span><span class="n">net</span><span class="p">,</span>
                                                      <span class="n">eng</span><span class="p">,</span>
                                                      <span class="n">src_vocab</span><span class="p">,</span>
                                                      <span class="n">tgt_vocab</span><span class="p">,</span>
                                                      <span class="n">num_steps</span><span class="p">,</span>
                                                      <span class="n">DEVICE</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">eng</span><span class="si">}</span><span class="s1"> =&gt; </span><span class="si">{</span><span class="n">translation</span><span class="si">}</span><span class="s1">, bleu </span><span class="si">{</span><span class="n">bleu</span><span class="p">(</span><span class="n">translation</span><span class="p">,</span> <span class="n">fra</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-4-ethical-aspects">
<h1>Section 4: Ethical aspects<a class="headerlink" href="#section-4-ethical-aspects" title="Permalink to this headline">¶</a></h1>
<p><em>Time estimate: ~7mins</em></p>
<div class="section" id="video-5-ethics-of-representation-and-generation">
<h2>Video 5: Ethics of Representation and Generation<a class="headerlink" href="#video-5-ethics-of-representation-and-generation" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "55dec691157a49388581c4445b29e8bd"}
</script></div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h1>
<p>During this day, we have learned about modern RNNs and their variants. Now let’s see some ethical aspects of representation and Generation, and then we will close the tutorials with an overview.</p>
<div class="section" id="video-6-beyond-sequence">
<h2>Video 6: Beyond Sequence<a class="headerlink" href="#video-6-beyond-sequence" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "45583a24a42e420aa9d0b611158f30a9"}
</script></div>
</div>
</div>
<div class="section" id="airtable-submission-link">
<h2>Airtable Submission Link<a class="headerlink" href="#airtable-submission-link" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Airtable Submission Link</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span> <span class="k">as</span> <span class="n">IPydisplay</span>
<span class="n">IPydisplay</span><span class="o">.</span><span class="n">HTML</span><span class="p">(</span>
   <span class="sa">f</span><span class="s2">"""</span>
<span class="s2"> &lt;div&gt;</span>
<span class="s2">   &lt;a href= "</span><span class="si">{</span><span class="n">atform</span><span class="o">.</span><span class="n">url</span><span class="p">()</span><span class="si">}</span><span class="s2">" target="_blank"&gt;</span>
<span class="s2">   &lt;img src="https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/static/SurveyButton.png?raw=1"</span>
<span class="s2"> alt="button link end of day Survey" style="width:410px"&gt;&lt;/a&gt;</span>
<span class="s2">   &lt;/div&gt;"""</span> <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<div>
<a href="https://portal.neuromatchacademy.org/api/redirect/to/3412a777-eb0e-4312-9254-eec266f0bee4?data=eyJmb3JtX2lkIjogImFwcG43VmRQUnNlU29NWEVHIiwgInRhYmxlX25hbWUiOiAiVzJEM19UMiIsICJhbnN3ZXJzIjoge30sICJldmVudHMiOiBbeyJldmVudCI6ICJpbml0IiwgInRzIjogMTY1NTIyNDc1OC42MjAyMjQ3fSwgeyJldmVudCI6ICJWaWRlbyAxOiBSZWN1cnJlbnQgTmV1cmFsIE5ldHdvcmtzIiwgInRzIjogMTY1NTIyNDc2Mi4zODcxOTE4fSwgeyJldmVudCI6ICJDb2RpbmcgRXhlcmNpc2UgMS4xOiBWYW5pbGxhIFJOTiIsICJ0cyI6IDE2NTUyMjQ4NjYuMTQ5ODI5Nn0sIHsiZXZlbnQiOiAiVmlkZW8gMjogQmlkaXJlY3Rpb25hbCBSTk5zIiwgInRzIjogMTY1NTIyNDk1NC4zMDU1MTh9LCB7ImV2ZW50IjogIlZpZGVvIDM6IExTVE0sIEdSVSAmIFRoZSBNZW1vcnkgQ2VsbHMiLCAidHMiOiAxNjU1MjI0OTU0LjU1Mzc1ODF9LCB7ImV2ZW50IjogIkNvZGluZyBFeGVyY2lzZSAyLjE6IEltcGxlbWVudGluZyBMU1RNIiwgInRzIjogMTY1NTIyNDk1NC41ODAyMzJ9LCB7ImV2ZW50IjogIkNvZGluZyBFeGVyY2lzZSAyLjI6IEJpTFNUTSIsICJ0cyI6IDE2NTUyMjQ5NTQuNjIzNDU4MX0sIHsiZXZlbnQiOiAiVmlkZW8gNDogU2VxMlNlcSAmIEVuY29kZXItRGVjb2RlciBOZXRzIiwgInRzIjogMTY1NTIyNDk1NC45MTYzMjQ0fSwgeyJldmVudCI6ICJDb2RpbmcgRXhlcmNpc2UgMzogRW5jb2RlciIsICJ0cyI6IDE2NTUyMjQ5NTQuOTQzMjU1fSwgeyJldmVudCI6ICJWaWRlbyA1OiBFdGhpY3Mgb2YgUmVwcmVzZW50YXRpb24gYW5kIEdlbmVyYXRpb24iLCAidHMiOiAxNjU1MjI0OTYxLjczMDM4MDh9LCB7ImV2ZW50IjogIlZpZGVvIDY6IEJleW9uZCBTZXF1ZW5jZSIsICJ0cyI6IDE2NTUyMjQ5NjEuOTQ1NjE4OX0sIHsiZXZlbnQiOiAidXJsIGdlbmVyYXRlZCIsICJ0cyI6IDE2NTUyMjQ5NjEuOTY5MjYyNn1dfQ%3D%3D" target="_blank">
<img alt="button link end of day Survey" src="https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/static/SurveyButton.png?raw=1" style="width:410px"/></a>
</div></div></div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="bonus-attention">
<h1>Bonus: Attention<a class="headerlink" href="#bonus-attention" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-7-attention-mechanisms">
<h2>Video 7: Attention mechanisms<a class="headerlink" href="#video-7-attention-mechanisms" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "6772c553cba94003ac5a545acbe8a98e"}
</script></div>
</div>
<p>Previously, we designed an encoder-decoder architecture based on two RNNs for sequence to sequence learning. Specifically, the RNN encoder transforms a variable-length sequence into a fixed-shape context variable, then the RNN decoder generates the output (target) sequence token by token based on the generated tokens and the context variable. However, even though not all the input (source) tokens are useful for decoding a certain token, the same context variable that encodes the entire input sequence is still used at each decoding step. It is challenging for the models to deal with long sentences.</p>
<p>In <a class="reference external" href="https://arxiv.org/abs/1409.0473">Bahdanau et al., 2014</a>, the authors proposed a technique called attention. When predicting a token, if not all the input tokens are relevant, the model aligns (or attends) only to parts of the input sequence that are relevant to the current prediction.</p>
<p>In contrast to seq2seq model, the encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states to the decoder.</p>
<p>In order to focus on the parts of input relevant to the decoder, look at the set of encoder hidden states it received. Each encoder hidden state is at most associated with a certain word in the input sentence. We can assign each hidden state a score and multiply it with the softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores.</p>
<p>Reference Links:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/</a></p></li>
<li><p><a class="reference external" href="https://d2l.ai/chapter_attention-mechanisms/attention-cues.html">https://d2l.ai/chapter_attention-mechanisms/attention-cues.html</a></p></li>
</ul>
<p>Media 1: Sequence to Sequence model with Attention</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Media 1: Sequence to Sequence model with Attention</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">"https://jalammar.github.io/images/seq2seq_7.mp4"</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="n">HTML</span><span class="p">(</span><span class="sa">f</span><span class="s2">"""&lt;video src=</span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2"> width=750 controls/&gt;"""</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><video controls="" src="https://jalammar.github.io/images/seq2seq_7.mp4" width="750"></video></div></div>
</div>
<p>Media 2: Mapping input to output</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Media 2: Mapping input to output</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">"https://jalammar.github.io/images/seq2seq_9.mp4"</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="n">HTML</span><span class="p">(</span><span class="sa">f</span><span class="s2">"""&lt;video src=</span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2"> width=750 controls/&gt;"""</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><video controls="" src="https://jalammar.github.io/images/seq2seq_9.mp4" width="750"></video></div></div>
</div>
</div>
<div class="section" id="queries-keys-and-values">
<h2>Queries, Keys, and Values<a class="headerlink" href="#queries-keys-and-values" title="Permalink to this headline">¶</a></h2>
<p>To calculate the attention mechanism we make use of Queries, Keys and Values. But what are Queries, Keys and Values? Query, Value and Key are the transformations of the input vector.</p>
<p>In an attention mechanism the context vector is computed as a weighted sum of values, where the weight assigned to each value is computed through an attention score. The score is usually the dot product between the query and key. The scores then go through the softmax function to yield a set of weights whose sum equals 1.</p>
<p>The query is from the decoder hidden state whereas the key and value are from the encoder hidden state.</p>
<p>Take a minute and look at this <a class="reference external" href="https://www.kdnuggets.com/2021/01/attention-mechanism-deep-learning-explained.html">article</a>. It has detailed graphical explanation on how to calculate attention scores.</p>
<div class="section" id="bonus-coding-exercise-attention-for-text-classification">
<h3>Bonus Coding Exercise: Attention for Text Classification<a class="headerlink" href="#bonus-coding-exercise-attention-for-text-classification" title="Permalink to this headline">¶</a></h3>
<p>Until now, we looked at attention aimed at seq2seq networks. Let’s try implementing attention for the above IMDB sentiment analysis dataset. Previously, using the LSTM, the classification completely depended on the last hidden state. In this exercise, we will compute the attention scores between the last hidden state and output of each sequence. The final attention vector will be the weighted average of the outputs at each sequence, with the weights being the attention scores. Lastly, we will concatenate the attention vector and the last hidden state to get the final output.</p>
<p>For simplicity’s sake, let’s implement attention over an LSTM with 1 layer.</p>
<p><a class="reference external" href="https://github.com/prakashpandey9/Text-Classification-Pytorch/blob/master/main.py"><em>Code reference</em></a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AttentionModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Attention Model with following structure:</span>
<span class="sd">  nn.Embedding(vocab_size, embedding_length) + nn.Parameter(weights, requires_grad=False) # Embedding Layer</span>
<span class="sd">  nn.LSTM(embedding_length, hidden_size) # LSTM layer</span>
<span class="sd">  nn.Linear(2*hidden_size, output_size) # First Fully Connected layer</span>
<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span>
               <span class="n">embedding_length</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Initialize parameters of AttentionModel</span>

<span class="sd">    Args:</span>
<span class="sd">      batch_size: int</span>
<span class="sd">        Batch size</span>
<span class="sd">      output_size: int</span>
<span class="sd">        Size of output layer</span>
<span class="sd">      hidden_size: int</span>
<span class="sd">        Size of hidden layer</span>
<span class="sd">      vocab_size: int</span>
<span class="sd">        Vocabulary size</span>
<span class="sd">      weights: torch.tensor</span>
<span class="sd">        Attention Weights</span>
<span class="sd">      device: string</span>
<span class="sd">        GPU/CUDA if available. CPU otherwise.</span>
<span class="sd">      embedding_length: int</span>
<span class="sd">        Length of the embeddding</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing</span>
<span class="sd">    """</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AttentionModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_length</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">embedding_length</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_seq</span> <span class="o">=</span> <span class="n">sentence_length</span>

  <span class="k">def</span> <span class="nf">attention_net</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lstm_output</span><span class="p">,</span> <span class="n">final_state</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Returns hidden states based on AttentionNet</span>

<span class="sd">    Args:</span>
<span class="sd">      lstm_output : torch.tensor</span>
<span class="sd">        LSTM Output of shape: (num_seq, batch_size, hidden_size)</span>
<span class="sd">      final_state : torch.tensor</span>
<span class="sd">        Final State of shape: (1, batch_size, hidden_size)</span>

<span class="sd">    Returns:</span>
<span class="sd">      new_hidden_state: torch.tensor</span>
<span class="sd">        Weighted LSTM output</span>
<span class="sd">    """</span>
    <span class="c1">####################################################</span>
    <span class="c1"># Implement the AttentionNet</span>
    <span class="c1"># Fill in missing code below (...)</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"perform the convolution"</span><span class="p">)</span>
    <span class="c1">####################################################</span>
    <span class="c1"># Permute the output to get the shape (batch_size, num_seq, hidden_size)</span>
    <span class="c1"># Get the attention weights</span>
    <span class="c1"># Use torch.bmm to compute the attention weights between each output and last hidden state</span>
    <span class="c1"># Pay attention to the tensor shapes, you may have to use squeeze and unsqueeze functions</span>
    <span class="c1"># Softmax the attention weights</span>
    <span class="c1"># Get the new hidden state, use torch.bmm to get the weighted lstm output</span>
    <span class="c1"># Pay attention to the tensor shapes, you may have to use squeeze and unsqueeze functions</span>
    <span class="n">lstm_output</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">attn_weights</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># Expected shape: (batch_size, num_seq)</span>
    <span class="n">soft_attn_weights</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">new_hidden_state</span> <span class="o">=</span> <span class="o">...</span>

    <span class="k">return</span> <span class="n">new_hidden_state</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_sentences</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Forward pass of NeuralNet</span>

<span class="sd">    Args:</span>
<span class="sd">      input_sentences: string</span>
<span class="sd">        Input Sentences</span>

<span class="sd">    Returns:</span>
<span class="sd">      logits: torch.tensor</span>
<span class="sd">        Output of the final fully connected layer</span>
<span class="sd">    """</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span><span class="p">(</span><span class="n">input_sentences</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">h_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">c_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">final_hidden_state</span><span class="p">,</span> <span class="n">final_cell_state</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">h_0</span><span class="p">,</span> <span class="n">c_0</span><span class="p">))</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_net</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">final_hidden_state</span><span class="p">)</span>
    <span class="n">final_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">attn_output</span><span class="p">,</span> <span class="n">final_hidden_state</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">final_output</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logits</span>


<span class="c1"># Uncomment to check AttentionModel class</span>
<span class="c1"># attention_model = AttentionModel(32, 2, 16, 20, 200, TEXT.vocab.vectors, DEVICE)</span>
<span class="c1"># print(attention_model)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AttentionModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Attention Model with following structure:</span>
<span class="sd">  nn.Embedding(vocab_size, embedding_length) + nn.Parameter(weights, requires_grad=False) # Embedding Layer</span>
<span class="sd">  nn.LSTM(embedding_length, hidden_size) # LSTM layer</span>
<span class="sd">  nn.Linear(2*hidden_size, output_size) # First Fully Connected layer</span>
<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_length</span><span class="p">,</span> <span class="n">sentence_length</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Initialize parameters of AttentionModel</span>

<span class="sd">    Args:</span>
<span class="sd">      batch_size: int</span>
<span class="sd">        Batch size</span>
<span class="sd">      output_size: int</span>
<span class="sd">        Size of output layer</span>
<span class="sd">      hidden_size: int</span>
<span class="sd">        Size of hidden layer</span>
<span class="sd">      vocab_size: int</span>
<span class="sd">        Vocabulary size</span>
<span class="sd">      weights: torch.tensor</span>
<span class="sd">        Attention Weights</span>
<span class="sd">      device: string</span>
<span class="sd">        GPU/CUDA if available. CPU otherwise.</span>
<span class="sd">      embedding_length: int</span>
<span class="sd">        Length of the embeddding</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing</span>
<span class="sd">    """</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AttentionModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_length</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">embedding_length</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_seq</span> <span class="o">=</span> <span class="n">sentence_length</span>

  <span class="k">def</span> <span class="nf">attention_net</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lstm_output</span><span class="p">,</span> <span class="n">final_state</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Returns hidden states based on AttentionNet</span>

<span class="sd">    Args:</span>
<span class="sd">      lstm_output : torch.tensor</span>
<span class="sd">        LSTM Output of shape: (num_seq, batch_size, hidden_size)</span>
<span class="sd">      final_state : torch.tensor</span>
<span class="sd">        Final State of shape: (1, batch_size, hidden_size)</span>

<span class="sd">    Returns:</span>
<span class="sd">      new_hidden_state: torch.tensor</span>
<span class="sd">        Weighted LSTM output</span>
<span class="sd">    """</span>
    <span class="c1"># Permute the output to get the shape (batch_size, num_seq, hidden_size)</span>
    <span class="c1"># Get the attention weights</span>
    <span class="c1"># Use torch.bmm to compute the attention weights between each output and last hidden state</span>
    <span class="c1"># Pay attention to the tensor shapes, you may have to use squeeze and unsqueeze functions</span>
    <span class="c1"># Softmax the attention weights</span>
    <span class="c1"># Get the new hidden state, use torch.bmm to get the weighted lstm output</span>
    <span class="c1"># Pay attention to the tensor shapes, you may have to use squeeze and unsqueeze functions</span>
    <span class="n">lstm_output</span> <span class="o">=</span> <span class="n">lstm_output</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">final_state</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">lstm_output</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
    <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="p">([</span><span class="n">batch_size</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">num_seq</span><span class="p">]))</span> <span class="c1"># Expected shape: (batch_size, num_seq)</span>
    <span class="n">soft_attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">new_hidden_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">lstm_output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">soft_attn_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_hidden_state</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_sentences</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Forward pass of NeuralNet</span>

<span class="sd">    Args:</span>
<span class="sd">      input_sentences: string</span>
<span class="sd">        Input Sentences</span>

<span class="sd">    Returns:</span>
<span class="sd">      logits: torch.tensor</span>
<span class="sd">        Output of the final fully connected layer</span>
<span class="sd">    """</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span><span class="p">(</span><span class="n">input_sentences</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">h_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">c_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">final_hidden_state</span><span class="p">,</span> <span class="n">final_cell_state</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">h_0</span><span class="p">,</span> <span class="n">c_0</span><span class="p">))</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_net</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">final_hidden_state</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">final_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">attn_output</span><span class="p">,</span> <span class="n">final_hidden_state</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">final_output</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>

<span class="c1"># Uncomment to check AttentionModel class</span>
<span class="n">attention_model</span> <span class="o">=</span> <span class="n">AttentionModel</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">vectors</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">attention_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AttentionModel(
  (word_embeddings): Embedding(20, 200)
  (lstm): LSTM(200, 16)
  (fc1): Linear(in_features=32, out_features=2, bias=True)
)
</pre></div>
</div>
</div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AttentionModel</span><span class="p">(</span>
  <span class="p">(</span><span class="n">word_embeddings</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
  <span class="p">(</span><span class="n">lstm</span><span class="p">):</span> <span class="n">LSTM</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
  <span class="p">(</span><span class="n">fc1</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Reload dataset using the default params since variables have been overwritten</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Reload dataset using the default params since variables have been overwritten</span>
<span class="n">TEXT</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">valid_iter</span><span class="p">,</span> <span class="n">test_iter</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data loading is completed. Sentence length: 50, Batch size: 32, and seed: 2021
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>  <span class="c1"># Initially was 16</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">embedding_length</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Initially was 12</span>
<span class="n">sentence_length</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">vectors</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>

<span class="n">attention_model</span> <span class="o">=</span> <span class="n">AttentionModel</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span>
                                 <span class="n">output_size</span><span class="p">,</span>
                                 <span class="n">hidden_size</span><span class="p">,</span>
                                 <span class="n">vocab_size</span><span class="p">,</span>
                                 <span class="n">embedding_length</span><span class="p">,</span> <span class="n">sentence_length</span><span class="p">,</span>
                                 <span class="n">word_embeddings</span><span class="p">,</span>
                                 <span class="n">DEVICE</span><span class="p">)</span>
<span class="n">attention_model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">attention_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">attention_train_loss</span><span class="p">,</span> <span class="n">attention_train_acc</span><span class="p">,</span> <span class="n">attention_validation_loss</span><span class="p">,</span> <span class="n">attention_validation_acc</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">attention_model</span><span class="p">,</span>
                                                                                                       <span class="n">DEVICE</span><span class="p">,</span>
                                                                                                       <span class="n">train_iter</span><span class="p">,</span>
                                                                                                       <span class="n">valid_iter</span><span class="p">,</span>
                                                                                                       <span class="n">epochs</span><span class="p">,</span>
                                                                                                       <span class="n">learning_rate</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"--- Time taken to train = </span><span class="si">%s</span><span class="s2"> seconds ---"</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">attention_start_time</span><span class="p">))</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">attention_model</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="se">\n\n</span><span class="s1">Test Accuracy: </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">}</span><span class="s1">%'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Random seed 2021 has been set.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1, Training Loss: 0.6925, Training Accuracy:  51.79%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Validation Loss: 0.6916, Validation Accuracy:  51.96%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 2, Training Loss: 0.6844, Training Accuracy:  55.26%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Validation Loss: 0.6872, Validation Accuracy:  53.65%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 3, Training Loss: 0.6753, Training Accuracy:  58.72%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Validation Loss: 0.6819, Validation Accuracy:  55.93%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 4, Training Loss: 0.6633, Training Accuracy:  61.89%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Validation Loss: 0.6757, Validation Accuracy:  57.68%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 5, Training Loss: 0.6475, Training Accuracy:  64.25%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Validation Loss: 0.6680, Validation Accuracy:  59.17%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 6, Training Loss: 0.6265, Training Accuracy:  67.22%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Validation Loss: 0.6578, Validation Accuracy:  60.81%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 7, Training Loss: 0.5981, Training Accuracy:  70.10%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Validation Loss: 0.6451, Validation Accuracy:  62.65%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 8, Training Loss: 0.5610, Training Accuracy:  73.31%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Validation Loss: 0.6318, Validation Accuracy:  64.37%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 9, Training Loss: 0.5178, Training Accuracy:  76.40%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Validation Loss: 0.6248, Validation Accuracy:  65.51%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 10, Training Loss: 0.4742, Training Accuracy:  78.90%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Validation Loss: 0.6217, Validation Accuracy:  66.63%
--- Time taken to train = 1425.5072739124298 seconds ---
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test Accuracy: 65.76%
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">plot_train_val</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span>
               <span class="n">attention_train_acc</span><span class="p">,</span>
               <span class="n">attention_validation_acc</span><span class="p">,</span>
               <span class="s1">'train accuracy'</span><span class="p">,</span>
               <span class="s1">'val accuracy'</span><span class="p">,</span>
               <span class="s1">'attention on IMDB text classification'</span><span class="p">,</span>
               <span class="s1">'loss'</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="s1">'C0'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">plot_train_val</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span>
               <span class="n">attention_train_loss</span><span class="p">,</span>
               <span class="n">attention_validation_loss</span><span class="p">,</span>
               <span class="s1">'train loss'</span><span class="p">,</span>
               <span class="s1">'val loss'</span><span class="p">,</span>
               <span class="s1">''</span><span class="p">,</span>
               <span class="s1">'loss'</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="s1">'C1'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W2D3_Tutorial2_128_0.png" src="../../../_images/W2D3_Tutorial2_128_0.png">
</img></div>
</div>
</div>
</div>
</div>
<script type="application/vnd.jupyter.widget-state+json">
{"state": {"e36b148f219c48e597434ea04135d0ba": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "af4c2db02f17417b9d8ed2b61cd74e3a": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_e36b148f219c48e597434ea04135d0ba", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://www.bilibili.com/video/BV1Ng41177az\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<__main__.BiliVideo at 0x7f9644651d10>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://player.bilibili.com/player.html?bvid=BV1Ng41177az&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "}}]}}, "e7627e3572024a278a4136c1ed2f2959": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "f66ceffb8fa54df7918faef02881c5f3": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_e7627e3572024a278a4136c1ed2f2959", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://youtube.com/watch?v=hIHocwdyY7M\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f9640013d50>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://www.youtube.com/embed/hIHocwdyY7M?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ", "image/jpeg": "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\n"}}]}}, "45f2458a4c034282b7830c4025489027": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c6d960a8cbae4b3d8fe1a3945b536c4e": {"model_name": "TabModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TabModel", "_titles": {"0": "Youtube", "1": "Bilibili"}, "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TabView", "box_style": "", "children": ["IPY_MODEL_f66ceffb8fa54df7918faef02881c5f3", "IPY_MODEL_af4c2db02f17417b9d8ed2b61cd74e3a"], "layout": "IPY_MODEL_45f2458a4c034282b7830c4025489027", "selected_index": 0}}, "28af94669b55417d9194228486cedfef": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8d039a86777544348bd0160728c37e63": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_28af94669b55417d9194228486cedfef", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://www.bilibili.com/video/BV1X64y1x7BA\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<__main__.BiliVideo at 0x7f9608e0a190>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://player.bilibili.com/player.html?bvid=BV1X64y1x7BA&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "}}]}}, "d4842070faac42dc90a9e0fd47c64171": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "eb5107831bf64b6788a51bcfd81dcf3e": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_d4842070faac42dc90a9e0fd47c64171", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://youtube.com/watch?v=TpgJwqB4i1c\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f9608dcef90>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://www.youtube.com/embed/TpgJwqB4i1c?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ", "image/jpeg": "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\n"}}]}}, "ab8d42258ab34ba282b07d46175a1381": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "71ae824511164c7cb9642bb2f9bcb5bb": {"model_name": "TabModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TabModel", "_titles": {"0": "Youtube", "1": "Bilibili"}, "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TabView", "box_style": "", "children": ["IPY_MODEL_eb5107831bf64b6788a51bcfd81dcf3e", "IPY_MODEL_8d039a86777544348bd0160728c37e63"], "layout": "IPY_MODEL_ab8d42258ab34ba282b07d46175a1381", "selected_index": 0}}, "b22c94adac67447d952ab441b54abf30": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8b4e8337a4d04a79a873d9c2f3318cb7": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_b22c94adac67447d952ab441b54abf30", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://www.bilibili.com/video/BV1Zq4y1X7kj\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<__main__.BiliVideo at 0x7f95cebb50d0>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://player.bilibili.com/player.html?bvid=BV1Zq4y1X7kj&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "}}]}}, "557adc04b3e14ee8bc2c5b3a1a1bf5ea": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "0b5f76ad2d824c0d8fe83b7f5a3b5034": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_557adc04b3e14ee8bc2c5b3a1a1bf5ea", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://youtube.com/watch?v=qC7O-2sXLAU\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f963ff6f110>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://www.youtube.com/embed/qC7O-2sXLAU?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ", "image/jpeg": "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\n"}}]}}, "12f6ed79cf9343888ea6553ef6631297": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "5536a83f548441e8b38f3af37e90780e": {"model_name": "TabModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TabModel", "_titles": {"0": "Youtube", "1": "Bilibili"}, "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TabView", "box_style": "", "children": ["IPY_MODEL_0b5f76ad2d824c0d8fe83b7f5a3b5034", "IPY_MODEL_8b4e8337a4d04a79a873d9c2f3318cb7"], "layout": "IPY_MODEL_12f6ed79cf9343888ea6553ef6631297", "selected_index": 0}}, "f4a719479b494956805918e092c7616a": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "372676dbf15146edb38d26da05de14d1": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_f4a719479b494956805918e092c7616a", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://www.bilibili.com/video/BV1WM4y1N7P7\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<__main__.BiliVideo at 0x7f96080149d0>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://player.bilibili.com/player.html?bvid=BV1WM4y1N7P7&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "}}]}}, "2c7199a22f2741d4bdf4c0b6a5cf2fe2": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "6128f7cb096245dcb6179330a2b9148e": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_2c7199a22f2741d4bdf4c0b6a5cf2fe2", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://youtube.com/watch?v=q-hf7mnZsXo\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f95ced5aa50>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://www.youtube.com/embed/q-hf7mnZsXo?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ", "image/jpeg": "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\n"}}]}}, "240c019484e5419cb75f76b0cc418557": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "2cf1394d54744506b9985fbcf1fab91e": {"model_name": "TabModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TabModel", "_titles": {"0": "Youtube", "1": "Bilibili"}, "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TabView", "box_style": "", "children": ["IPY_MODEL_6128f7cb096245dcb6179330a2b9148e", "IPY_MODEL_372676dbf15146edb38d26da05de14d1"], "layout": "IPY_MODEL_240c019484e5419cb75f76b0cc418557", "selected_index": 0}}, "68b57cb68fed4a7d927c6365b12daeda": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "dd8dbda61efc43e4aa717a206245834e": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_68b57cb68fed4a7d927c6365b12daeda", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://www.bilibili.com/video/BV1v54y1J7SC\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<__main__.BiliVideo at 0x7f963f154050>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://player.bilibili.com/player.html?bvid=BV1v54y1J7SC&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "}}]}}, "1937965837b44ea0849035d3bb4b30fe": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "adcba15f9d45489384485478436780c2": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_1937965837b44ea0849035d3bb4b30fe", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://youtube.com/watch?v=2ULMWtaGQrQ\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f95e9e8b390>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://www.youtube.com/embed/2ULMWtaGQrQ?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ", "image/jpeg": "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\n"}}]}}, "1fd53f8bbd604fb0a031204e58ceed6e": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "55dec691157a49388581c4445b29e8bd": {"model_name": "TabModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TabModel", "_titles": {"0": "Youtube", "1": "Bilibili"}, "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TabView", "box_style": "", "children": ["IPY_MODEL_adcba15f9d45489384485478436780c2", "IPY_MODEL_dd8dbda61efc43e4aa717a206245834e"], "layout": "IPY_MODEL_1fd53f8bbd604fb0a031204e58ceed6e", "selected_index": 0}}, "fd83fe398c03487594f6724c16ae909d": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "56d281e17e184491b787d6093f6bc075": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_fd83fe398c03487594f6724c16ae909d", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://www.bilibili.com/video/BV1864y1x7Ek\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<__main__.BiliVideo at 0x7f9624aeb250>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://player.bilibili.com/player.html?bvid=BV1864y1x7Ek&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "}}]}}, "72e7805adae0434482e4be28ccaf0333": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "fc5c1dd160ca4053836a5259accefaa0": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_72e7805adae0434482e4be28ccaf0333", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://youtube.com/watch?v=lVcpMSJileA\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f9624aeb550>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://www.youtube.com/embed/lVcpMSJileA?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ", "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDBoYFhoaGRoeHRsfIiomIiIiIS0qLSgtLi8xMC0tLS01QlBCNzhLOS8tRWFFS1NWW11bMkFlbWRYbFBZW1cBERISGRYZLRsbJVc3NTdXV1dXV1dXV1dXV1dXV1dXV1dXV1dXV1dXV1dXV1dXV1dXV1dXV1dXV11XV11XXVdXV//AABEIAWgB4AMBIgACEQEDEQH/xAAbAAEAAgMBAQAAAAAAAAAAAAAAAwQCBQcBBv/EAEgQAAIBAgIECQoEBAUEAQUAAAABAgMRBCEFEjFRExRBU2FxkZLSBhYXIjJUgZOx0zNScqEjQsHRFWKC4fBjc7LxsyQmNTZD/8QAFwEBAQEBAAAAAAAAAAAAAAAAAAECA//EAB4RAQEBAQEBAAIDAAAAAAAAAAABESECMVFxEkFh/9oADAMBAAIRAxEAPwDn4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJ+Ky3ocVlvQNQAn4rLehxWW9A1ACfist6HFZb0DUAJ+Ky3ocVlvQNQAn4rLehxWW9A1ACfist6HFZb0DUAJ+Ky3ocVlvQNQAn4rLehxWW9A1ACfist6HFZb0DUAJ+Ky3ocVlvQNQAn4rLehxWW9A1ACfist6HFZb0DUAJ+Ky3ocVlvQNQAn4rLehxWW9A1ACfist6HFZb0DUAJ+Ky3ocVlvQNQAn4rLehxWW9A1ACfist6HFZb0DUAJ+Ky3ocVlvQNQAn4rLehxWW9A1ACfist6HFZb0DUAJ+Ky3ocVlvQNQAn4rLehxWW9A1ACfist6MZ4dxV3YCIAAAAAAAAAAAAAAAGyABWQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACPEewyQjxHsMCiACNAAAGUIOTsjE2mjMG5SVk22stwFWng5PbkQ1aTi+g+lxGArQVpLPduKVTDvbKNsuUmp2NGCbEwtLLYQlUAAGyABWQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACPE+wyQjxPsMCiACNAAAH3PkjGnCnGtV9VKL1ZPl5LLe9uSPhjpmDwaeGjKMZOMoLVtnqpxUrpPKz5epfDPr4sm1Xr4inXblF+qtt8irXp054edmmtl9zuS06NSlhZ6k2lUbaTSexJN57H/AGKeHw+rTutVyXI45W/zf82mJ5jr6tzsfOYuyhZrM15b0ndVpRbvquxUOrhAABWyABWQAAHJbzzWW9H3GisZHC6EjiOChUkqjVpLfUa2lLzzi8qmAoSjyq62fGIHyoPo/KvRlGnHD4nDLVo4iN9XkTaUk0uS6ezksfOxi20km29iSu31IDwEtbDVKdnUpzgns1oON+0xhTlK+rFyttsm7AYAGWpK2tqvV32du0DEElGhOpfg4TnbbqxcrddjGpCUXqyi4yW1STT7GBiDJQk02otpbWlkutkkcLVcddUqjht1lCWrbfe1gIT08JaOHqVL8HTnO23Ui5W67ARXB9RoOkv8J0i5RWtFvas16q7D5cAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABHifYZIR4n2GBRABGgAlhhpy2RfxyAiPuPJDS0uKVac5N8HbV5WovkX726z5KngW025JW+LN55O16dCUoyvq1Ypp7nG+XZJ9iJ6nF8+utlX0jTlUVOMppR9XVaa6yHSul+BorVScnlG/V7XwJsTODm5KT+J89pe1Wokn7KSv0u7a+hjzNrr79ZGobbd3m2eE08NJdPURNW2nRx14AANkACsgAA+50VQoVNBxjiKrpUnUleatk1UdtqfKUqWhtEayvj5NbnKMU/jq5ElOlKfk6owjKcuF2RTb/F3I+ajozEyaSw9dt/9Kf8AYg33lvCspUFqRjhIR1aGpK62cvTZKy3La8y5gaU8DoynXw9F1cViLPWVNz1IvNZLktb4sw0xSeG0JQw9f8aU1qxvdx9Zyt8Fl8SxUxmJlofC1cFOSlRShVUUm7RjqvJp7Gk+p3CoNCaSx1WsqGNo1atCreMuEoOKjdZO6ila+/eVdEy/w7TMqF/4cpcHm+SaUqd97u0viyPQ2k9J4ysqdPEzSz1p6kGodeX7Go0zKpxqrwlbhakZWdVK13FJZW3Wt8Coz03o10MbVoRj/P8Aw1vU84pdtvgfQafw6dTR+iqcso6rm+l5X67a7/1I2eHw0cbUwGkXZKFOXC/qjs7Jaz+B8nhtLqWlI4uplF1ru/JB+qr9UbdgG28odPVMJV4ngrUKdFJNxim22k+VPes9rdyXA4h6WwdelXUXiKMdanVSSbve17dKs+TNGs8tMBOljalVp8FVtKM+S9kmm991+5sPJGk8LhsXjKycKbglC+Wta+zrbSW8K98j6sI6Ox06kNeEXruL/m1YJ2/YqYHyxxjxNPXlF05TjF01BJJNpZPbl1mfk4raH0kuh/8Agj53Bfj0f+5D/wAkEb7yi0TF6XjQprUVZwbtlbWb1muxvrLPlFp+rha3FME1Qp0Uk9WMW22k+VPLNdN7mflHio0NN0KsvZjGnrPcm5Jv4J3Nb5a4GdLG1KrX8KraUZ8nspNX35djCt5R0rxvRGNnOMVWjFxqOKtr+qtWXZl8D4Q+x0TgalLQuOnUi48LFuKeT1UrXt0u58cEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAI8T7DJDCtFuLSAoAm4tLd+5Pg6CjK8+TYRbUmDw+onKS9bk6CzN7CNTyz27fqeymrLoOkxh7G9lL4Mivqy3r+XPZyksZpa253MZ6r+guVYmeNkk3bPp5Cok+Xkz+LzPdX1dud9h6thmSRbbXlsz2UFJZo9TDeRriKGIo6j3p7CIv4iGsrLaVuLS3fuYrUq4AAgAANto/yjxWGpKlRnGME20nBPa7vN9JYl5Y49q3DJdKpx/qjQgCfF4yrXnr1qkqkt8n+yWxLqJtG6Vr4WTlQqON/aW2L60//AGUgBvMX5XY2rBwdSME9rpx1W/jm18DU4XC1KstSlBzla+rHbZEJNhMVUoVFUpTcJrZJW5du0D6irwmA0PKjVbjWxM3aDecYtJS/Zdsj5EsYzG1cRPXrVJVJWteXItyWxFcDcaP8p8Xh4KnCopQWyM461uhPbbouV9J6axOLa4apeK2RStFfBbfjc14AuYbSdalRq0ISSp1fbVk75W28mRVpzcZRktsWmutO6MQBa0jpCriqnC1mpTso3SSyV7ZLrZe0d5T4vDU1ThUUoL2VOOtq9Ce2xpwBta/lFi6ka0Z1daNZWmnFbNy3I1QAAAAAAAAAAAAAAAAAAAAAAAAAAAAZcHL8r7D3g5flfYzeQ9ldSMgNDwcvyvsY4OX5X2M3wA0PBy/K+xjg5flfYzfHl+QDRcHL8r7GODl+V9jN8ANDwcvyvsY4OX5X2M3rZjTqayvlbrBjScHL8r7GODl+V9hvUz0DQOD3PsHBy/K+w3zVz0DQ8HL8r7BwcvyvsN8ANDwcvyvsHBy/K+w3wA0DhLc+wxN5iPw5/pf0NGAAAAAAbjQegJYtTqzqRo4en7dSXa0upbW9/KbCOh9EyerHSE1LZeVlHtcUv3LGgYwxujKmAVRU6ynrJP8AmWspJ25VfJ7sjUYzyVx1K96Dmt9NqX7e1+wEOnNCVcFUUZtShJXhNbJW25cjzWXSaw2OO0vXq0KWGqqKjQat6rU8k1aV3ue5chf0ToSjxV4zGzlChe0IR9qfJ9d2697AfPg+owuB0Zjm6OH4bD12m4cI9ZSsr22vsumabCaJq1cWsLbVqazjK+ajb2n0q3blvAoHp9Li6WicPUdCccRVlF6s6sZZRfLZXV7dT+J5p3QFDDYGlXpVHUc6iSnfJwkpSjlvskB80Dd4rRVKGisPilrcLUqOMs8rXqbF/pRX8nMDDE4ynRqX1Ja17Ozyi2s/gBrAbJ6LlUx88LR52cIt52jFvNvoSNti8PorCTdGqsRiKkcpyg7JPdtXZn1gfLg3mnNC06VKnisLN1MNUyz9qDzyfRk10NFzA6Aw09HUcXWqSpLWk6sk73ipSioxW9vVA+XB9bgNH6KxspUaCr0atm4ym761ui7XwyZ8/h9F1KmL4qrcJryg3yLVb1n1ZNgUgfUYrD6JwlTgKqxGIqRdpzi7KL5VZNbOi5S8odCww8aVfDzdTD1vYbzae23Tls5cmBpAfUVND4PA0qbx7qVK9RXVKm7aq6Xddt+pEdfQ+FxWGqV9HupGVJXqUambttutu58rva2QHzYAAAAAAAAAAAAAAAAAAAADfw9ldSPTyHsrqR6AB6eAA2GxcAmRVatmlfN/83GCqa0snktnT0o9km3ts+RpZ9JGs/L2bur3Vrbr2e89TurJr4r+hlfLcQRVmrJ9OSz6bsix7JqM4u+drW/2M417vK3xf9DCtHlzfQv7iKkkrW5MrWKvLE8ZZXf7HrkkrvIwhe2e3rPW5cjVhrDOMk1dO6B5DZssZFR4D08AjxH4c/0v6GjN5iPw5/pf0NGAAAAAAbLE6Gq0cLRxetFwqNauq3eLabV8stluskwflLjaNtWvKSX8tT112vP9yzoLyijQpSw2IpcNhpX9XK8b5uyeTV8+SzLTeg/atX/R/E7L/wC4E+nqkMbounjpU1CtGeo7fzLW1Wulcq3Zot46thIaK0fxmjUq03CGqoS1bS1M75r/ADHz+ntPLEwp0KNPgcNT9mGV27ZN2yW15Z7ybRGnaKwzweNpyqUL3jKPtQzv9d3UBPgtJaMp1oTo4LE8JF3habk79WvmX/J7GQr6br1VCdPXou0Zq0k1wad11K5RweP0ZgpOth1XxFZJ6nCJJRvltsu2zZpKWlq0cXxvWTq6zk9zvk423WyArYm/C1Nb2teV+u7v+59Fjtb/AADCa1/x3a+69Wx7isXonFT4eqsRSqPOdOGyT67cu9NFzykxMK2h8LOnT4ODrJRhujFVIr9kBUx//wCv4T/vP61Sn5FK+kqPQpt9xk2h9NYfijwWNhJ0tZyhOG2Oet17bu632LOj9MaOwVWLw0Ks9bKpVqLNR22jHLltyL4gT+Tdv8cxd/8Ar26+Ej/S5SxWM0VwtTXweIc9eWs+FftXet/Pvuat6VlTx88VR5as5xT5YybyfWmbfF4vRWLnw1XjFCq/bjBJqT37H25AeY3TGGejqmGw+FxFOnKSalL1oqSkm/Wu937mWJf/ANvYbprSv36j/oU9N6cp1KEMJhKbp4aGfre1N7c+i+eebe4wr6WpS0TRwi1uFhUcnl6tnKb2/wCpAeeR3/5PDdc//jmbrQFv8exd/wDq269eP9Lnznk/jYYbGUa1S+pDWvZXecJRX7tGVTSzhpCeLo87KcVLlTvdPrTAp42/DVr7eEnfr1nf9y5o/C1VUwlWpGfF3XpqMn7Pt5pbtjNricXonFT4eqsRQqSznCCupPfez/axR0/puOIjSo0IOlh6PsRvm3ayk91s7Z8rA3/lVicBHFtYrDVqlXUj60ZtLVztlrLluV9EabwGHlN4bBYq8o2nZ6+XSnJ2K9TTeDx1KEcfGpCtBWVakr3+GfZZoxnpvC4TD1KOj41HUqq061RWdtmWzPbbJJXvmFfMJbtgCAQAAAAAAAAAAAAAAAAAAH0eHoSnFW3Il4pPo7TPR7tTfw+hahO5N7hrX8Vqf5e0yWFm93abC55dAUeJz6O0jq4WSWa7GX5TW7tMXP4/EmkaxYOplFJaltreZ7HBzSs2l1SLkpSadtVPkujBSd9redtjGun1Xlg5q7unffyGCwE1m3DW6XbIv1NZbOwwalJbEr9o0QvCytydpHHDTey3VctSVkks31mMZaud/wCxLUs4g4rPZl0esecWqflt8dvUW41byTbVlyW5SbXjZu+Rf0xlUOLVdZO61eVf7knFaj2avaXF/wA5TKMrcolFTic+jtHE59HaXz00NPjaLhTlfljL6Hz59Rpj8N/pl9D5cAAAAAAAAD0HgAAAD022J0xGpo6hg9RqVKes53Vn7eVv9X7GoAHp4AAAAAAAengAAAAAAAAAAAAAAAAAAAAAAAAAAAAfWaN9h/D6FiUejIraNXqdn0LhKI4t78vgZOyzMarttvYpzrtuy+uwzuESSr3eeSMHst9Txt3cXmeNNJttW5LGWmUKas3ty5TJWUbt2/Y8b9R2sPVnGKvf+pWo8ptb7t8vQScItizYnqqOdkeYezVlt5Sj1087t9hg0rXas+nlEk7vPLeZKnbN3fWBjGancxhKzfQezS6jBReslsRLcSrFKals5P8AnIS67KdtWWtFdZahK6TsyXb8Ys/CVyWSu02ZEcKkXsb3cpIkdcFDTH4b/TL6Hy59Rpj8N/pl9D5cC1xqHu1HvVvuDjUPdqPerfcKoAtcah7tR71b7g41D3aj3q33CqALXGoe7Ue9W+4ONQ92o96t9wqgC1xqHu1HvVvuDjUPdqPerfcKoAtcah7tR71b7g41D3aj3q33CqALXGoe7Ue9W+4ONQ92o96t9wqgC1xqHu1HvVvuDjUPdqPerfcKoAtcah7tR71b7g41D3aj3q33CqALXGoe7Ue9W+4ONQ92o96t9wqgC1xqHu1HvVvuDjUPdqPerfcKoAtcah7tR71b7g41D3aj3q33CqALXGoe7Ue9W+4ONQ92o96t9wqgC1xqHu1HvVvuDjUPdqPerfcKoAtcah7tR71b7g41D3aj3q33CqALXGoe7Ue9W+4ONQ92o96t9wqgC1xqHu1HvVvuDjUPdqPerfcKoAtcah7tR71b7g41D3aj3q33CqALXGoe7Ue9W+4ONQ92o96t9wqgC1xqHu1HvVvuDjUPdqPerfcKoAtcah7tR71b7h5xqHu1HvVvuFYAfZaPrxcPwaa2bHU3dMi06seah2z8Rr9G+x2fQtu4EWLrrJcFDPpn4iprRv8AhwXxn4jLEu8slsyuRxaT1XynOtSJeMpZKlF9K1/EZ08VB5OlD46/iIVCyyfKHUSe9hUk5RTWrCGf6/EI4mnGVuCgn1z8RV4S88viSYmKdna4Wf6sKUZK8qUWuRJz8RjLFxhlGhFN7+E+usR8I+rdYj1nL4MKu8PBJLgqbtntnt7xG8Snd8HDdtn4itV33aZ5G+pZPNhFpYiGtH+HDqvPxGM8TBuzpQv1z8RWjK0rXzszConbWW0uS8otqvTyfBQ6rz+msSU8Sl//ACh2z8RSpTdldZvL4EqlsL8qNnCUeWlB/GfiM1Vjf8KHbPxFSjP1Vcm2o0wg0tXiqb/g036strqbuiR8zxmHu1HvVvuG/wBLfhv9MvofLgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAA+r0a/VeW76Fwp6O9js+hcAoYpPWlyEF2n0by3i4WV0s+UoyjeMktb/nSYsaSK97J9vIFRSfrNsxglFK2dz2U7bQrNNXdlY8nK6IKcpOTsm0Z1ZWWcQr1ysk79qChN+tsXTyntOOSbz6NxK5gRJOW3Z0Gc3sslY9lU6iGrLkTzfKuQD1P1uhBzeta6t1nsI6qsjyUf/ZR5HJ25Oszks75PrIpWVnkZztbN2KizhZO9rWut9yzwivt/sUsMryey9iRRd7WzKs8yo9KP+HZ7dSWV+hHzR9JpWK1G/8AJJXz3HzYcwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwAPrNG+x2fQtN9DKujfY7PoW0wI7bVb+xW4tf2WuouNEbVssv2CbijPCTv/AGtb9zGdCaSukl159hss+Uq4q+T5Okliyqs5atlFdRJFb9pGtvwPczLbKbuukwir3u2ZJ3PM1y5BXnqx5EHnsyR7JdNjy3T2Aey2bfoRTbyzy6bGc7La10XI5SV0tmfJsKsZR1c23fMysntWfWeQve3JvJYUG/8A2yzqXiXD0nt3bGXaewiw6ajaxOisbqhpj8N/pl9D5c+o0x+G/wBMvofLhAAAADOk4qcXNNw1lrJOzavmk99gMuLT4PhdSXB62rr2y1rXtciPtdFaNwdbC1nKVelQlPhIwqyUbKCznF53i1KzfR8TUY6rHDqm1hcDOlUT1dWU6krLJ3ndNPPcBoQbPEYWlVoyr4ZShqW4WjJ6zgnkpQltlG+Wea6jX0qUpyjCCcpSaUUuVvYgMAbGGgsZKMpLDVGotp5csXZ2W15rkKuEwdWvPUo05VJbbRXJve4CAFrE6Or0akadWlKE5+ymvau7ZPY82jCWEqKrwLg+F1lHU5dZ7EBAC1HR1duolSm3TkozSV3GTeqlbffLIkehsVwqo8BPhXHW1bZ6ryu+RLJ7QKILOO0fWw7Sr0pU77NZZPqayZZhoDGScksNUbjbWyWV1dcueTWSA1oL1DQ2KqQdSGHqSgrpvV3bbJ5u3QQ4PBVcRLUo05VJWu1FbF0vYgK4L1TQ2Ki2pUKicXGLy5Zu0bb7vLIvaT8m6tChRqxp1Hem517pWptWyy5LX7ANGC/htC4qtT4Snh6koPZJLb1X2/Av0dFcJo1ShRcsS8U4ZJ61lHOLXJYDQgtY3R1fDtKvSlTb2X5eprIqgAAAAAAAAAAAAAAAAAAAADA+s0b7HZ9C0ln/ALlXRvsdn0LbjcDz4mNuoya/5YxXLlYI8SteySI60NaNrksU+Uxkv+WKla+UJR2p9e08bubCC5M31mDowb9nPoyM2LPTX8trk9LDt52S6S3CmlsijN58gxr+TX4uDgk1Z3yK6nKxtK1HXVpLLoKtbBtRvFttbU/6CwlU1Sbd2xOmtq7bE+HpOSvydRap4eK23f8AQmNSqdGi5Oydne7drm1iugwUVst/QlissyxLdexdzIxirLl+JkVGv0x+G/0y+h8ufUaY/Df6ZfQ+XCAAAAADe1cZKjxLFOXGHKjODjUilFKN4OCttXrPbt+JojY4HF03Slh8RrcE5a8JxV5Up2s3bli1tXZmWsDg+BqOcMRo+rFxcf403az5XBrWT/3A90VheBrUtdOrHFYeaUKLUpNSTWq72V1b4NdBU0DFrHYZSVmq0U103zPpJ6RwccDKNDWUqTUJVaEGnT4RuUpQc3rKDcWtvL1HytecaOI1sNVlNQkpQqONnfJ3s+m+0De0sXVenc6ksq8oWu7aqutW26xPBU44PSbfCr/6yaqcDZSUFL1dv8t7/ufMRxlVVuHU3wutr69l7TzvbYZ4bSdelVlWp1ZRqTbcpK3rXd3dbHn0AbahjaUqOFpUoYhwhi6co1auq4xzScE45dNusmr0pPT9lF34eE9n8qUW5dVltNLj9LYjEpKtVlNLNRsopPfaKSuTy8o8a4xi8TO0Wmso3yd1d2u9nLt5QN8q86cdNzhJxkqkbNbVeUll05mvwdSf+EYqcJSdTh48LK71uDsrZ7bXv+5p3pGs1WTqO1dp1cl6zTuuTLPdY8wWPrYeevRqSpyas7WzXSnkwNrTnKWg8S6jbiqsOBcs88tbVvyWvs6S/wCUeJqLTFBKckoyo2SbSV5K+XTc+d0hpOvibcPVlNLYskl1JJIxxGOq1aqrVJuVVatpWS9n2disB9LPFVH5QJa8rKoopXdlHU2WI6zcdH6QdG6lx2aquOTVNPLZ/L/ufPvSFbh+Ma74bW1tey27L2tb9j3C6Tr0as6tKrKM5tubVnrXd3dNWebfJygbnC1Kr0HinJycFVp8G23+eF9V7r/vcx09UfFtF68pakqNpu7zzje+92uavFaZxNaM4Va0pRnq6yajZ6rvG1llZ7hDSWKpUOBVSUaNSL9RpNNNtO11dK6ezluBtfLCrWjj9WMpxiow4BQbStZexblvfZ0ElTEVoaHqa0pxqTxbVS91J+rdp8u1Gqw3lBjKVNU6eIkoJWStF2W5NptFapj60qbpSqSlBz4Rp53m1Zyb2/uDW2xc3LQuHcm244qUU272WrN26jQE7xdR0VRcnwSnrqNllK1r327CAAAAAAAAAAAAAAAAAAAAAAA+gptqKz5EZa73vtMIbF1I9A8ldteszydZr8z6jIinSSTewqVk6r3u/wAROcrb/iyLYr3Stm0ZrO6+gZ7WLq3y9ZfEOrJWvftPVPk5eRcpik930LGfUrJyknfWkIzd7qT7QobWeRhfb+zI1GTrPfL9zDh3fKXa2ZSilyNt9BFqpcvwv/YzuN3EkJ7279DGs1sezbeTeRlFJ5pHsYPl29BrlZ826yp1G0s2n1mXCNbZPtIacFrt/wA1s8nn1Esop2ur22EdKz13vfaNd732nh4GWGJbdOf6X9DRG8xH4c/0v6GjAAAAAAAAAmpYqpCFSnCbjCpbXitkrZq5CAAAAAAAAAAAAG9ngI8Vm5wjHVp0pqpCnK3rSgn/ABHL13qyd0lZNclhVwUniVSWFpwpqo405yjO00lJx9ZP+JdK+W12WSdjRAD6GrgYpwnGheq6E5xpOm4KUo1FG/B6zeUG3q3z1b2M5YaMnT16NqkMIpRoqDnm609Z8HrJuyberfK+zI+cPANziVTo061RYdaynRSjWg1q60Kjl6mtkm4qybyv0IkxUaFOpiL0IcHh8RCFlra0oS4RSUm3m/VTW5miDAv6RwkcPGNK6lUcnNzX5NlO36knL/VEoEuIryqTlOVtaW2ystlkktySS+BEAAAAAAAAAAAAAAAAAAAAAAb+GxdSPSCOKp2XrrYjLjdP86AlPSHjdP8AOhxun+dAZKLWxLt/2MrfAj43T/Ohxun+dBGaprre884PbfNbjHjdP86PHiqf50FZzWXQYwvbJL4s8WJppWU0YU8TTX8yv2BJP7TSgntI+Dey1117P2PXiaT2yiR1qlOVv4iViVrJfqxTWWxrrMyHjVP86HGqf50VEwIeN0/zocap/nQEoIeNU/zolw041ZqEZxu77+RN8mfIBhiPw5/pf0NGfQaQpcHSblKNpKSVr7bb7Z7eQ+fAA6X5o4DmH82p4h5o4DmH82p4gY5oDpfmjgOYfzaniHmjgOYfzaniBjmgOl+aOA5h/NqeIeaOA5h/NqeIGOaA6X5o4DmH82p4h5o4DmH82p4gY5oDpfmjgOYfzaniHmjgOYfzaniBjmgOl+aOA5h/NqeIeaOA5h/NqeIGOaA6X5o4DmH82p4h5o4DmH82p4gY5oDpfmjgOYfzaniHmjgOYfzaniBjmgOl+aOA5h/NqeIeaOA5h/NqeIGOaA6X5o4DmH82p4h5o4DmH82p4gY5oDpfmjgOYfzaniHmjgOYfzaniBjmgOl+aOA5h/NqeIeaOA5h/NqeIGOaA6X5o4DmH82p4h5o4DmH82p4gY5oDpfmjgOYfzaniHmjgOYfzaniBjmgOl+aOA5h/NqeIeaOA5h/NqeIGOaA6X5o4DmH82p4h5o4DmH82p4gY5oDpfmjgOYfzaniHmjgOYfzaniBjmgOl+aOA5h/NqeIeaOA5h/NqeIGOaA6X5o4DmH82p4h5o4DmH82p4gY5oDpfmjgOYfzaniHmjgOYfzaniBjmgOl+aOA5h/NqeIeaOA5h/NqeIGOaA6X5o4DmH82p4h5o4DmH82p4gY5oDpfmjgOYfzaniHmjgOYfzaniBjmgOl+aOA5h/NqeIeaOA5h/NqeIGOaHp0rzRwHMP5tTxDzRwHMP5tTxAxzU8Ol+aOA5h/NqeIeaOA5h/NqeIGOaA6X5o4DmH82p4h5o4DmH82p4gY5oenSvNHAcw/m1PEPNHAcw/m1PEDHNTKlVlCWtG17NZpNNNWaaeTyZ0jzRwHMP5tTxDzRwHMP5tTxAxzmtiJz9pp5uWUUruW1uyzeSzZEdL80cBzD+bU8Q80cBzD+bU8QMbsAEaAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB0sHNPSHjObw/dn4h6Q8ZzeH7s/EB8kAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/9k=\n"}}]}}, "8e226830911a4a0e8e7538efcd799cbb": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "45583a24a42e420aa9d0b611158f30a9": {"model_name": "TabModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TabModel", "_titles": {"0": "Youtube", "1": "Bilibili"}, "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TabView", "box_style": "", "children": ["IPY_MODEL_fc5c1dd160ca4053836a5259accefaa0", "IPY_MODEL_56d281e17e184491b787d6093f6bc075"], "layout": "IPY_MODEL_8e226830911a4a0e8e7538efcd799cbb", "selected_index": 0}}, "4cf74130006048da91b3fb7d096208c6": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "e75d543327fc4405af4bfb3f6e4a2a4c": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_4cf74130006048da91b3fb7d096208c6", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://www.bilibili.com/video/BV1Mo4y1S7oK\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<__main__.BiliVideo at 0x7f963f154490>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://player.bilibili.com/player.html?bvid=BV1Mo4y1S7oK&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "}}]}}, "c039fe09f15e4cd5a47e89d9541d7ac7": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7baf270a55f64b8cbea8ba20e940164b": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_c039fe09f15e4cd5a47e89d9541d7ac7", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://youtube.com/watch?v=Vt9GhJKcEiM\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f95f7bf6490>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://www.youtube.com/embed/Vt9GhJKcEiM?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ", "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDBoYFhwaGRoeHRwfIjQlJCIiITEuLycwLz0yNzMyPS03QVBCOThLRTEtR2FFS1NWW1xbNUFlbWVYbFBZW1cBERISGBYZLxsbLVc/OUNdV11XWF9XV1dXV1dXXVdXV1dXWFdXV1dXV1dXY1dXV1dXV1dXV1dXV1dXV1dXV11XV//AABEIAWgB4AMBIgACEQEDEQH/xAAbAAEAAgMBAQAAAAAAAAAAAAAABAUDBgcCAf/EAEwQAAIBAwAECQgGBwcDBAMAAAABAgMEEQUSITEGExRBUVNxktIWFyIyYYGRk1ShscHR0yNCUmKCsvAVJDNyc8LhJaLiNENj8Qdko//EABkBAQADAQEAAAAAAAAAAAAAAAABAgQDBf/EAC8RAQACAgEDAgIJBQEAAAAAAAABAgMRIQQSMSJBE1EyM2FxgZGhwdEUI0Lh8AX/2gAMAwEAAhEDEQA/AOfgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAz8ll0ocll0oG2AGfksulDksulA2wAz8ll0ocll0oG2AGfksulDksulA2wAz8ll0ocll0oG2AGfksulDksulA2wAz8ll0ocll0oG2AGfksulDksulA2wAz8ll0ocll0oG2AGfksulDksulA2wAz8ll0ocll0oG2AGfksulDksulA2wAz8ll0ocll0oG2AGfksulDksulA2wAz8ll0ocll0oG2AGfksulDksulA2wAz8ll0ocll0oG2AGfksulDksulA2wAz8ll0ocll0oG2AGfksulDksulA2wAz8ll0ocll0oG2AGfksulDksulA2wAz8ll0ocll0oG2AGfksulHyVtJJvK2A2wgAAAAAAAAAAAAAAAsgASqAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAeavqvsPR5q+q+wCvABCwAABZWmjdZNzyvY9hh0dQjOW3a+ZG60dAN0FXqvGVlJf1sREzpHMzqGo1tHrD1eYrqkHF4e82qro7LeH7nuKPSFPa1q4cdgidmpjyrwASkAAFkACVQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADzV9V9h6PNX1X2AV4AIWAABe8FE1WlJRzsSS6W933m26b0zWhRpwxDL2NKLaXMtvTz7tmecq//AMe1NZTgt8aim+xprOO1JezWL7TtCm60MJ4cszit2Hv7M9qz8Ssz7JrXnbWKmkJRcU6aWss7X0kDSlZa0paqTS6d/MXmknCrcSlHMcSaWzcv6RQ8IoRi4pPMnt9y+wiuk3iVJJ5eT4AXVAABZAAlUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA81fVfYejzV9V9gFeACFgHqFNy9VN9hIp2Mm0pNRz0k6RuIetEV5U7qjKEnF8ZHan7Vs7Dot/GbrTlmSW2OI434yn2Y2fE0Wno6NOclNyUox1k3sxse33F9LhDKtSzPMZ6qTcf1mucreJhaluUdSltlKTfTn8TWb24dWrKbbw3s7OYurq4bhqJNynsS7dm4qrixlCUoyTjKLw4tbmKwWtyhgySoyXNnsMZKAAAWQAJVAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPNX1X2Ho+TWU10oCuMlClryUfj2I98ll0oz29LUz0vKEJmU21pbXCnF5klqpbW3+Je21CFnFyrR1rhYcYNJxg3tz7ZLZ7F7SBobSdG2U5unOVfdCSxiMcLPP6z28273mG80iqjcvTcnvbS3/HcLWmeITSIjmX2pRq3VSXO2nJyb9Vc7b6PxIFaMqctWck+hrc0WP9pQjb8XDXUpPNRtLDS9VLbnC2vtZW1pa2faT769ke2/d81pQqRntUl6UcrnW1PsLm6pK6hxsccZjOOnpXatuCNd6SVfPGReWt65n0o+aO0gqUZRkpNZ1o4S2Pn3vsKTvz7rRrxvhnjoymrCNZuXHSbklzKKeFs92feUVxR1stb19ZsFxpWjKChGFRJQa2qOxvO7bu2lN0k1ne9q21GtKwEiVs29jR85LLpQNpYAJQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAZKtGcMa8JRzu1otZ+JP4O6P5TeUqbWY51p/5Y7X8di95sWmrhaRs7mcfXtK0nH2092exrL/AIQNKBL0ZYzua0aVNJt7cN4WFv2my8LdAVZXMJUYU40nGFKCTUcPbsxzIDTwbhwq0FONvbzpwhGFCj+kw0vSerl459udpQ6M0Fc3acqUFqLfOb1Y56M8/uArQWWk9B3FrFSqwWo9inCWtHPRnm95jtNE169KVWlDXjGahhPbrPHN71tAggva/BG9hBz1IyxtcYTzJe78DNwJ0Zx90qsoQnSp+trYe1p6rxzga9CDk0opyb3JLLfuR8lFptNNNbGmsNe4v56OurbSUNRU+OqTnOlmWVt1t/uyVV3SrVLqpCUdavKrJOMFnM8vOPZnIEQF/wCR17jOrT1sZ1OMWt+H1lNyafGqi4uNTXUNWWzDbxh/EDCC7o8E7yc5x1Ix1HhylPEW9m58+8gaT0XWtJqFeGq2sxaeVJexgQwAAAAAAAAAAAAAAAAAAAAAAAAAABMr6LuKdKNaVKXFSWspxxKOHztxzj34IaeQAM9paVK8+LpR1pYbxlLYt7y2kfLy1qUJalaEqcv3ljPY9z9wGEH2PpNRj6UnuS2t9iW8n3mg7qhBTrUdSLeE3OGW37E8/URtMRMzqFeD3UpShFSksRexS5s9Gen2bz3ZWlS4nxdCDqT6I83a9y95JMTE6lhBIvrGrbVOLrwcJ41sNp7Hz5Ta5mRwgAbS3mava1KSg6kJQ11mOssNpc+HtAwgAAAAAAAAAAAAAAA3HglZOFlcXCnCnUqp0qU6ksJY3vPb/KZ+C+iHaV3rXNrOnUhqShGplv8AZwsbede8p+Et/RlRtbW2mp0qMcykk9st3Pz+s/4igi2mnF4knlPoa3MJTtIWDtryVB7oVFq554tpxfwaLLh6v+oS/wBKP+49cKNIULlW1zTkuO1UqtPDysbVt9j1l70ZOFlW0uv71RuM1HGMeJccPftfsxn6gh54af4di+i3+6Ja8INH0nb2ts7ylbU4U86k/wBd7PS3rPP72Vul69pd2VCfKNSvQo6vFOPrPC2fVvPjvLXSFtRpXNbk9xQWrGpKOYzjs3/Bc62hKXouna29tc0KmkKFWnVh6ME8astu1bd+74IjaCuZ0tC3s4NxlxiWU8NayhHY+naR6rsbS2qQhOF5cVFhT4v0aa27VnO3s37NyMVhfUoaJuqEppVZ1IuMduWlqZ+x/AhD3wIqOOkaaTaU1JS/e2N7enajPwYilpqWFunVS/7iv4L3dOhfUqlWShCOtlvmzFpGXQ+kqdDSjrzf6J1KmZJZ2S1sPG/nRI9aBX/WYf69T7JlvoVKF9pSvjM6PGOPvlNv+VECDtLbSlGtTuVUpOc5zlj1MqWFs37zFY6chb6RuKrXGUK05qWOeLk2pJfd0NgUfKKnGcdry43OtxmfSz05Nr4SJSvNG18YnWVNy90otfzP4EH+yNG6+v8A2guI38XqPjMfs9Pvx+Ji0ppqNzf0aqWpRpSgop80YyTba/rYkBk4c3Eql/OnJtwpqKjHmWUm3jp2mfTVRz0NYyk3KWu45e/C11v9y+BW8J7qnXvq1SlJShLVw1z4jFP7GSNIX1KeirSjGadWnUk5R27E9f8AFAUQAAAAAAAAAAAAAAAAAAAAAAAB9PmSTo+wq3U1ChBzfO16sfa5bkgJ1hdV7WjRqUakoKetsT2Nxk16r2bsEz+0bSu/73Zx1nvqUHqSfak0n8TNwmtYW1O0tYy1pUoScn0ubTb97TZQmS1pradPewYKZsNZvHLZ6Fzou1o1ZW/GSrVKbgtdSb282XsS3Z7CqsNOV6EdRNVKfV1FrR93QVoKzktPLtj6TFSs11vfzbPQ4XQpL9HY0oS6YtL7IlLpTSlW7qa9V7tkYrdFewhAib2nyvj6bFjndYSrC/qW8nKm1t2SjJZjJdDXOXvllONPVpW1Km+lPZ3Ul9prAEXtHEJv0+LJO7RyzzuXUrcbXiqzb9JSeM9jW2PsxuLeha6GktaUrin+43J/Wk/tKBzS3tL3ma3tatX/AAqc5+2Mdne3Fq5LVcM+DBfm06bRR0rou022tu5z5pNPPfntXuKXS+m6l3UjOUKS1IyjFNZwprDeelbGn7Cu1Ja2pqvX1tTV59bOMfE2K04KpxzXqS1v2aeEl7MtPP1Fcmft+lLjOHpsceN7a9W0NdQSk6E5Re1TprXi105jnZ2kV0Km7iqmf9OX4G2XFpc6Pg6lrXm6S2yhLDx0vGMNdOxMWPCC5uq1OhUnqwlnLp+hJ4i3jWTyl2YO1eppNe5589LMxNqzw1mlo25n6tvXfZSl9uDDVoVIVHSlTkqi3xW1r3RztN80nbKlQnVpucZ01r6+vKUsR2yw5N7Wsr3mXQ9GNOnONP1VVlja23t6XtZxnrK9ndEOcYZ3qXO3Fp6rTUt2q08/DeZJ21SK1pUqkY9LpyS+LRsdtLOnqvsj/siifwyralhNZ2zlGK+OfuLT1M99a686R8KNTO/DSAAbHEAAAAAAAB9PgAAAAfT4AAAAH0+AAD6fAAAAAAAAAAAAAAAAAAAAAHunTcpKK5/qNn0DwZlcx4xzdGjui0lr1Mc+3Yl/XtBpqoN50hwETg3Qryc1zVcbf4opY+DNKuKE6VSVOpFwnF4cXzA0xgGaztalxU4ujBznv1cpfa1kCZoK4hRrOco05vHqVIJxnjm1n6kuh4xzM2Ctw1qOGrQoQpLmec492EiPo7gNXm1K6nGlT54xlmT9mdy7dpA08qCuqit8cWsJau7OFnHsOOSZjxL0Ohx0yWmLxtCr1pVJynOTlKTy297PABle9ERADNaWlWs8UoSn0tbl2yewsbng9UpW9StUqRzBZ1YrPPzyfs9hEzETqZcL9Rjr7qgy21rUrT1KUHOWM7MbF0tvYjZdB6Ht5UKVaUOMlOOXr7UnzpR3GDRttybSk6S2RnTk4L2Npr4YkvcU+JHqiPMM9ur3HphXLQdVXFKhUlCDqRlJNZljV3rm2l9bcGbeG2alVf7z2d1YXxyYeEFdUbqxrSzqxlOMsLOxpc3xJmiNOUryVRUlNcXjLkks5zu2+w45LZJpFq+Pf82Oc17Tq0pMrKnGlOFOnCCcWsRilvXsIfBepraPoP8Ada+Emiu4T6ZuaFaFKhqJSp62XFN5y1z7OjmJnBBNWFOL3qUl/wBzK2paMPdM+Zj93P8Ay8cIOhLbW0ldye6lOWr2zb2/BS+Jslxc06UdapOMI7syaS+srNDRSuL7p45fy/8AJQcMpureQpN+hTp62PbJ7fuLWp8XL2z4iE17piIjy3VNSWViUWu1NM07RlvxOlI0luhOaXY4Nx+poteCFRu3lDmpzxH2JpPHxbI0l/1xY54az7kl+BGOvZN6fYvuabifubLOClFxe6Sw+xlRwUpOFmoy9ZVJp9qk0/sLfWWUudpvHZjP2oxWlDi1JLnnKXebf3meLeia/crMc7avoyWtpyu/86+CijJw7q+hb0/2qjl3Vj/cReDctbSteXS6r/7keeGtXWvKUP2KeffJv8EelFd9RWPlDPv+3KjPgB6LMlcqh9Go96t+YOVQ+jUe9W/MIoAlcqh9Go96t+YOVQ+jUe9W/MIoAlcqh9Go96t+YOVQ+jUe9W/MIoAlcqh9Go96t+YOVQ+jUe9W/MIoAlcqh9Go96t+YOVQ+jUe9W/MIoAlcqh9Go96t+YOVQ+jUe9W/MIoAlcqh9Go96t+YOVQ+jUe9W/MIoAlcqh9Go96t+YOVQ+jUe9W/MIoAlcqh9Go96t+YOVQ+jUe9W/MIoAlcqh9Go96t+YOVQ+jUe9W/MIoAlcqh9Go96t+YOVQ+jUe9W/MIoAlcqh9Go96t+YOVQ+jUe9W/MIoAlcqh9Go96t+YOVQ+jUe9W/MIoAlcqh9Go96t+YOVQ+jUe9W/MIoAlcqh9Go96t+YOVQ+jUe9W/MIp9Ak8qh9Go96t+YOVQ+jUe9W/MIxN0XoqrdT1aa9FetN7o/i/YBlsqSryUYU403OSpZg57pes/SlLcjpVJxhGMIJRjFJRS5ktiRplhZRo30KcctU5SeXvb1Es/GTNnVUhZZxqlJwr0IrulxtOKdekspdZFbXB429OPuyToVT5c2sK8dWbqJfuVJQ/laz7wOYxvKbwlbUcvmUqzfw4wutFaFr15J/wBnwhHmnKdWnqvmazNt+5e82K3t6tnVcKNSgoS2whUpRU30+lDGdvPhvpKvTGmNJRkoTxTU3iPErOs+hP1s+5HGc1J9O3XHhm0+Yj71ZpmV1Co6NzWlUccfrZi+h4K379i9pfWfBurVevcTcM7Wk9ab7XtS+snKwpWl5bOnDEakZ02223rbJJ5fP6LRjtmrM6jmXr/1NcdYrWP2hU2PB6vVw5/oY/vLMn/Dze/HYZtPaEpW1nKrDXlOEk25PensxhYWNq+Bs9C7p1XJU5xnqPEtV5w+g1PhXpS542pawUFScVl42tNZ3vdtzu6Dlivkvk14iGXJlvf322ynKEKSa1YU1HOzYksZ+BrOnOFdtKjVo0lKo5xcdZLEVnZz7X8C50M+NsKSbzmlqP3ei/sK+44PWVtbVG1FT4uSU6ktucPDWdieehHPFGOLz37mduNu7XCTwSqZsor9mUl8Xrf7iXfWua9vWW+nNxf+Waa+3VKrgZV1qVVfvKXeSX+0uLK716lem/WpVMe6SUk/ra9xGWJrktMf9teYjwqOGa/RUH0VcfGL/Ai8EGlcVYrZrU8/B/8AkTOGi/u1J9FeP2SK7gs8XnbSkvrgdac4Pzdqx/at98PfDBf3qi//AIn9pa8FH/dX7KkvuKvhh/6mh/py+1FlwRf92n/qv7IkZPqIRH1X4/s86Grf3+/h+9GS+GH9xWcK7dxulVfqzppJ+2LeV8GjFSvlQ0rXqv1eNcJ/5Xjb7mk/cblUpQqJKUYzjvWUmvYxe3wrxb2mHPHukxb8VVwXtnTttaSw6stdL2YSXxSz7yrsqvGadqyTyopx7sUn9eS901pSFpQlUk1rboR/al+HSavwLg3da0tspU5yb7XH8Sce5rfJPvwrbdrfrLZLy51b+1h+3Cqv5X/tLNyws9G01jTlfV0tZr9lLP8AHJxL3S1V07WvPnjSk/fh4OF6fQ+3+URPlqOgI1KUYXNOFOrVqxn+jUpqb9JbedYWrvxHfvZAvb+VW7q1K1vFSwo8XNz9HCXPFxft95f8AqceIqy/W10n2JJr63IquFNWMr+oo71CKl2//TXwPSx3ic811+LPaPRtC5VD6NR71b8wcqh9Go96t+YRz4bXAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA+nqlSlN4hGUn+6m/sLK14PXVV4UFH/M/uWWVm9Y8ytFbT4hVn2MW2kk23sSSy32I2204B1HJcdWUY9EFmX17F9ZtejNB29qv0NNKXPN7ZP3v7FsJ2jTUNC8DZ1MTus0481NP0n2v9Vezf2G6W1pClBQpwUIrckthMUD7qhLRq+zSc+nM19UGWsIzfMyFpCPF6Xg+ac03/FTcF9cS+zFARdWUcZW8ySuoU4uc5KMVzv8Arf7CvjdSdaUZNtNvKfM1nH2YPV3/AOy+itH68x+84Uzxek2iPDtkwzjtET7q/hPQq3tBTjS1FRzOOv8A4k1zrV/V2YeHteFsRV8E7+KrVONmv8JOM5y9VRe1Ze5elnHsNpq30YV6dGWx1IycX0uONn1/UaRwm0YqFw0l+jq5nD2P9Ze5vPY0ZceSc8TS/G/CbV7PVVvVneU68NelJSjlrK6VvNC0xXuK1zONaeadOo0oc2zYnheznfSXfAeTjSqUmmllTi9uJZ2PHwRXafp6t9W6Jasl70k/rTOeKkYstqw1YIjLavclcDMU61Wms+nDW98X/wCR84WU8XcJftUv5W/xRH0DU1byi+luL96f34LDhnH9JbS9k1/K19jJn67fzh3tWMeaIjxuE/gnUzauP7FSS+OJf7irp8DJVarqXNdvLeyOW8Z2elL8DDofTdOzpVeMjOWtJNKKXRje37EebnhnXqbKFBRX7Usyf3JfWIpmi8zTiJ92bNWO+azCTwKmtetGO5wjj+FyX3mWN0rfTNbXkowqwjlt4Saimm3/AAte81q2U6dPEZyhJrDcZNc+eY80aGrluTk5b2zpbHWbWmZ88O8dNktMccf6bFwo0zbVqMaVKqpzVSL9FPGzPPuK3Rt9yarxupr4jJYzjfjn9xBhbwjuijIIrSte2PDTi6a0VtF58/Jl0hped5WhOVLi1CDS25zl9OEe6OlbmhTcLeUUnLWeYpvakt77ERwW9PjXC1elrFOyZeIObc51GnOcnJtdLJVPSd3Sjq0K2rHmjJJ47Mp47DABM78wvPTUmkU+TDVhVrVOMuKjqS9r/rC9iMzzh6snFtYym19gBM2mU06fHSs1iPLFTotS1nOUnje3tXPvPFS1cm3KpPb7SQXXB60oTjVrV9Vqk1sl6qWM6zXP0e4i2Wax3OObFhx05qg6OrvVcaMp0qmrqqrTi5J43KcUnn2SSyvaVE6MqdScZtuedsmpLPt9JJ/FG33PC6nH0aNKUkudvVXuW1/YVOkOEDuFqzt6LXM3rNrsaawXxWy75p/Lx81qWnccKYH0+GxmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAS9GaNqXVVU6S287e6K6X+BtlLQlCnsjTU8frTWW/b0L3GPgA4/pFj0pN7exLH2s2DVhHe0YepvaY1E6a8FYrO5jaDCg8YWxdCLXQ9FJTfOnq/Y/vXwIzuILdtPVlfqMprHrYkvhh/YjP08VjJHLvmm1qTwuT7krZX79iMMrxvnPVeetnNLnPDroqXdHh3QFXwwg1KnWistL66bU4r+c8vSS2ST2b0Z9Ly4yi+dx9Ne7evetZe8r9B04Om44TlTljPSnti/hs9zAnRnGrLXjGWs+f9Xbsb7fvM2klijF9FWk//wCkDPTpnzSUP7vJ9Di/hKJTsrWJ1Hleb2tMblG0lonlFalUc9RU090U3l4aab2JrG888JLDj7WWF6dP04dq3r3rK+BF4R3tSUqVra1oxr1amq0pLKWHve+P2mtUdPXdvRdCnKLcZyXGYy9/t2b8vL6TzcWHLatbxPjw7XvWJmEzgxcOnV42pJxoarp68niKl6ySb58Re4w6e0nQr3anSnrLi1FtppZTe7PaY9CxtaaU7uWu1nVpRi2lnnfN7iztNOWVssW9rJPdrPGs/wCLLZoyV9c2rWZn9EYcnw5id+FIq2q1JScWnlNPGPeYKdJOpxjqOcs9Ofr5zZ3wz/8A138z/wATxU4R21XZWtM92X24KxOWP8P1hrt1eO9otaI3CjaBJvlb60ZW024z2cXJPWi/tafvJNvoG6qLPFqC/wDklh/BJte8TOo54ehXqcVq921aCReWVWhJRqx1W9zTypdjI4d63raNwAALABmt7OtVWadKc10qOz4vYFbXrX6UsJ5lUit7S95mvLWtRi5VKU4YW9x2fFbDabvSFvo+EKUYa0nHOI4y1+1JvpKzMxrUb2y5urrT6PLUYyT3NPsZa6K0HK6puaqxglJxxqaz2Y9qxvJFThFaVf8AGtM+3EZP7mY7mVrK3qqzrSpyliTpSk1rY5lrc+OZPbhCYv41MfqyX66bV9PDNU4KVUvRrQl2xcfvZDuNAXsIyUIRlrJKSjNbUmmvWxzopoVpx9Wco9kmiXR0zdQ9WvP+J6382TrGHLXmJifwZbdXa9e23hClFxbUk4yTw096Z5JV7fTryUqii5rZrKOG+3GxkU2V3rlin7AAEgAAAB7pOKnFzTcNZayTw2s7Un04A9cmqcXxupLi9bV18bM78ZMRuuitG2da1rOUq9KhKfGRhVko4UFtlF7cxxLDfs95UX1WNuqbVrYzpVE9XVlOpJpbHmeU09vQBQgs7i1pVaMq9spQ1McbRk9bUT2KUZb3HPTtXYV9KlKc4wgnKUnhJc7YHgFjDQV5KMpK2qNRbT2c62PC3vdzZItpZ1a89SjTlUlvxFfb0e8DACVc6Or0akadWlKE5+qmvWzs2Pc+Y8StKircS4PjdbV1OfL3IDACVHR1d8YlSm3SkozSWXFt4Sx052bDI9DXXGqjxE+NcdbVxzPnfMlse8CCCTfaPrW7Sr0pU87srY+xrYSYaAvJOSVtUbjv2LZlZ6duxrcBWgn0NC3VSDqQt6jispvHRv2Pa/cYLOyq3EtSjTlUljOIrcva9yAjgnVNDXUW1KhUTi4xeznm8R7cvZsJ2k+DdWhQo1Y06ks03OtlLFNrGzZzb+ncBRgn22hbqtT4ynb1JQe6SW/szv8AcT6OiuM0apQouVw7rU2J62FHasc2AKEEq+0dXt2lXpSpuW7PP71sIoFloXSTtqqlzZ/r6jZ699FrXi8xltz0GjGSlWlD1JOPY8GfL08ZIaMXUTTXG22Sv+gx8tllNLDRrdO7mpqTlKWOZsv7alrpS/Ve1e08/Jg+FMS9XBnpmieE3+1JY9TL7dhW1+EdSE5RdJbH+0/wLFJLGFuNf09TSr6y3SW3tWz8DR0+e1r9tmTqsFK07qQmLhO+el8J/wDBlhwjpv1oTXwZrZ9PQebttlPTFGW6ol/m2faR7a4VtcKf/tP0ZNbtRvY/4Xs7O01o90qjhJNfDma50E7dUhTMztozi4TipRksNPc0anwb4QwpRjSry/Q7qdR/qfuS6EuZ9HsN3o6rSlFpp7U08p+8hLXZcHLSzda+1XrwUqkEtkYYWxKK3+85vTXoo3Xhxp+FSHJKElJZ/SST2bN0U+fbtfYl0mmBEgAJQH0+ADPZV+KrU6iytSSezfjn+rKLa+4U16japYpR+Mvi/uKM+HO2Kl57rRtaLTEahLuNKV6kHGpUdRJ6yUtuGuh71zr3l7o3g5xlNVK03BSWYxhjOOltpr3Grk240pVqUKdBvFOCxhfrdGezYsHLLimdRTh3xdRakTG0rSVjxElqVI1acniM4tPb+y8c/wBpD+t7klvfsI0JtZxue/7idozSUaFXjJUuMaXorW1Unzvc9v8AyVtitEcct2L/ANCIpMW8+zYdG6Dp0Ycdd6uUs6sn6MO3pl/S6TDfcLsPFvTyv2p5x7or72U2l9MVLuSytSC3QTys9LezLK4pj6Xu9WXmfl7Qw5M82nheQ4WXOfSjSkudarX3kDS+kFc1VUUNT0FFxznDWdz6CCDTXDjrO6xpwm9pjUy+nwA6qgAAAAAAAAAAAAC9q3kqPI7py5Q50ZwcaiSilHMHDZvXpPfv9+yiLGxu6bpO3uNbinLXhOKzKlPdnHPFrevhtJdjZ8TUc4XGj6sXFx/TTeMPn1GtZP8A5A+6JteJrUtdOrG6t5pQotOTUk1qvOFlY9zXsIegItX1spLDVWKa6GntNknpGzjYyjQ1oypNQlVoQadPjG25Rc3rajcWt/P2Gq15xo3GtbVZTUJKUKjjh52POH7c7wL2ld1Xp3bUlsruGNZ41VlYx0GeCpxs9JZ43/1klU4nCkoJ7N/6uc/WavG8qqtx6m+N1tfXwvWe3ONxkttJV6VWVanVlGpNtyksell5eVue32AW1vfUpUbalShcOELuEo1auq4x2rMU47PbjtM1ejJ6fwovPHxnu/VSi2+z2lLf6WuLlJVqsppPKjhJL24iksmeXCO9cYxdzPEWmtkc7Nqy8Ze7n384F8q8qcdMzhJxkqiw1vWZSWz27Svs6s/7IuZwlJ1OPjxssvW1MLG3fjOfrKd6RrNVk6jxXeauxek089Gzb0YPllf1reevRqSpyaw2ude1PYwLalOUtB3DqNuKrR4ly27dmtq55sZ3e0ncJLqotMUUpySi6WEm8LLWdntya7pDSde6xx9VzS3LCSXuSSPNxf1atVVqk3KosYlhfq7tywBstS6qeUCWvLCqKKWXjV1d2PrMdZyjo+/dHKly2SqOO9QT2bv1f+TXnpCtx/KNd8dra2vhb927GPqPdrpSvRqyq0qsozm25NY9LLztTWHtb5ucC4talV6DuXJycVVhxbbf7UM4fRn68nnT1R8m0ZrylqSo4m8vb6uc9L3lZdaZua0Zwq1pSjPGsmlj0dq3LZj2CnpO6pUOJVSUaNSL9FpNNNtPGVlLKe72gWnDCrWjf6sZTjFRjxCg2tmF6uOfOd3sMlS4rQ0PUcpTjUnduNTOVJ7MtPn3pFVbcILylTVOncSUEsJYi8djabRGqX9aVN0pVJSg58Y09uZvZrZ3/WBbXc3LQtBybbV04pt5wtWbx2FAZ3d1HRVFzfFKWuo4WyWMZzvMAAAACy0TpB05KnJ/o5PG39Vv7itBS9IvXUr47zS3dDcpt4eN/Ma3pOlJNSeWt3Yy30bdcbSTfrR2S/H3nu6oqaeVse/8Ty8dpw5OXtXpGfFx7tXBluKDpzcX7n0oxHrRMTG4eHas1nUh9PgJQ9wqOLyn29D9x941YaUcJ70ptRfbExgB/WzmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAfT4ABmpXdSEJ04TcYVMa8V+tjcYQAAAAAAAAAAAAvZaPjyWTnCMdWnTmqsKcselKCf6Ry9N4k8pLCfRgVbGTuVSVrThTVVxpzlGeJpKTXpJ/pMpJ7N7wtieCiAGw1bGCcZqhmq7eU40nTcFKUZpZ4vWb2RbernbjOD3O2jJ09ejipC0Uo0VBy2urPWfF6ybwm3q52Z3bDXD4BdXKp0YVqit1rKdJKNaDWrrQqOXoa2xPVWxvZ7kZLqNCnOvmhDi7e5jDCzrShLjFJNt7X6Ka6GUIYE/SNpG3jGllSqOTm5r9jdT7yTl74kEyXFeVScpyxrSe3CwuhJLoSSXuMQAAAAABM0Zc8XVWfVlsf3Mv5zSWXuNUJUr+o0k2tiwn/XOZM/Tze0TDd03VRjrNbMuk66liONqeez2EA+nw0Y6RSuoZcuSclptIAC7mAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABltqEqtSNOONaTwsvCXtfsQGIGevaypxhKW6aytkt2x72sPetzftwYAAOl+SNh1D+bU8Q8kbDqH82p4gac0B0vyRsOofzaniHkjYdQ/m1PEDTmgOl+SNh1D+bU8Q8kbDqH82p4gac0B0vyRsOofzaniHkjYdQ/m1PEDTmgOl+SNh1D+bU8Q8kbDqH82p4gac0B0vyRsOofzaniHkjYdQ/m1PEDTmgOl+SNh1D+bU8Q8kbDqH82p4gac0B0vyRsOofzaniHkjYdQ/m1PEDTmgOl+SNh1D+bU8Q8kbDqH82p4gac0B0vyRsOofzaniHkjYdQ/m1PEDTmgOl+SNh1D+bU8Q8kbDqH82p4gac0B0vyRsOofzaniHkjYdQ/m1PEDTmgOl+SNh1D+bU8Q8kbDqH82p4gac0B0vyRsOofzaniHkjYdQ/m1PEDTmgOl+SNh1D+bU8Q8kbDqH82p4gac0B0vyRsOofzaniHkjYdQ/m1PEDTmgOl+SNh1D+bU8Q8kbDqH82p4gac0B0vyRsOofzaniHkjYdQ/m1PEDTmgOl+SNh1D+bU8Q8kbDqH82p4gac0B0vyRsOofzaniHkjYdQ/m1PEDTmgOl+SNh1D+bU8Q8kbDqH82p4gac0B0vyRsOofzaniHkjYdQ/m1PEDTmgOl+SNh1D+bU8Q8kbDqH82p4gac0B0vyRsOofzaniHkjYdQ/m1PEDTmgOl+SNh1D+bU8Q8kbDqH82p4gac0B0vyRsOofzaniHkjYdQ/m1PEDTmgOl+SNh1D+bU8Q8kbDqH82p4gac0B0vyRsOofzaniHkjYdQ/m1PEDTmhloV50pKdOTjJbmn/Wz2HRvJGw6h/NqeIeSNh1D+bU8QNOdVbmc0lKWUtu5Zb2La972JLaYTpfkjYdQ/m1PEPJGw6h/NqeIGl2ACFgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABzTzh3nV2/dn4h5w7zq7fuz8QHSwc084d51dv3Z+IecO86u37s/EB0sHNPOHedXb92fiHnDvOrt+7PxAdLBzTzh3nV2/dn4h5w7zq7fuz8QHSwc084d51dv3Z+IecO86u37s/EB0sHNPOHedXb92fiHnDvOrt+7PxAdLBzTzh3nV2/dn4h5w7zq7fuz8QHSwc084d51dv3Z+IecO86u37s/EB0sHNPOHedXb92fiHnDvOrt+7PxAdLBzTzh3nV2/dn4h5w7zq7fuz8QHSwc084d51dv3Z+IecO86u37s/EB0sHNPOHedXb92fiHnDvOrt+7PxAdLBzTzh3nV2/dn4h5w7zq7fuz8QHSwc084d51dv3Z+IecO86u37s/EB0sHNPOHedXb92fiHnDvOrt+7PxAdLBzTzh3nV2/dn4h5w7zq7fuz8QHSwc084d51dv3Z+IecO86u37s/EB0sHNPOHedXb92fiHnDvOrt+7PxAdLBzTzh3nV2/dn4h5w7zq7fuz8QHSwc084d51dv3Z+IecO86u37s/EB0sHNPOHedXb92fiHnDvOrt+7PxAdLBzTzh3nV2/dn4h5w7zq7fuz8QHSwc084d51dv3Z+IecO86u37s/EB0sHNPOHedXb92fiHnDvOrt+7PxAdLBzTzh3nV2/dn4h5w7zq7fuz8QHSwc084d51dv3Z+IecO86u37s/EB0sHNPOHedXb92fiHnDvOrt+7PxAdLBzTzh3nV2/dn4h5w7zq7fuz8QHSwc084d51dv3Z+IecO86u37s/EB0sHNPOHedXb92fiHnDvOrt+7PxAakAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/2Q==\n"}}]}}, "9e4ae024fc7649969e8015fcf32ac0d6": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "6772c553cba94003ac5a545acbe8a98e": {"model_name": "TabModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TabModel", "_titles": {"0": "Youtube", "1": "Bilibili"}, "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TabView", "box_style": "", "children": ["IPY_MODEL_7baf270a55f64b8cbea8ba20e940164b", "IPY_MODEL_e75d543327fc4405af4bfb3f6e4a2a4c"], "layout": "IPY_MODEL_9e4ae024fc7649969e8015fcf32ac0d6", "selected_index": 0}}}, "version_major": 2, "version_minor": 0}
</script>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tutorials/W2D3_ModernRecurrentNeuralNetworks/student"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</div>
</main>
<footer class="footer-article noprint">
<!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev" href="W2D3_Tutorial1.html" id="prev-link" title="previous page">
<i class="fas fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Tutorial 1: Modeling sequencies and encoding text</p>
</div>
</a>
<a class="right-next" href="../../W2D4_AttentionAndTransformers/chapter_title.html" id="next-link" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Attention And Transformers</p>
</div>
<i class="fas fa-angle-right"></i>
</a>
</div>
</footer>
</div>
</div>
<div class="footer-content row">
<footer class="col footer"><p>
  
    By Neuromatch<br/>
  
      © Copyright 2021.<br/>
</p>
</footer>
</div>
</div>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
</body>
</html>