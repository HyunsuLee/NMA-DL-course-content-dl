{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W1D1_BasicsAndPytorch/W1D1_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D1_BasicsAndPytorch/W1D1_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 1: PyTorch\n",
    "**Week 1, Day 1: Basics and PyTorch**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "\n",
    "__Content creators:__ Shubh Pachchigar, Vladimir Haltakov, Matthew Sargent, Konrad Kording\n",
    "\n",
    "__Content reviewers:__ Deepak Raya, Siwei Bai, Kelson Shilling-Scrivo\n",
    "\n",
    "__Content editors:__ Anoop Kulkarni, Spiros Chavlis\n",
    "\n",
    "__Production editors:__ Arush Tagade, Spiros Chavlis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Tutorial Objectives\n",
    "\n",
    "Then have a few specific objectives for this tutorial:\n",
    "* Learn about PyTorch and tensors\n",
    "* Tensor Manipulations\n",
    "* Data Loading\n",
    "* GPUs and Cuda Tensors\n",
    "* Train NaiveNet\n",
    "* Get to know your pod\n",
    "* Start thinking about the course as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"854\"\n",
       "            height=\"480\"\n",
       "            src=\"https://mfr.ca-1.osf.io/render?url=https://osf.io/wcjrv/?direct%26mode=render%26action=download%26mode=render\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ffa5a1e67c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/wcjrv/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "These are the slides for all videos in this tutorial. If you want to locally dowload the slides, click [here](https://osf.io/wcjrv/download).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Throughout your Neuromatch tutorials, most (probably all!) notebooks contain setup cells. These cells will import the required Python packages (e.g., PyTorch, NumPy); set global or environment variables, and load in helper functions for things like plotting. In some tutorials, you will notice that we install some dependencies even if they are preinstalled on Google Colab or Kaggle. This happens because we have added automation to our repository through [GitHub Actions](https://docs.github.com/en/actions/learn-github-actions/introduction-to-github-actions).\n",
    "\n",
    "Be sure to run all of the cells in the setup section. Feel free to expand them and have a look at what you are loading in, but you should be able to fulfill the learning objectives of every tutorial without having to look at these cells.\n",
    "\n",
    "If you start building your own projects built on this code base we highly recommend looking at them in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "!pip install pandas --quiet\n",
    "!pip install git+https://github.com/NeuromatchAcademy/evaltools --quiet\n",
    "\n",
    "from evaltools.airtable import AirtableForm\n",
    "atform = AirtableForm('appn7VdPRseSoMXEG','W1D1_T1','https://portal.neuromatchacademy.org/api/redirect/to/f2da6480-069c-49b9-a37b-8750f9d6f359')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "import ipywidgets as widgets\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Helper Functions\n",
    "\n",
    "def checkExercise1(A, B, C, D):\n",
    "  \"\"\"\n",
    "  Helper function for checking Exercise 1.\n",
    "\n",
    "  Args:\n",
    "    A: torch.Tensor\n",
    "      Torch Tensor of shape (20, 21) consisting of ones.\n",
    "    B: torch.Tensor\n",
    "      Torch Tensor of size([3,4])\n",
    "    C: torch.Tensor\n",
    "      Torch Tensor of size([20,21])\n",
    "    D: torch.Tensor\n",
    "      Torch Tensor of size([19])\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  assert torch.equal(A.to(int),torch.ones(20, 21).to(int)), \"Got: {A} \\n Expected: {torch.ones(20, 21)} (shape: {torch.ones(20, 21).shape})\"\n",
    "  assert np.array_equal(B.numpy(),np.vander([1, 2, 3], 4)), \"Got: {B} \\n Expected: {np.vander([1, 2, 3], 4)} (shape: {np.vander([1, 2, 3], 4).shape})\"\n",
    "  assert C.shape == (20, 21), \"Got: {C} \\n Expected (shape: {(20, 21)})\"\n",
    "  assert torch.equal(D, torch.arange(4, 41, step=2)), \"Got {D} \\n Expected: {torch.arange(4, 41, step=2)} (shape: {torch.arange(4, 41, step=2).shape})\"\n",
    "  print(\"All correct\")\n",
    "\n",
    "def timeFun(f, dim, iterations, device='cpu'):\n",
    "  \"\"\"\n",
    "  Helper function to calculate amount of time taken per instance on CPU/GPU\n",
    "\n",
    "  Args:\n",
    "    f: BufferedReader IO instance\n",
    "      Function name for which to calculate computational time complexity\n",
    "    dim: Integer\n",
    "      Number of dimensions in instance in question\n",
    "    iterations: Integer\n",
    "      Number of iterations for instance in question\n",
    "    device: String\n",
    "      Device on which respective computation is to be run\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  iterations = iterations\n",
    "  t_total = 0\n",
    "  for _ in range(iterations):\n",
    "    start = time.time()\n",
    "    f(dim, device)\n",
    "    end = time.time()\n",
    "    t_total += end - start\n",
    "\n",
    "  if device == 'cpu':\n",
    "    print(f\"time taken for {iterations} iterations of {f.__name__}({dim}, {device}): {t_total:.5f}\")\n",
    "  else:\n",
    "    print(f\"time taken for {iterations} iterations of {f.__name__}({dim}, {device}): {t_total:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Important note: Colab users**\n",
    "\n",
    "*Scratch Code Cells*\n",
    "\n",
    "If you want to quickly try out something or take a look at the data, you can use scratch code cells. They allow you to run Python code, but will not mess up the structure of your notebook.\n",
    "\n",
    "To open a new scratch cell go to *Insert* â†’ *Scratch code cell*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 1: Welcome to Neuromatch Deep learning course\n",
    "\n",
    "*Time estimate: ~25mins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18191171d8b040be91c316f047f28c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Video 1: Welcome and History\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1Av411n7oL\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"ca21SNqt78I\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add timing\n",
    "atform.add_event('Video 1: Welcome and History')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This will be an intensive 3 week adventure. We will all learn Deep Learning (DL) in a group. Groups need standards. Read our \n",
    "[Code of Conduct](https://docs.google.com/document/d/1eHKIkaNbAlbx_92tLQelXnicKXEcvFzlyzzeWjEtifM/edit?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219645f9e29e454f94f080dfb41950a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Video 2: Why DL is cool\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1gf4y1j7UZ\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"l-K6495BN-4\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add timing to airtable\n",
    "atform.add_event('Video 2: Why DL is cool')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Discuss with your pod: What do you hope to get out of this course? [in about 100 words]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 2: The Basics of PyTorch\n",
    "\n",
    "*Time estimate: ~2 hours 05 mins*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "PyTorch is a Python-based scientific computing package targeted at two sets of\n",
    "audiences:\n",
    "\n",
    "-  A replacement for NumPy optimized for the power of GPUs\n",
    "-  A deep learning platform that provides significant flexibility\n",
    "   and speed\n",
    "\n",
    "At its core, PyTorch provides a few key features:\n",
    "\n",
    "- A multidimensional [Tensor](https://pytorch.org/docs/stable/tensors.html) object, similar to [NumPy Array](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) but with GPU acceleration.\n",
    "- An optimized **autograd** engine for automatically computing derivatives.\n",
    "- A clean, modular API for building and deploying **deep learning models**.\n",
    "\n",
    "You can find more information about PyTorch in the Appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.1: Creating Tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce1b8be093945a785ee9bf00d306352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Video 3: Making Tensors\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1Rw411d7Uy\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"jGKd_4tPGrw\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add timing to airtable\n",
    "atform.add_event('Video 3: Making Tensors')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "There are various ways of creating tensors, and when doing any real deep learning project, we will usually have to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Construct tensors directly:**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor a: tensor([0, 1, 2])\n",
      "Tensor b: tensor([[1.0000, 1.1000],\n",
      "        [1.2000, 1.3000]])\n",
      "Tensor c: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# We can construct a tensor directly from some common python iterables,\n",
    "# such as list and tuple nested iterables can also be handled as long as the\n",
    "# dimensions are compatible\n",
    "\n",
    "# tensor from a list\n",
    "a = torch.tensor([0, 1, 2])\n",
    "\n",
    "#tensor from a tuple of tuples\n",
    "b = ((1.0, 1.1), (1.2, 1.3))\n",
    "b = torch.tensor(b)\n",
    "\n",
    "# tensor from a numpy array\n",
    "c = np.ones([2, 3])\n",
    "c = torch.tensor(c)\n",
    "\n",
    "print(f\"Tensor a: {a}\")\n",
    "print(f\"Tensor b: {b}\")\n",
    "print(f\"Tensor c: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Some common tensor constructors:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor x: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Tensor y: tensor([0., 0.])\n",
      "Tensor z: tensor([[[4.2186e+17, 3.0848e-41, 1.2000e+00, 1.3000e+00, 8.9683e-44]]])\n"
     ]
    }
   ],
   "source": [
    "# The numerical arguments we pass to these constructors\n",
    "# determine the shape of the output tensor\n",
    "\n",
    "x = torch.ones(5, 3)\n",
    "y = torch.zeros(2)\n",
    "z = torch.empty(1, 1, 5)\n",
    "print(f\"Tensor x: {x}\")\n",
    "print(f\"Tensor y: {y}\")\n",
    "print(f\"Tensor z: {z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Notice that `.empty()` does not return zeros, but seemingly random numbers. Unlike `.zeros()`, which initialises the elements of the tensor with zeros, `.empty()` just allocates the memory. It is hence a bit faster if you are looking to just create a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Creating random tensors and tensors like other tensors:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor a: tensor([[0.3151, 0.7282, 0.2175]])\n",
      "Tensor b: tensor([[-2.3871, -0.4662, -1.1377, -0.3005],\n",
      "        [-0.3038,  0.4855,  0.0557, -0.9620],\n",
      "        [ 0.4499,  1.1194,  0.3979, -0.7005]])\n",
      "Tensor c: tensor([[0., 0., 0.]])\n",
      "Tensor d: tensor([[0.1871, 0.1508, 0.1521]])\n"
     ]
    }
   ],
   "source": [
    "# There are also constructors for random numbers\n",
    "\n",
    "# Uniform distribution\n",
    "a = torch.rand(1, 3)\n",
    "\n",
    "# Normal distribution\n",
    "b = torch.randn(3, 4)\n",
    "\n",
    "# There are also constructors that allow us to construct\n",
    "# a tensor according to the above constructors, but with\n",
    "# dimensions equal to another tensor.\n",
    "\n",
    "c = torch.zeros_like(a)\n",
    "d = torch.rand_like(c)\n",
    "\n",
    "print(f\"Tensor a: {a}\")\n",
    "print(f\"Tensor b: {b}\")\n",
    "print(f\"Tensor c: {c}\")\n",
    "print(f\"Tensor d: {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "*Reproducibility*: \n",
    "\n",
    "- PyTorch Random Number Generator (RNG): You can use `torch.manual_seed()` to seed the RNG for all devices (both CPU and GPU):\n",
    "\n",
    "```python\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "```\n",
    "- For custom operators, you might need to set python seed as well:\n",
    "\n",
    "```python\n",
    "import random\n",
    "random.seed(0)\n",
    "```\n",
    "\n",
    "- Random number generators in other libraries (e.g., NumPy):\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Here, we define for you a function called `set_seed` that does the job for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, let's use the `set_seed` function in the previous example. Execute the cell multiple times to verify that the numbers printed are always the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def simplefun(seed=True, my_seed=None):\n",
    "  \"\"\"\n",
    "  Helper function to verify effectiveness of set_seed attribute\n",
    "\n",
    "  Args:\n",
    "    seed: Boolean\n",
    "      Specifies if seed value is provided or not\n",
    "    my_seed: Integer\n",
    "      Initializes seed to specified value\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  if seed:\n",
    "    set_seed(seed=my_seed)\n",
    "\n",
    "  # uniform distribution\n",
    "  a = torch.rand(1, 3)\n",
    "  # normal distribution\n",
    "  b = torch.randn(3, 4)\n",
    "\n",
    "  print(\"Tensor a: \", a)\n",
    "  print(\"Tensor b: \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 0 has been set.\n",
      "Tensor a:  tensor([[0.4963, 0.7682, 0.0885]])\n",
      "Tensor b:  tensor([[ 0.3643,  0.1344,  0.1642,  0.3058],\n",
      "        [ 0.2100,  0.9056,  0.6035,  0.8110],\n",
      "        [-0.0451,  0.8797,  1.0482, -0.0445]])\n"
     ]
    }
   ],
   "source": [
    "simplefun(seed=True, my_seed=0)  # Turn `seed` to `False` or change `my_seed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Numpy-like number ranges:**\n",
    "---\n",
    "The ```.arange()``` and ```.linspace()``` behave how you would expect them to if you are familar with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor a: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\n",
      "Numpy array b: [0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "Tensor c: tensor([0.0000, 0.5000, 1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000,\n",
      "        4.5000, 5.0000])\n",
      "\n",
      "Numpy array d: [0.  0.5 1.  1.5 2.  2.5 3.  3.5 4.  4.5 5. ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0, 10, step=1)\n",
    "b = np.arange(0, 10, step=1)\n",
    "\n",
    "c = torch.linspace(0, 5, steps=11)\n",
    "d = np.linspace(0, 5, num=11)\n",
    "\n",
    "print(f\"Tensor a: {a}\\n\")\n",
    "print(f\"Numpy array b: {b}\\n\")\n",
    "print(f\"Tensor c: {c}\\n\")\n",
    "print(f\"Numpy array d: {d}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.1: Creating Tensors\n",
    "\n",
    "Below you will find some incomplete code. Fill in the missing code to construct the specified tensors.\n",
    "\n",
    "We want the tensors: \n",
    "\n",
    "$A:$ 20 by 21 tensor consisting of ones\n",
    "\n",
    "$B:$ a tensor with elements equal to the elements of numpy array $Z$\n",
    "\n",
    "$C:$ a tensor with the same number of elements as $A$ but with values $\n",
    "\\sim \\mathcal{U}(0,1)^\\dagger$\n",
    "\n",
    "$D:$ a 1D tensor containing the even numbers between 4 and 40 inclusive.\n",
    "\n",
    "<br>\n",
    "\n",
    "$^\\dagger$: $\\mathcal{U(\\alpha, \\beta)}$ denotes the [uniform distribution](https://en.wikipedia.org/wiki/Continuous_uniform_distribution) from $\\alpha$ to $\\beta$, with $\\alpha, \\beta \\in \\mathbb{R}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All correct\n"
     ]
    }
   ],
   "source": [
    "def tensor_creation(Z):\n",
    "  \"\"\"\n",
    "  A function that creates various tensors.\n",
    "\n",
    "  Args:\n",
    "    Z: numpy.ndarray\n",
    "      An array of shape (3,4)\n",
    "\n",
    "  Returns:\n",
    "    A : Tensor\n",
    "      20 by 21 tensor consisting of ones\n",
    "    B : Tensor\n",
    "      A tensor with elements equal to the elements of numpy array Z\n",
    "    C : Tensor\n",
    "      A tensor with the same number of elements as A but with values âˆ¼U(0,1)\n",
    "    D : Tensor\n",
    "      A 1D tensor containing the even numbers between 4 and 40 inclusive.\n",
    "  \"\"\"\n",
    "  #################################################\n",
    "  ## for students: fill in the missing code\n",
    "  ## from the first expression\n",
    "  #################################################\n",
    "  A = torch.ones(20,21)\n",
    "  B = torch.tensor(Z)\n",
    "  C = torch.rand_like(A)\n",
    "  D = torch.arange(4,41,2)\n",
    "\n",
    "  return A, B, C, D\n",
    "\n",
    "\n",
    "# add timing to airtable\n",
    "atform.add_event('Coding Exercise 2.1: Creating Tensors')\n",
    "\n",
    "\n",
    "\n",
    "# numpy array to copy later\n",
    "Z = np.vander([1, 2, 3], 4)\n",
    "\n",
    "# Uncomment below to check your function!\n",
    "A, B, C, D = tensor_creation(Z)\n",
    "checkExercise1(A, B, C, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "All correct!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.2: Operations in PyTorch\n",
    "\n",
    "**Tensor-Tensor operations**\n",
    "\n",
    "We can perform operations on tensors using methods under `torch.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a049ef022e42b08b173a9c27bbc38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Video 4: Tensor Operators\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1G44y127As\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"R1R8VoYXBVA\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add timing to airtable\n",
    "atform.add_event('Video 4: Tensor Operators')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Tensor-Tensor operations**\n",
    "\n",
    "We can perform operations on tensors using methods under `torch.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6464, 1.5228, 1.0491],\n",
      "        [1.9147, 1.7692, 1.9970],\n",
      "        [1.7526, 1.1700, 1.9173],\n",
      "        [1.5269, 1.7371, 1.0991],\n",
      "        [1.3562, 1.0091, 1.3053]])\n",
      "tensor([[0.6464, 0.5228, 0.0491],\n",
      "        [0.9147, 0.7692, 0.9970],\n",
      "        [0.7526, 0.1700, 0.9173],\n",
      "        [0.5269, 0.7371, 0.0991],\n",
      "        [0.3562, 0.0091, 0.3053]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5, 3)\n",
    "b = torch.rand(5, 3)\n",
    "c = torch.empty(5, 3)\n",
    "d = torch.empty(5, 3)\n",
    "\n",
    "# this only works if c and d already exist\n",
    "torch.add(a, b, out=c)\n",
    "\n",
    "# Pointwise Multiplication of a and b\n",
    "torch.multiply(a, b, out=d)\n",
    "\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "However, in PyTorch, most common Python operators are overridden.\n",
    "The common standard arithmetic operators ($+$, $-$, $*$, $/$, and $**$) have all been lifted to elementwise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2,  4,  7, 12]),\n",
       " tensor([0, 0, 1, 4]),\n",
       " tensor([ 1,  4, 12, 32]),\n",
       " tensor([1.0000, 1.0000, 1.3333, 2.0000]),\n",
       " tensor([   1,    4,   64, 4096]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 4, 8])\n",
    "y = torch.tensor([1, 2, 3, 4])\n",
    "x + y, x - y, x * y, x / y, x**y  # The `**` is the exponentiation operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Tensor Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Tensors also have a number of common arithmetic operations built in. A full list of **all** methods can be found  in the appendix (there are a lot!) \n",
    "\n",
    "All of these operations should have similar syntax to their numpy equivalents (feel free to skip if you already know this!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6079, 0.1074, 0.6594],\n",
      "        [0.7684, 0.5697, 0.1655],\n",
      "        [0.1123, 0.3457, 0.7195]])\n",
      "\n",
      "\n",
      "Sum of every element of x: 4.055744171142578\n",
      "Sum of the columns of x: tensor([1.4886, 1.0228, 1.5443])\n",
      "Sum of the rows of x: tensor([1.3747, 1.5035, 1.1776])\n",
      "\n",
      "\n",
      "Mean value of all elements of x 0.4506382346153259\n",
      "Mean values of the columns of x tensor([0.4962, 0.3409, 0.5148])\n",
      "Mean values of the rows of x tensor([0.4582, 0.5012, 0.3925])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, 3)\n",
    "print(x)\n",
    "print(\"\\n\")\n",
    "# sum() - note the axis is the axis you move across when summing\n",
    "print(f\"Sum of every element of x: {x.sum()}\")\n",
    "print(f\"Sum of the columns of x: {x.sum(axis=0)}\")\n",
    "print(f\"Sum of the rows of x: {x.sum(axis=1)}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Mean value of all elements of x {x.mean()}\")\n",
    "print(f\"Mean values of the columns of x {x.mean(axis=0)}\")\n",
    "print(f\"Mean values of the rows of x {x.mean(axis=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Matrix Operations**\n",
    "\n",
    "The `@` symbol is overridden to represent matrix multiplication. You can also use `torch.matmul()` to multiply tensors. For dot multiplication, you can use `torch.dot()`, or manipulate the axes of your tensors and do matrix multiplication (we will cover that in the next section). \n",
    "\n",
    "Transposes of 2D tensors are obtained using `torch.t()` or `Tensor.T`. Note the lack of brackets for `Tensor.T` - it is an attribute, not a method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.2 : Simple tensor operations\n",
    "\n",
    "Below are two expressions involving operations on matrices. \n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{A} = \n",
    "\\begin{bmatrix}2 &4 \\\\5 & 7 \n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix} 1 &1 \\\\2 & 3\n",
    "\\end{bmatrix} \n",
    "+ \n",
    "\\begin{bmatrix}10 & 10  \\\\ 12 & 1 \n",
    "\\end{bmatrix} \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "and\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "b = \n",
    "\\begin{bmatrix} 3 \\\\ 5 \\\\ 7\n",
    "\\end{bmatrix} \\cdot \n",
    "\\begin{bmatrix} 2 \\\\ 4 \\\\ 8\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "The code block below that computes these expressions using PyTorch is incomplete - fill in the missing lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20, 24],\n",
      "        [31, 27]])\n"
     ]
    }
   ],
   "source": [
    "def simple_operations(a1: torch.Tensor, a2: torch.Tensor, a3: torch.Tensor):\n",
    "  \"\"\"\n",
    "  Helper function to demonstrate simple operations\n",
    "  i.e., Multiplication of tensor a1 with tensor a2 and then add it with tensor a3\n",
    "\n",
    "  Args:\n",
    "    a1: Torch tensor\n",
    "      Tensor of size ([2,2])\n",
    "    a2: Torch tensor\n",
    "      Tensor of size ([2,2])\n",
    "    a3: Torch tensor\n",
    "      Tensor of size ([2,2])\n",
    "\n",
    "  Returns:\n",
    "    answer: Torch tensor\n",
    "      Tensor of size ([2,2]) resulting from a1 multiplied with a2, added with a3\n",
    "  \"\"\"\n",
    "  ################################################\n",
    "  ## for students:  complete the first computation using the argument matricies\n",
    "  ################################################\n",
    "  #\n",
    "  answer = a1 @ a2 + a3\n",
    "  return answer\n",
    "\n",
    "# add timing to airtable\n",
    "atform.add_event('Coding Exercise 2.2 : Simple tensor operations-simple_operations')\n",
    "\n",
    "# Computing expression 1:\n",
    "\n",
    "# init our tensors\n",
    "a1 = torch.tensor([[2, 4], [5, 7]])\n",
    "a2 = torch.tensor([[1, 1], [2, 3]])\n",
    "a3 = torch.tensor([[10, 10], [12, 1]])\n",
    "## uncomment to test your function\n",
    "A = simple_operations(a1, a2, a3)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "tensor([[20, 24],\n",
    "        [31, 27]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(82)\n"
     ]
    }
   ],
   "source": [
    "def dot_product(b1: torch.Tensor, b2: torch.Tensor):\n",
    "  ###############################################\n",
    "  ## for students:  complete the first computation using the argument matricies\n",
    "  ###############################################\n",
    "  \"\"\"\n",
    "  Helper function to demonstrate dot product operation\n",
    "  Dot product is an algebraic operation that takes two equal-length sequences\n",
    "  (usually coordinate vectors), and returns a single number.\n",
    "  Geometrically, it is the product of the Euclidean magnitudes of the\n",
    "  two vectors and the cosine of the angle between them.\n",
    "\n",
    "  Args:\n",
    "    b1: Torch tensor\n",
    "      Tensor of size ([3])\n",
    "    b2: Torch tensor\n",
    "      Tensor of size ([3])\n",
    "\n",
    "  Returns:\n",
    "    product: Tensor\n",
    "      Tensor of size ([1]) resulting from b1 scalar multiplied with b2\n",
    "  \"\"\"\n",
    "  # Use torch.dot() to compute the dot product of two tensors\n",
    "  product = torch.dot(b1, b2)\n",
    "  return product\n",
    "\n",
    "# add timing to airtable\n",
    "atform.add_event('Coding Exercise 2.2 : Simple tensor operations-dot_product')\n",
    "\n",
    "\n",
    "# Computing expression 2:\n",
    "b1 = torch.tensor([3, 5, 7])\n",
    "b2 = torch.tensor([2, 4, 8])\n",
    "## Uncomment to test your function\n",
    "b = dot_product(b1, b2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "tensor(82)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.3 Manipulating Tensors in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9ff01628e84f1cb1e3450118cb3d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Video 5: Tensor Indexing\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1BM4y1K7pD\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"0d0KSJ3lJbg\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add timing to airtable\n",
    "atform.add_event('Video 5: Tensor Indexing')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Indexing**\n",
    "\n",
    "Just as in numpy, elements in a tensor can be accessed by index. As in any numpy array, the first element has index 0 and ranges are specified to include the first to last_element-1. We can access elements according to their relative position to the end of the list by using negative indices. Indexing is also referred to as slicing.\n",
    "\n",
    "For example, `[-1]` selects the last element; `[1:3]` selects the second and the third elements, and `[:-2]` will select all elements excluding the last and second-to-last elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor(9)\n",
      "tensor([1, 2])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 10)\n",
    "print(x)\n",
    "print(x[-1])\n",
    "print(x[1:3])\n",
    "print(x[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "When we have multidimensional tensors, indexing rules work the same way as NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " shape of x[0]:torch.Size([2, 3, 4, 5])\n",
      " shape of x[0][0]:torch.Size([3, 4, 5])\n",
      " shape of x[0][0][0]:torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "# make a 5D tensor\n",
    "x = torch.rand(1, 2, 3, 4, 5)\n",
    "\n",
    "print(f\" shape of x[0]:{x[0].shape}\")\n",
    "print(f\" shape of x[0][0]:{x[0][0].shape}\")\n",
    "print(f\" shape of x[0][0][0]:{x[0][0][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Flatten and reshape**\n",
    "\n",
    "There are various methods for reshaping tensors. It is common to have to express 2D data in 1D format. Similarly, it is also common to have to reshape a 1D tensor into a 2D tensor. We can achieve this with the `.flatten()` and `.reshape()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original z: \n",
      " tensor([[ 0,  1],\n",
      "        [ 2,  3],\n",
      "        [ 4,  5],\n",
      "        [ 6,  7],\n",
      "        [ 8,  9],\n",
      "        [10, 11]])\n",
      "Flattened z: \n",
      " tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "Reshaped (3x4) z: \n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.arange(12).reshape(6, 2)\n",
    "print(f\"Original z: \\n {z}\")\n",
    "\n",
    "# 2D -> 1D\n",
    "z = z.flatten()\n",
    "print(f\"Flattened z: \\n {z}\")\n",
    "\n",
    "# and back to 2D\n",
    "z = z.reshape(3, 4)\n",
    "print(f\"Reshaped (3x4) z: \\n {z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "You will also see the `.view()` methods used a lot to reshape tensors. There is a subtle difference between `.view()` and `.reshape()`, though for now we will just use `.reshape()`. The documentation can be found in the Appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Squeezing tensors**\n",
    "\n",
    "When processing batches of data, you will quite often be left with singleton dimensions. E.g., `[1,10]` or `[256, 1, 3]`. This dimension can quite easily mess up your matrix operations if you don't plan on it being there...\n",
    "\n",
    "In order to compress tensors along their singleton dimensions we can use the `.squeeze()` method. We can use the `.unsqueeze()` method to do the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "x[0]: tensor([-1.6057,  0.2758, -0.0624, -1.5593, -0.8883, -0.7485,  0.1456, -0.6005,\n",
      "         0.8546,  1.1705])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 10)\n",
    "# printing the zeroth element of the tensor will not give us the first number!\n",
    "\n",
    "print(x.shape)\n",
    "print(f\"x[0]: {x[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Because of that pesky singleton dimension, `x[0]` gave us the first row instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "x[0]: -1.6056749820709229\n"
     ]
    }
   ],
   "source": [
    "# Let's get rid of that singleton dimension and see what happens now\n",
    "x = x.squeeze(0)\n",
    "print(x.shape)\n",
    "print(f\"x[0]: {x[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y: torch.Size([5, 5])\n",
      "Shape of y: torch.Size([5, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "# Adding singleton dimensions works a similar way, and is often used when tensors\n",
    "# being added need same number of dimensions\n",
    "\n",
    "y = torch.randn(5, 5)\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "\n",
    "# lets insert a singleton dimension\n",
    "y = y.unsqueeze(1)\n",
    "print(f\"Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Permutation**\n",
    "\n",
    "Sometimes our dimensions will be in the wrong order! For example, we may be dealing with RGB images with dim $[3\\times48\\times64]$, but our pipeline expects the colour dimension to be the last dimension, i.e., $[48\\times64\\times3]$. To get around this we can use the `.permute()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 64, 3])\n"
     ]
    }
   ],
   "source": [
    "# `x` has dimensions [color,image_height,image_width]\n",
    "x = torch.rand(3, 48, 64)\n",
    "\n",
    "# We want to permute our tensor to be [ image_height , image_width , color ]\n",
    "x = x.permute(1, 2, 0)\n",
    "# permute(1,2,0) means:\n",
    "# The 0th dim of my new tensor = the 1st dim of my old tensor\n",
    "# The 1st dim of my new tensor = the 2nd\n",
    "# The 2nd dim of my new tensor = the 0th\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "You may also see `.transpose()` used. This works in a similar way as permute, but can only swap two dimensions at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Concatenation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this example, we concatenate two matrices along rows (axis 0, the first element of the shape) vs. columns (axis 1, the second element of the shape). We can see that the first output tensorâ€™s axis-0 length (`6`) is the sum of the two input tensorsâ€™ axis-0 lengths (`3+3`); while the second output tensorâ€™s axis-1 length (`8`) is the sum of the two input tensorsâ€™ axis-1 lengths (`4+4`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated by rows: shape[6, 4] \n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 2.,  1.,  4.,  3.],\n",
      "        [ 1.,  2.,  3.,  4.],\n",
      "        [ 4.,  3.,  2.,  1.]])\n",
      "\n",
      " Concatenated by colums: shape[3, 8]  \n",
      " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
      "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "# Create two tensors of the same shape\n",
    "x = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "\n",
    "\n",
    "# Concatenate along rows\n",
    "cat_rows = torch.cat((x, y), dim=0)\n",
    "\n",
    "# Concatenate along columns\n",
    "cat_cols = torch.cat((x, y), dim=1)\n",
    "\n",
    "# Printing outputs\n",
    "print('Concatenated by rows: shape{} \\n {}'.format(list(cat_rows.shape), cat_rows))\n",
    "print('\\n Concatenated by colums: shape{}  \\n {}'.format(list(cat_cols.shape), cat_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Conversion to Other Python Objects**\n",
    "\n",
    "Converting a tensor to a numpy.ndarray, or vice versa, is easy, and the converted result does not share memory. This minor inconvenience is quite important: when you perform operations on the CPU or GPUs, you do not want to halt computation, waiting to see whether the NumPy package of Python might want to be doing something else with the same chunk of memory.\n",
    "\n",
    "When converting to a NumPy array, the information being tracked by the tensor will be lost, i.e., the computational graph. This will be covered in detail when you are introduced to autograd tomorrow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([-2.2472,  0.2189, -0.6323,  0.9613,  0.5662])  |  x type:  torch.FloatTensor\n",
      "y: [-2.2472162   0.21886738 -0.6322729   0.96129405  0.5662092 ]  |  y type:  <class 'numpy.ndarray'>\n",
      "z: tensor([-2.2472,  0.2189, -0.6323,  0.9613,  0.5662])  |  z type:  torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "print(f\"x: {x}  |  x type:  {x.type()}\")\n",
    "\n",
    "y = x.numpy()\n",
    "print(f\"y: {y}  |  y type:  {type(y)}\")\n",
    "\n",
    "z = torch.tensor(y)\n",
    "print(f\"z: {z}  |  z type:  {z.type()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "To convert a size-1 tensor to a Python scalar, we can invoke the item function or Pythonâ€™s built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.3: Manipulating Tensors\n",
    "Using a combination of the methods discussed above, complete the functions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Function A** \n",
    "\n",
    "This function takes in two 2D tensors $A$ and $B$ and returns the column sum of A multiplied by the sum of all the elmements of $B$, i.e., a scalar, e.g.,\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{If }\n",
    "  A = \\begin{bmatrix}\n",
    "  1 & 1 \\\\\n",
    "  1 & 1\n",
    "  \\end{bmatrix}\n",
    "  \\text{and }\n",
    "  B = \\begin{bmatrix}\n",
    "  1 & 2 & 3 \\\\\n",
    "  1 & 2 & 3\n",
    "  \\end{bmatrix}\n",
    "  \\text{ then }\n",
    "  Out =  \\begin{bmatrix}\n",
    "  2 & 2\n",
    "  \\end{bmatrix} \\cdot 12 = \\begin{bmatrix}\n",
    "  24 & 24\n",
    "  \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "**Function B** \n",
    "\n",
    "This function takes in a square matrix $C$ and returns a 2D tensor consisting of a flattened $C$ with the index of each element appended to this tensor in the row dimension, e.g.,\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{If }\n",
    "  C = \\begin{bmatrix}\n",
    "  2 & 3 \\\\\n",
    "  -1 & 10\n",
    "  \\end{bmatrix}\n",
    "  \\text{ then }\n",
    "  Out = \\begin{bmatrix}\n",
    "  0 & 2 \\\\\n",
    "  1 & 3 \\\\\n",
    "  2 & -1 \\\\\n",
    "  3 & 10\n",
    "  \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "**Hint:** Pay close attention to singleton dimensions.\n",
    "\n",
    "**Function C**\n",
    "\n",
    "This function takes in two 2D tensors $D$ and $E$. If the dimensions allow it, this function returns the elementwise sum of $D$-shaped $E$, and $D$; else this function returns a 1D tensor that is the concatenation of the two tensors, e.g.,\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{If }\n",
    "  D = \\begin{bmatrix}\n",
    "  1 & -1 \\\\\n",
    "  -1 & 3\n",
    "  \\end{bmatrix}\n",
    "  \\text{and } \n",
    "  E = \\begin{bmatrix}\n",
    "  2 & 3 & 0 & 2 \\\\\n",
    "  \\end{bmatrix}\n",
    "  \\text{ then } \n",
    "  Out = \\begin{bmatrix}\n",
    "  3 & 2 \\\\\n",
    "  -1 & 5\n",
    "  \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{If }\n",
    "  D = \\begin{bmatrix}\n",
    "  1 & -1 \\\\\n",
    "  -1 & 3\n",
    "  \\end{bmatrix}\n",
    "  \\text{and }\n",
    "  E = \\begin{bmatrix}\n",
    "  2 & 3 & 0  \\\\\n",
    "  \\end{bmatrix}\n",
    "  \\text{ then }\n",
    "  Out = \\begin{bmatrix}\n",
    "  1 & -1 & -1 & 3  & 2 & 3 & 0  \n",
    "  \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "**Hint:** `torch.numel()` is an easy way of finding the number of elements in a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24, 24])\n",
      "tensor([[ 0,  2],\n",
      "        [ 1,  3],\n",
      "        [ 2, -1],\n",
      "        [ 3, 10]])\n"
     ]
    }
   ],
   "source": [
    "def functionA(my_tensor1, my_tensor2):\n",
    "  \"\"\"\n",
    "  This function takes in two 2D tensors `my_tensor1` and `my_tensor2`\n",
    "  and returns the column sum of\n",
    "  `my_tensor1` multiplied by the sum of all the elmements of `my_tensor2`,\n",
    "  i.e., a scalar.\n",
    "\n",
    "  Args:\n",
    "    my_tensor1: torch.Tensor\n",
    "    my_tensor2: torch.Tensor\n",
    "\n",
    "  Retuns:\n",
    "    output: torch.Tensor\n",
    "      The multiplication of the column sum of `my_tensor1` by the sum of\n",
    "      `my_tensor2`.\n",
    "  \"\"\"\n",
    "  ################################################\n",
    "  ## for students: complete functionA\n",
    "  ################################################\n",
    "  #  multiplication the sum of the tensors\n",
    "  output = my_tensor1.sum(axis=0) * my_tensor2.sum()\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "def functionB(my_tensor):\n",
    "  \"\"\"\n",
    "  This function takes in a square matrix `my_tensor` and returns a 2D tensor\n",
    "  consisting of a flattened `my_tensor` with the index of each element\n",
    "  appended to this tensor in the row dimension.\n",
    "\n",
    "  Args:\n",
    "    my_tensor: torch.Tensor\n",
    "\n",
    "  Returns:\n",
    "    output: torch.Tensor\n",
    "      Concatenated tensor.\n",
    "  \"\"\"\n",
    "  ################################################\n",
    "  ##  for students: complete functionB\n",
    "  \n",
    "  ################################################\n",
    "  # flatten the tensor `my_tensor`\n",
    "  my_tensor = my_tensor.flatten().unsqueeze(1)\n",
    "  # create the idx tensor to be concatenated to `my_tensor`\n",
    "  idx_tensor = torch.arange(0, len(my_tensor)).unsqueeze(1)\n",
    "  # concatenate the two tensors\n",
    "  output = torch.cat((idx_tensor, my_tensor), dim=1)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "def functionC(my_tensor1, my_tensor2):\n",
    "  \"\"\"\n",
    "  This function takes in two 2D tensors `my_tensor1` and `my_tensor2`.\n",
    "  If the dimensions allow it, it returns the\n",
    "  elementwise sum of `my_tensor1`-shaped `my_tensor2`, and `my_tensor2`;\n",
    "  else this function returns a 1D tensor that is the concatenation of the\n",
    "  two tensors.\n",
    "\n",
    "  Args:\n",
    "    my_tensor1: torch.Tensor\n",
    "    my_tensor2: torch.Tensor\n",
    "\n",
    "  Returns:\n",
    "    output: torch.Tensor\n",
    "      Concatenated tensor.\n",
    "  \"\"\"\n",
    "  ################################################\n",
    "  ## for students: complete functionB\n",
    "  \n",
    "  ################################################\n",
    "  # check we can reshape `my_tensor2` into the shape of `my_tensor1`\n",
    "  if ...:\n",
    "    # TODO reshape `my_tensor2` into the shape of `my_tensor1`\n",
    "    my_tensor2 = ...\n",
    "    # TODO sum the two tensors\n",
    "    output = ...\n",
    "  else:\n",
    "    # TODO flatten both tensors\n",
    "    my_tensor1 = ...\n",
    "    my_tensor2 = ...\n",
    "    # TODO concatenate the two tensors in the correct dimension\n",
    "    output = ...\n",
    "\n",
    "  return output\n",
    "\n",
    "# add timing to airtable\n",
    "atform.add_event('Coding Exercise 2.3: Manipulating Tensors')\n",
    "\n",
    "\n",
    "\n",
    "## Implement the functions above and then uncomment the following lines to test your code\n",
    "print(functionA(torch.tensor([[1, 1], [1, 1]]), torch.tensor([[1, 2, 3], [1, 2, 3]])))\n",
    "print(functionB(torch.tensor([[2, 3], [-1, 10]])))\n",
    "# print(functionC(torch.tensor([[1, -1], [-1, 3]]), torch.tensor([[2, 3, 0, 2]])))\n",
    "# print(functionC(torch.tensor([[1, -1], [-1, 3]]), torch.tensor([[2, 3, 0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24, 24])\n",
      "tensor([[ 0,  2],\n",
      "        [ 1,  3],\n",
      "        [ 2, -1],\n",
      "        [ 3, 10]])\n",
      "tensor([[ 3,  2],\n",
      "        [-1,  5]])\n",
      "tensor([ 1, -1, -1,  3,  2,  3,  0])\n"
     ]
    }
   ],
   "source": [
    "# to_remove solution\n",
    "def functionA(my_tensor1, my_tensor2):\n",
    "  \"\"\"\n",
    "  This function takes in two 2D tensors `my_tensor1` and `my_tensor2`\n",
    "  and returns the column sum of\n",
    "  `my_tensor1` multiplied by the sum of all the elmements of `my_tensor2`,\n",
    "  i.e., a scalar.\n",
    "\n",
    "  Args:\n",
    "    my_tensor1: torch.Tensor\n",
    "    my_tensor2: torch.Tensor\n",
    "\n",
    "  Returns:\n",
    "    output: torch.Tensor\n",
    "      The multiplication of the column sum of `my_tensor1` by the sum of\n",
    "      `my_tensor2`.\n",
    "  \"\"\"\n",
    "  # TODO multiplication the sum of the tensors\n",
    "  output = my_tensor1.sum(axis=0) * my_tensor2.sum()\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "def functionB(my_tensor):\n",
    "  \"\"\"\n",
    "  This function takes in a square matrix `my_tensor` and returns a 2D tensor\n",
    "  consisting of a flattened `my_tensor` with the index of each element\n",
    "  appended to this tensor in the row dimension.\n",
    "\n",
    "  Args:\n",
    "    my_tensor: torch.Tensor\n",
    "\n",
    "  Returns:\n",
    "    output: torch.Tensor\n",
    "      Concatenated tensor.\n",
    "  \"\"\"\n",
    "  # TODO flatten the tensor `my_tensor`\n",
    "  my_tensor = my_tensor.flatten()\n",
    "  # TODO create the idx tensor to be concatenated to `my_tensor`\n",
    "  idx_tensor = torch.arange(0, len(my_tensor))\n",
    "  # TODO concatenate the two tensors\n",
    "  output = torch.cat([idx_tensor.unsqueeze(1), my_tensor.unsqueeze(1)], axis=1)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "def functionC(my_tensor1, my_tensor2):\n",
    "  \"\"\"\n",
    "  This function takes in two 2D tensors `my_tensor1` and `my_tensor2`.\n",
    "  If the dimensions allow it, it returns the\n",
    "  elementwise sum of `my_tensor1`-shaped `my_tensor2`, and `my_tensor2`;\n",
    "  else this function returns a 1D tensor that is the concatenation of the\n",
    "  two tensors.\n",
    "\n",
    "  Args:\n",
    "    my_tensor1: torch.Tensor\n",
    "    my_tensor2: torch.Tensor\n",
    "\n",
    "  Returns:\n",
    "    output: torch.Tensor\n",
    "      Concatenated tensor.\n",
    "  \"\"\"\n",
    "  # TODO check we can reshape `my_tensor2` into the shape of `my_tensor1`\n",
    "  if torch.numel(my_tensor1) == torch.numel(my_tensor2):\n",
    "    # TODO reshape `my_tensor2` into the shape of `my_tensor1`\n",
    "    my_tensor2 = my_tensor2.reshape(my_tensor1.shape)\n",
    "    # TODO sum the two tensors\n",
    "    output = my_tensor1 + my_tensor2\n",
    "  else:\n",
    "    # TODO flatten both tensors\n",
    "    my_tensor1 = my_tensor1.reshape(1, -1)\n",
    "    my_tensor2 = my_tensor2.reshape(1, -1)\n",
    "    # TODO concatenate the two tensors in the correct dimension\n",
    "    output = torch.cat([my_tensor1, my_tensor2], axis=1).squeeze()\n",
    "\n",
    "  return output\n",
    "\n",
    "# add timing to airtable\n",
    "atform.add_event('Coding Exercise 2.3: Manipulating Tensors')\n",
    "\n",
    "\n",
    "## Implement the functions above and then uncomment the following lines to test your code\n",
    "print(functionA(torch.tensor([[1, 1], [1, 1]]), torch.tensor([[1, 2, 3], [1, 2, 3]])))\n",
    "print(functionB(torch.tensor([[2, 3], [-1, 10]])))\n",
    "print(functionC(torch.tensor([[1, -1], [-1, 3]]), torch.tensor([[2, 3, 0, 2]])))\n",
    "print(functionC(torch.tensor([[1, -1], [-1, 3]]), torch.tensor([[2, 3, 0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "tensor([24, 24])\n",
    "tensor([[ 0,  2],\n",
    "        [ 1,  3],\n",
    "        [ 2, -1],\n",
    "        [ 3, 10]])\n",
    "tensor([[ 3,  2],\n",
    "        [-1,  5]])\n",
    "tensor([ 1, -1, -1,  3,  2,  3,  0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.4: GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a2c6e93f7143d9b694ca6befdffc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Video 6: GPU vs CPU\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1nM4y1K7qx\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"9Mc9GFUtILY\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add timing to airtable\n",
    "atform.add_event('Video 6: GPU vs CPU')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "By default, when we create a tensor it will *not* live on the GPU! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10)\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "When using Colab notebooks, by default, will not have access to a GPU. In order to start using GPUs we need to request one. We can do this by going to the runtime tab at the top of the page. \n",
    "\n",
    "By following *Runtime* â†’ *Change runtime type* and selecting **GPU** from the *Hardware Accelerator* dropdown list, we can start playing with sending tensors to GPUs.\n",
    "\n",
    "Once you have done this your runtime will restart and you will need to rerun the first setup cell to reimport PyTorch. Then proceed to the next cell.\n",
    "\n",
    "For more information on the GPU usage policy you can view in the Appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Now we have a GPU.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The cell below should return `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "[CUDA](https://developer.nvidia.com/cuda-toolkit) is an API developed by Nvidia for interfacing with GPUs. PyTorch provides us with a layer of abstraction, and allows us to launch CUDA kernels using pure Python.\n",
    "\n",
    "In short, we get the power of parallelizing our tensor computations on GPUs, whilst only writing (relatively) simple Python!\n",
    "\n",
    "Here, we define the function `set_device`, which returns the device use in the notebook, i.e., `cpu` or `cuda`. Unless otherwise specified, we use this function on top of every tutorial, and we store the device variable such as\n",
    "\n",
    "```python\n",
    "DEVICE = set_device()\n",
    "```\n",
    "\n",
    "Let's define the function using the PyTorch package `torch.cuda`, which is lazily initialized, so we can always import it, and use `is_available()` to determine if our system supports CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"GPU is not enabled in this notebook. \\n\"\n",
    "          \"If you want to enable it, in the menu under `Runtime` -> \\n\"\n",
    "          \"`Hardware accelerator.` and select `GPU` from the dropdown menu\")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook. \\n\"\n",
    "          \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n",
    "          \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's make some CUDA tensors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is enabled in this notebook. \n",
      "If you want to disable it, in the menu under `Runtime` -> \n",
      "`Hardware accelerator.` and select `None` from the dropdown menu\n",
      "torch.float32\n",
      "cuda:0\n",
      "y before calling to() | device: cpu | dtype: torch.FloatTensor\n",
      "y after calling to() | device: cuda:0 | dtype: torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# common device agnostic way of writing code that can run on cpu OR gpu\n",
    "# that we provide for you in each of the tutorials\n",
    "DEVICE = set_device()\n",
    "\n",
    "# we can specify a device when we first create our tensor\n",
    "x = torch.randn(2, 2, device=DEVICE)\n",
    "print(x.dtype)\n",
    "print(x.device)\n",
    "\n",
    "# we can also use the .to() method to change the device a tensor lives on\n",
    "y = torch.randn(2, 2)\n",
    "print(f\"y before calling to() | device: {y.device} | dtype: {y.type()}\")\n",
    "\n",
    "y = y.to(DEVICE)\n",
    "print(f\"y after calling to() | device: {y.device} | dtype: {y.type()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Operations between cpu tensors and cuda tensors**\n",
    "\n",
    "Note that the type of the tensor changed after calling `.to()`. What happens if we try and perform operations on tensors on devices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/hyunsu/Documents/GitHub/NMA-DL-course-content-dl/tutorials/W1D1_BasicsAndPytorch/W1D1_Tutorial1.ipynb Cell 99\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bubuntu223/home/hyunsu/Documents/GitHub/NMA-DL-course-content-dl/tutorials/W1D1_BasicsAndPytorch/W1D1_Tutorial1.ipynb#Y200sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m], device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bubuntu223/home/hyunsu/Documents/GitHub/NMA-DL-course-content-dl/tutorials/W1D1_BasicsAndPytorch/W1D1_Tutorial1.ipynb#Y200sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m## Uncomment the following line and run this cell\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bubuntu223/home/hyunsu/Documents/GitHub/NMA-DL-course-content-dl/tutorials/W1D1_BasicsAndPytorch/W1D1_Tutorial1.ipynb#Y200sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m z \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39;49m y\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([0, 1, 2], device=DEVICE)\n",
    "y = torch.tensor([3, 4, 5], device=\"cpu\")\n",
    "\n",
    "## Uncomment the following line and run this cell\n",
    "z = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We cannot combine CUDA tensors and CPU tensors in this fashion. If we want to compute an operation that combines tensors on different devices, we need to move them first! We can use the `.to()` method as before, or the `.cpu()` and `.cuda()` methods. Note that using the `.cuda()` will throw an error, if CUDA is not enabled in your machine.\n",
    "\n",
    "Generally, in this course, all Deep Learning is done on the GPU, and any computation is done on the CPU, so sometimes we have to pass things back and forth, so you'll see us call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 5, 7])\n",
      "tensor([ 9, 11, 13], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([0, 1, 2], device=DEVICE)\n",
    "y = torch.tensor([3, 4, 5], device=\"cpu\")\n",
    "z = torch.tensor([6, 7, 8], device=DEVICE)\n",
    "\n",
    "# moving to cpu\n",
    "x = x.to(\"cpu\")  # alternatively, you can use x = x.cpu()\n",
    "print(x + y)\n",
    "\n",
    "# moving to gpu\n",
    "y = y.to(DEVICE)  # alternatively, you can use y = y.cuda()\n",
    "print(y + z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.4: Just how much faster are GPUs?\n",
    "\n",
    "Below is a simple function `simpleFun`. Complete this function, such that it performs the operations:\n",
    "\n",
    "- Elementwise multiplication\n",
    "\n",
    "- Matrix multiplication\n",
    "\n",
    "The operations should be able to perfomed on either the CPU or GPU specified by the parameter `device`. We will use the helper function `timeFun(f, dim, iterations, device)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "dim = 10000\n",
    "iterations = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken for 1 iterations of simpleFun(10000, cpu): 7.78943\n",
      "time taken for 1 iterations of simpleFun(10000, cuda): 1.69751\n"
     ]
    }
   ],
   "source": [
    "def simpleFun(dim, device):\n",
    "  \"\"\"\n",
    "  Helper function to check device-compatiblity with computations\n",
    "\n",
    "  Args:\n",
    "    dim: Integer\n",
    "    device: String\n",
    "      \"cpu\" or \"cuda\"\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  ###############################################\n",
    "  ## TODO for students: recreate the function, but\n",
    "  ## ensure all computations happens on the `device`\n",
    "  ###############################################\n",
    "  # 2D tensor filled with uniform random numbers in [0,1), dim x dim\n",
    "  x = torch.rand(dim, dim).to(device)\n",
    "  # 2D tensor filled with uniform random numbers in [0,1), dim x dim\n",
    "  y = torch.rand(dim, dim).to(device)\n",
    "  # 2D tensor filled with the scalar value 2, dim x dim\n",
    "  z = torch.full((dim, dim), 2.0).to(device)\n",
    "\n",
    "  # elementwise multiplication of x and y\n",
    "  a = x * y\n",
    "  # matrix multiplication of x and z\n",
    "  b = x @ z\n",
    "\n",
    "  del x\n",
    "  del y\n",
    "  del z\n",
    "  del a\n",
    "  del b\n",
    "\n",
    "\n",
    "## TODO: Implement the function above and uncomment the following lines to test your code\n",
    "timeFun(f=simpleFun, dim=dim, iterations=iterations)\n",
    "timeFun(f=simpleFun, dim=dim, iterations=iterations, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Sample output (depends on your hardware)\n",
    "\n",
    "```\n",
    "time taken for 1 iterations of simpleFun(10000, cpu): 23.74070\n",
    "time taken for 1 iterations of simpleFun(10000, cuda): 0.87535\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Discuss!**\n",
    "\n",
    "Try and reduce the dimensions of the tensors and increase the iterations. You can get to a point where the cpu only function is faster than the GPU function. Why might this be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "The main difference between CPU and GPU architecture is that a\n",
    "- CPU is designed to handle a wide-range of tasks quickly\n",
    "(as measured by CPU clock speed), but are limited in the concurrency of tasks\n",
    "that can be running.\n",
    "- A GPU is designed to quickly render high-resolution images and video concurrently.\n",
    "\n",
    "In general, CPU is significantly faster when handling several different types of system\n",
    "operations (random access memory, mid-range computational operations,\n",
    "managing an operating system, I/O operations).\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.5: Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 7: Getting Data\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1744y127SQ\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"LSkjPM1gFu0\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add timing to airtable\n",
    "atform.add_event('Video 7: Getting Data')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "When training neural network models you will be working with large amounts of data. Fortunately, PyTorch offers some great tools that help you organize and manipulate your data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Import dataset and dataloaders related packages\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Grayscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Datasets**\n",
    "\n",
    "The `torchvision` package gives you easy access to many of the publicly available datasets. Let's load the [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset, which contains color images of 10 different classes, like vehicles and animals.\n",
    "\n",
    "Creating an object of type `datasets.CIFAR10` will automatically download and load all images from the dataset. The resulting data structure can be treated as a list containing data samples and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Number of samples: 50000\n",
      "Class names: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "# Download and load the images from the CIFAR10 dataset\n",
    "cifar10_data = datasets.CIFAR10(\n",
    "    root=\"/media/hyunsu/data2/01.DataAnalysis/data_ML/cifar10/\",  # path where the images will be stored\n",
    "    download=True,  # all images should be downloaded\n",
    "    transform=ToTensor()  # transform the images to tensors\n",
    "    )\n",
    "\n",
    "# Print the number of samples in the loaded dataset\n",
    "print(f\"Number of samples: {len(cifar10_data)}\")\n",
    "print(f\"Class names: {cifar10_data.classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We have 50,000 samples loaded. Now, let's take a look at one of them in detail. Each sample consists of an image and its corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: horse\n",
      "Image size: torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Choose a random sample\n",
    "random.seed(2021)\n",
    "image, label = cifar10_data[random.randint(0, len(cifar10_data))]\n",
    "print(f\"Label: {cifar10_data.classes[label]}\")\n",
    "print(f\"Image size: {image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Color images are modeled as 3 dimensional tensors. The first dimension corresponds to the channels ($\\text{C}$) of the image (in this case we have RGB images). The second dimensions is the height ($\\text{H}$) of the image and the third is the width ($\\text{W}$). We can denote this image format as $\\text{C} \\times \\text{H} \\times \\text{W}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.5: Display an image from the dataset\n",
    "\n",
    "Let's try to display the image using `matplotlib`. The code below will not work, because `imshow` expects to have the image in a different format, i.e., $\\text{C} \\times \\text{H} \\times \\text{W}$.\n",
    "\n",
    "You need to reorder the dimensions of the tensor using the `permute` method of the tensor. PyTorch `torch.permute(*dims)` rearranges the original tensor according to the desired ordering and returns a new multidimensional rotated tensor. The size of the returned tensor remains the same as that of the original.\n",
    "\n",
    "**Code hint:**\n",
    "\n",
    "```python\n",
    "# create a tensor of size 2 x 4\n",
    "input_var = torch.randn(2, 4)\n",
    "# print its size and the tensor\n",
    "print(input_var.size())\n",
    "print(input_var)\n",
    "\n",
    "# dimensions permuted\n",
    "input_var = input_var.permute(1, 0)\n",
    "# print its size and the permuted tensor\n",
    "print(input_var.size())\n",
    "print(input_var)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 280,
       "width": 282
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Uncomment the following line to see the error that arises from the current image format\n",
    "#plt.imshow(image)\n",
    "\n",
    "# TODO: Comment the above line and fix this code by reordering the tensor dimensions\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@title Video 8: Train and Test\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1rV411H7s5\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"JokSIuPs-ys\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add timing to airtable\n",
    "atform.add_event('Video 8: Train and Test')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Training and Test Datasets**\n",
    "\n",
    "When loading a dataset, you can specify if you want to load the training or the test samples using the `train` argument. We can load the training and test datasets separately. For simplicity, today we will not use both datasets separately, but this topic will be adressed in the next days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the training samples\n",
    "training_data = datasets.CIFAR10(\n",
    "    root=\"/media/hyunsu/data2/01.DataAnalysis/data_ML/cifar10/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    "    )\n",
    "\n",
    "# Load the test samples\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"/media/hyunsu/data2/01.DataAnalysis/data_ML/cifar10/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 9: Data Augmentation - Transformations\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV19B4y1N77t\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"sjegA9OBUPw\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add timing to airtable\n",
    "atform.add_event('Video 9: Data Augmentation - Transformations')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Dataloader**\n",
    "\n",
    "Another important concept is the `Dataloader`. It is a wrapper around the `Dataset` that splits it into minibatches (important for training the neural network) and makes the data iterable. The `shuffle` argument is used to shuffle the order of the samples across the minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Create dataloaders with\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "*Reproducibility:* DataLoader will reseed workers following Randomness in multi-process data loading algorithm. Use `worker_init_fn()` and a `generator` to preserve reproducibility:\n",
    "\n",
    "\n",
    "```python\n",
    "def seed_worker(worker_id):\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  numpy.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "\n",
    "g_seed = torch.Generator()\n",
    "g_seed.manual_seed(my_seed)\n",
    "\n",
    "DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g_seed\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Important:** For the `seed_worker` to have an effect, `num_workers` should be 2 or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can now query the next batch from the data loader and inspect it. For this we need to convert the dataloader object to a Python iterator using the function `iter` and then we can query the next batch using the function `next`.\n",
    "\n",
    "We can now see that we have a 4D tensor. This is because we have a 64 images in the batch ($B$) and each image has 3 dimensions: channels ($C$), height ($H$) and width ($W$). So, the size of the 4D tensor is $B \\times C \\times H \\times W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: torch.Size([64, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 280,
       "width": 282
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the next batch\n",
    "batch_images, batch_labels = next(iter(train_dataloader))\n",
    "print('Batch size:', batch_images.shape)\n",
    "\n",
    "# Display the first image from the batch\n",
    "plt.imshow(batch_images[0].permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Transformations**\n",
    "\n",
    "Another useful feature when loading a dataset is applying transformations on the data - color conversions, normalization, cropping, rotation etc. There are many predefined transformations in the `torchvision.transforms` package and you can also combine them using the `Compose` transform. Checkout the [pytorch documentation](https://pytorch.org/vision/stable/transforms.html) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.6: Load the CIFAR10 dataset as grayscale images\n",
    "\n",
    "The goal of this excercise is to load the images from the CIFAR10 dataset as grayscale images. Note that we rerun the `set_seed` function to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def my_data_load():\n",
    "  \"\"\"\n",
    "  Function to load CIFAR10 data as grayscale images\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    data: DataFrame\n",
    "      CIFAR10 loaded Dataframe of shape (3309, 14)\n",
    "  \"\"\"\n",
    "  ###############################################\n",
    "  ## TODO for students: load the CIFAR10 data,\n",
    "  ## but as grayscale images and not as RGB colored.\n",
    "  ###############################################\n",
    "  ## TODO Load the CIFAR10 data using a transform that converts the images to grayscale tensors\n",
    "  data = datasets.CIFAR10(...,\n",
    "                          transform=...)\n",
    "  # Display a random grayscale image\n",
    "  image, label = data[random.randint(0, len(data))]\n",
    "  plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "  plt.show()\n",
    "\n",
    "  return data\n",
    "\n",
    "\n",
    "set_seed(seed=2021)\n",
    "## After implementing the above code, uncomment the following lines to test your code\n",
    "# data = my_data_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def my_data_load():\n",
    "  \"\"\"\n",
    "  Function to load CIFAR10 data as grayscale images\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    data: DataFrame\n",
    "      CIFAR10 loaded Dataframe of shape (3309, 14)\n",
    "  \"\"\"\n",
    "  ## TODO Load the CIFAR10 data using a transform that converts the images to grayscale tensors\n",
    "  data = datasets.CIFAR10(root=\"data\", download=True,\n",
    "                          transform=Compose([ToTensor(), Grayscale()]))\n",
    "  # Display a random grayscale image\n",
    "  image, label = data[random.randint(0, len(data))]\n",
    "  plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "  plt.show()\n",
    "\n",
    "  return data\n",
    "\n",
    "\n",
    "set_seed(seed=2021)\n",
    "## After implementing the above code, uncomment the following lines to test your code\n",
    "data = my_data_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 3: Neural Networks\n",
    "\n",
    "*Time estimate: ~1 hour 30 mins (excluding video)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now it's time for you to create your first neural network using PyTorch. This section will walk you through the process of:\n",
    "\n",
    "- Creating a simple neural network model\n",
    "- Training the network\n",
    "- Visualizing the results of the network\n",
    "- Tweaking the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 10: CSV Files\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1xy4y1T7kv\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"JrC_UAJWYKU\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add timing to airtable\n",
    "atform.add_event('Video 10: CSV Files')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.1: Data Loading\n",
    "\n",
    "First we need some sample data to train our network on. You can use the function below to generate an example dataset consisting of 2D points along two interleaving half circles. The data will be stored in a file called `sample_data.csv`. You can inspect the file directly in Colab by going to Files on the left side and opening the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Generate sample data\n",
    "# @markdown we used `scikit-learn` module\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Create a dataset of 256 points with a little noise\n",
    "X, y = make_moons(256, noise=0.1)\n",
    "\n",
    "# Store the data as a Pandas data frame and save it to a CSV file\n",
    "df = pd.DataFrame(dict(x0=X[:,0], x1=X[:,1], y=y))\n",
    "df.to_csv('sample_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now we can load the data from the CSV file using the Pandas library. Pandas provides many functions for reading files in various formats. When loading data from a CSV file, we can reference the columns directly by their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Load the data from the CSV file in a Pandas DataFrame\n",
    "data = pd.read_csv(\"sample_data.csv\")\n",
    "\n",
    "# Create a 2D numpy array from the x0 and x1 columns\n",
    "X_orig = data[[\"x0\", \"x1\"]].to_numpy()\n",
    "\n",
    "# Create a 1D numpy array from the y column\n",
    "y_orig = data[\"y\"].to_numpy()\n",
    "\n",
    "# Print the sizes of the generated 2D points X and the corresponding labels Y\n",
    "print(f\"Size X:{X_orig.shape}\")\n",
    "print(f\"Size y:{y_orig.shape}\")\n",
    "\n",
    "# Visualize the dataset. The color of the points is determined by the labels `y_orig`.\n",
    "plt.scatter(X_orig[:, 0], X_orig[:, 1], s=40, c=y_orig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Prepare Data for PyTorch**\n",
    "\n",
    "Now let's prepare the data in a format suitable for PyTorch - convert everything into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Initialize the device variable\n",
    "DEVICE = set_device()\n",
    "\n",
    "# Convert the 2D points to a float32 tensor\n",
    "X = torch.tensor(X_orig, dtype=torch.float32)\n",
    "\n",
    "# Upload the tensor to the device\n",
    "X = X.to(DEVICE)\n",
    "\n",
    "print(f\"Size X:{X.shape}\")\n",
    "\n",
    "# Convert the labels to a long interger tensor\n",
    "y = torch.from_numpy(y_orig).type(torch.LongTensor)\n",
    "\n",
    "# Upload the tensor to the device\n",
    "y = y.to(DEVICE)\n",
    "\n",
    "print(f\"Size y:{y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.2: Create a Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 11: Generating the Neural Network\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1fK4y1M74a\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"PwSzRohUvck\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add timing to airtable\n",
    "atform.add_event('Video 11: Generating the Neural Network')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "For this example we want to have a simple neural network consisting of 3 layers:\n",
    "\n",
    "- 1 input layer of size 2 (our points have 2 coordinates)\n",
    "- 1 hidden layer of size 16 (you can play with different numbers here)\n",
    "- 1 output layer of size 2 (we want the have the scores for the two classes)\n",
    "\n",
    "During the course you will deal with differend kinds of neural networks. On Day 2, we will focus on linear networks, but you will work with some more complicated architectures in the next days. The example here is meant to demonstrate the process of creating and training a neural network end-to-end.\n",
    "\n",
    "**Programing the Network**\n",
    "\n",
    "PyTorch provides a base class for all neural network modules called [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). You need to inherit from `nn.Module` and implement some important methods:\n",
    "\n",
    "* `__init__`\n",
    "\n",
    "  In the `__init__` method you need to define the structure of your network. Here you will specify what layers will the network consist of, what activation functions will be used etc.\n",
    "\n",
    "* `forward`\n",
    "\n",
    "  All neural network modules need to implement the `forward` method. It specifies the computations the network needs to do when data is passed through it.\n",
    "\n",
    "* `predict`\n",
    "\n",
    "  This is not an obligatory method of a neural network module, but it is a good practice if you want to quickly get the most likely label from the network. It calls the `forward` method and chooses the label with the highest score.\n",
    "\n",
    "* `train`\n",
    "\n",
    "  This is also not an obligatory method, but it is a good practice to have. The method will be used to train the network parameters and will be implemented later in the notebook.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note:** You can use the `__call__` method of a module directly and it will invoke the `forward` method: `net()` does the same as `net.forward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Inherit from nn.Module - the base class for neural network modules provided by Pytorch\n",
    "class NaiveNet(nn.Module):\n",
    "  \"\"\"\n",
    "  NaiveNet architecture\n",
    "  Structure is as follows:\n",
    "  Linear Layer (2, 16) -> ReLU activation -> Linear Layer (16, 2)\n",
    "  \"\"\"\n",
    "  # Define the structure of your network\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "    Defines the NaiveNet structure by initialising following attributes\n",
    "    nn.Linear (2, 16):  Transformation from the input to the hidden layer\n",
    "    nn.ReLU: Activation function (ReLU) is a non-linearity which is widely used because it reduces computation.\n",
    "             The function returns 0 if it receives any negative input, but for any positive value x, it returns that value back.\n",
    "    nn.Linear (16, 2): Transformation from the hidden to the output layer\n",
    "\n",
    "    Args:\n",
    "      None\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super(NaiveNet, self).__init__()\n",
    "\n",
    "    # The network is defined as a sequence of operations\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Linear(2, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 2),\n",
    "    )\n",
    "\n",
    "  # Specify the computations performed on the data\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Defines the forward pass through the above defined structure\n",
    "\n",
    "    Args:\n",
    "      x: torch.Tensor\n",
    "        Input tensor of size ([3])\n",
    "\n",
    "    Returns:\n",
    "      layers: nn.module\n",
    "        Initialised Layers in order to re-use the same layer for each forward pass of data you make.\n",
    "    \"\"\"\n",
    "    # Pass the data through the layers\n",
    "    return self.layers(x)\n",
    "\n",
    "  # Choose the most likely label predicted by the network\n",
    "  def predict(self, x):\n",
    "    \"\"\"\n",
    "    Performs the prediction task of the network\n",
    "\n",
    "    Args:\n",
    "      x: torch.Tensor\n",
    "        Input tensor of size ([3])\n",
    "\n",
    "    Returns:\n",
    "      Most likely class i.e., Label with the highest score\n",
    "    \"\"\"\n",
    "    # Pass the data through the networks\n",
    "    output = self.forward(x)\n",
    "\n",
    "    # Choose the label with the highest score\n",
    "    return torch.argmax(output, 1)\n",
    "\n",
    "  # Train the neural network (will be implemented later)\n",
    "  def train(self, X, y):\n",
    "    \"\"\"\n",
    "    Training the Neural Network\n",
    "\n",
    "    Args:\n",
    "      X: torch.Tensor\n",
    "        Input data\n",
    "      y: torch.Tensor\n",
    "        Class Labels/Targets\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Check that your network works**\n",
    "\n",
    "Create an instance of your model and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Create new NaiveNet and transfer it to the device\n",
    "model = NaiveNet().to(DEVICE)\n",
    "\n",
    "# Print the structure of the network\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 3.2: Classify some samples\n",
    "\n",
    "Now, let's pass some of the points of our dataset through the network and see if it works. You should not expect the network to actually classify the points correctly, because it has not been trained yet. \n",
    "\n",
    "The goal here is just to get some experience with the data structures that are passed to the forward and predict methods and their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "## Get the samples\n",
    "# X_samples = ...\n",
    "# print(\"Sample input:\\n\", X_samples)\n",
    "\n",
    "## Do a forward pass of the network\n",
    "# output = ...\n",
    "# print(\"\\nNetwork output:\\n\", output)\n",
    "\n",
    "## Predict the label of each point\n",
    "# y_predicted = ...\n",
    "# print(\"\\nPredicted labels:\\n\", y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "## Get the samples\n",
    "X_samples = X[0:5]\n",
    "print(\"Sample input:\\n\", X_samples)\n",
    "\n",
    "## Do a forward pass of the network\n",
    "output = model.forward(X_samples)\n",
    "print(\"\\nNetwork output:\\n\", output)\n",
    "\n",
    "## Predict the label of each point\n",
    "y_predicted = model.predict(X_samples)\n",
    "print(\"\\nPredicted labels:\\n\", y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "Sample input:\n",
    " tensor([[ 0.9066,  0.5052],\n",
    "        [-0.2024,  1.1226],\n",
    "        [ 1.0685,  0.2809],\n",
    "        [ 0.6720,  0.5097],\n",
    "        [ 0.8548,  0.5122]], device='cuda:0')\n",
    "\n",
    "Network output:\n",
    " tensor([[ 0.1543, -0.8018],\n",
    "        [ 2.2077, -2.9859],\n",
    "        [-0.5745, -0.0195],\n",
    "        [ 0.1924, -0.8367],\n",
    "        [ 0.1818, -0.8301]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
    "\n",
    "Predicted labels:\n",
    " tensor([0, 0, 1, 0, 0], device='cuda:0')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.3: Train Your Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 12: Train the Network\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1v54y1n7CS\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"4MIqnE4XPaA\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add timing to airtable\n",
    "atform.add_event('Video 12: Train the Network')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now it is time to train your network on your dataset. Don't worry if you don't fully understand everything yet - we will cover training in much more details in the next days. For now, the goal is just to see your network in action!\n",
    "\n",
    "You will usually implement the `train` method directly when implementing your class `NaiveNet`. Here, we will implement it as a function outside of the class in order to have it in a separate cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Helper function to plot the decision boundary\n",
    "\n",
    "# Code adapted from this notebook: https://jonchar.net/notebooks/Artificial-Neural-Network-with-Keras/\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_decision_boundary(model, X, y, device):\n",
    "  \"\"\"\n",
    "  Helper function to plot decision boundary\n",
    "\n",
    "  Args:\n",
    "    model: nn.module\n",
    "      NaiveNet instance\n",
    "    X: torch.tensor\n",
    "      Input CIFAR10 data\n",
    "    y: torch.tensor\n",
    "      Class Labels/Targets\n",
    "    device: String\n",
    "      \"cpu\" or \"cuda\"\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  # Transfer the data to the CPU\n",
    "  X = X.cpu().numpy()\n",
    "  y = y.cpu().numpy()\n",
    "\n",
    "  # Check if the frames folder exists and create it if needed\n",
    "  frames_path = Path(\"frames\")\n",
    "  if not frames_path.exists():\n",
    "    frames_path.mkdir()\n",
    "\n",
    "  # Set min and max values and give it some padding\n",
    "  x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "  y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "  h = 0.01\n",
    "\n",
    "  # Generate a grid of points with distance h between them\n",
    "  xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "  # Predict the function value for the whole gid\n",
    "  grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "  grid_points = torch.from_numpy(grid_points).type(torch.FloatTensor)\n",
    "  Z = model.predict(grid_points.to(device)).cpu().numpy()\n",
    "  Z = Z.reshape(xx.shape)\n",
    "\n",
    "  # Plot the contour and training examples\n",
    "  plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "  plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Implement the train function given a training dataset X and correcsponding labels y\n",
    "def train(model, X, y):\n",
    "  \"\"\"\n",
    "    Training the Neural Network\n",
    "\n",
    "    Args:\n",
    "      X: torch.Tensor\n",
    "        Input data\n",
    "      y: torch.Tensor\n",
    "        Class Labels/Targets\n",
    "\n",
    "    Returns:\n",
    "      losses: Float\n",
    "        Cross Entropy Loss; Cross-entropy builds upon the idea of entropy\n",
    "        from information theory and calculates the number of bits required\n",
    "        to represent or transmit an average event from one distribution\n",
    "        compared to another distribution.\n",
    "    \"\"\"\n",
    "  # The Cross Entropy Loss is suitable for classification problems\n",
    "  loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "  # Create an optimizer (Stochastic Gradient Descent) that will be used to train the network\n",
    "  learning_rate = 1e-2\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  # Number of epochs\n",
    "  epochs = 15000\n",
    "\n",
    "  # List of losses for visualization\n",
    "  losses = []\n",
    "\n",
    "  for i in range(epochs):\n",
    "    # Pass the data through the network and compute the loss\n",
    "    # We'll use the whole dataset during the training instead of using batches\n",
    "    # in to order to keep the code simple for now.\n",
    "    y_logits = model.forward(X)\n",
    "    loss = loss_function(y_logits, y)\n",
    "\n",
    "    # Clear the previous gradients and compute the new ones\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Adapt the weights of the network\n",
    "    optimizer.step()\n",
    "\n",
    "    # Store the loss\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Print the results at every 1000th epoch\n",
    "    if i % 1000 == 0:\n",
    "      print(f\"Epoch {i} loss is {loss.item()}\")\n",
    "\n",
    "      plot_decision_boundary(model, X, y, DEVICE)\n",
    "      plt.savefig('frames/{:05d}.png'.format(i))\n",
    "\n",
    "  return losses\n",
    "\n",
    "\n",
    "# Create a new network instance a train it\n",
    "model = NaiveNet().to(DEVICE)\n",
    "losses = train(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Plot the loss during training**\n",
    "\n",
    "Plot the loss during the training to see how it reduces and converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(1, len(losses), len(losses)), losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Visualize the training process\n",
    "# @markdown Execute this cell!\n",
    "!pip install imageio --quiet\n",
    "!pip install pathlib --quiet\n",
    "\n",
    "import imageio\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Make a list with all images\n",
    "images = []\n",
    "for i in range(10):\n",
    "  filename = Path(\"frames/0\"+str(i)+\"000.png\")\n",
    "  images.append(imageio.imread(filename))\n",
    "# Save the gif\n",
    "imageio.mimsave('frames/movie.gif', images)\n",
    "gifPath = Path(\"frames/movie.gif\")\n",
    "with open(gifPath,'rb') as f:\n",
    "  display(Image(data=f.read(), format='png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 13: Play with it\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1Cq4y1W7BH\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"_GGkapdOdSY\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add timing to airtable\n",
    "atform.add_event('Video 13: Play with it')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Exercise 3.3: Tweak your Network\n",
    "\n",
    "You can now play around with the network a little bit to get a feeling of what different parameters are doing. Here are some ideas what you could try:\n",
    "\n",
    "- Increase or decrease the number of epochs for training\n",
    "- Increase or decrease the size of the hidden layer\n",
    "- Add one additional hidden layer\n",
    "\n",
    "Can you get the network to better fit the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "Typically:\n",
    "1. Training for longer number of epochs tends to increase performance (but early stopping also works in some cases).\n",
    "2. Increased size of hidden layers aka width increases capacity leading to double descent.\n",
    "3. Increasing number of hidden layers aka depth typically makes the network more expressive (but networks that're very deep overfit).\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 14: XOR Widget\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1mB4y1N7QS\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"oTr1nE2rCWg\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add timing to airtable\n",
    "atform.add_event('Video 14: XOR Widget')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Exclusive OR (XOR) logical operation gives a true (`1`) output when the number of true inputs is odd. That is, a true output result if one, and only one, of the inputs to the gate is true. If both inputs are false (`0`) or both are true or false output results. Mathematically speaking, XOR represents the inequality function, i.e., the output is true if the inputs are not alike; otherwise, the output is false.\n",
    "\n",
    "In case of two inputs ($X$ and $Y$) the following truth table is applied:\n",
    "\n",
    "\\begin{matrix}\n",
    "  X & Y & \\text{XOR}\\\\\n",
    "  \\hline\n",
    "  0 & 0 & 0\\\\\n",
    "  0 & 1 & 1\\\\\n",
    "  1 & 0 & 1\\\\\n",
    "  1 & 1 & 0\n",
    "\\end{matrix}\n",
    "\n",
    "Here, with `0`, we denote `False`, and with `1` we denote `True` in boolean terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Interactive Demo 3.3: Solving XOR\n",
    "\n",
    "Here we use an open source and famous visualization widget developed by Tensorflow team available [here](https://github.com/tensorflow/playground).\n",
    "* Play with the widget and observe if you can solve the continuous XOR dataset.\n",
    "* Now add one hidden layer with three units, play with the widget, and set weights by hand to solve this dataset perfectly.\n",
    "\n",
    "For the second part, you should set the weights by clicking on the connections and either type the value or use the up and down keys to change it by one increment. You could also do the same for the biases by clicking on the tiny square to each neuron's bottom left.\n",
    "Even though there are infinitely many solutions, a neat solution when $f(x)$ is ReLU is: \n",
    "\n",
    "\\begin{equation}\n",
    "  y = f(x_1)+f(x_2)-f(x_1+x_2)\n",
    "\\end{equation}\n",
    "\n",
    "Try to set the weights and biases to implement this function after you played enough :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Play with the parameters to solve XOR\n",
    "from IPython.display import IFrame\n",
    "IFrame(\"https://playground.arashash.com/#activation=relu&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.91390&showTestData=false&discretize=false&percTrainData=90&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\", width=1020, height=660)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Do you think we can solve the discrete XOR (only 4 possibilities) with only 2 hidden units?\n",
    "w1_min_xor = 'Select' #@param ['Select', 'Yes', 'No']\n",
    "if w1_min_xor == 'Yes':\n",
    "  print(\"Awesome. Indeed, yes. We take the two points for which the output should be 1 and dedicate one of the hidden units to each of them. Each of theses  ReLU functions are diagonal and tuned so that only for one of those two points the output is 1. In the end, we add these two together. And voila - discrete xor.\")\n",
    "else:\n",
    "  print(\"How about giving it another try?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 4: Ethics And Course Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 15: Ethics\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1Hw41197oB\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"Kt6JLi3rUFU\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 16: Be a group\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1j44y1272h\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"Sfp6--d_H1A\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 17: Syllabus\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1iB4y1N7uQ\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"cDvAqG_hAvQ\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Meet our lecturers\n",
    "\n",
    "### Week 1: the building blocks\n",
    "* [Konrad Kording](https://kordinglab.com)\n",
    "* [Andrew Saxe](https://www.saxelab.org/)\n",
    "* [Surya Ganguli](https://ganguli-gang.stanford.edu/)\n",
    "* [Ioannis Mitliagkas](http://mitliagkas.github.io/)\n",
    "* [Lyle Ungar](https://www.cis.upenn.edu/~ungar/)\n",
    "\n",
    "### Week 2: making things work\n",
    "* [Alona Fyshe](https://webdocs.cs.ualberta.ca/~alona/)\n",
    "* [Alexander Ecker](https://eckerlab.org/)\n",
    "* [James Evans](https://sociology.uchicago.edu/directory/james-evans)\n",
    "* [He He](https://hhexiy.github.io/)\n",
    "* [Vikash Gilja](https://tnel.ucsd.edu/bio) and [Akash Srivastava](https://akashgit.github.io/)\n",
    "\n",
    "### Week 3: more magic\n",
    "* [Tim Lillicrap](https://contrastiveconvergence.net/~timothylillicrap/index.php) and [Blake Richards](https://www.mcgill.ca/neuro/blake-richards-phd)\n",
    "* [Jane Wang](http://www.janexwang.com/) and [Feryal Behbahani](https://feryal.github.io/)\n",
    "* [Tim Lillicrap](https://contrastiveconvergence.net/~timothylillicrap/index.php) and [Blake Richards](https://www.mcgill.ca/neuro/blake-richards-phd)\n",
    "* [Josh Vogelstein](https://jovo.me/) and [Vincenzo Lamonaco](https://www.vincenzolomonaco.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, go to the [visualization of ICLR papers](https://iclr.cc/virtual/2021/paper_vis.html). Read a few abstracts. Look at the various clusters. Where do you see yourself in this map?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Submit to Airtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 18: Submission info\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1e44y127ti\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"JwTn7ej2dq8\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This is Darryl, the Deep Learning Dapper Lion, and he's here to teach you about content submission to airtable.\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/static/DapperLion.png\" alt=\"Darryl\"> \n",
    "<br><br>\n",
    "At the end of each tutorial there will be an <b>Airtable Submission Cell</b>. Run the cell to generate the airtable submission button and click on it to submit your information to airtable.\n",
    "<br><br>\n",
    "if it is the last tutorial of the day your button will look like this and take you to the end of day survey: \n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/static/SurveyButton.png?raw=1\" alt=\"Survey Button\">\n",
    "\n",
    "Otherwise it look like this: \n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/static/AirtableSubmissionButton.png?raw=1\" alt=\"Submission Button\"> \n",
    "<br><br>\n",
    "\n",
    "It is critical that you push the submit button for every tutorial you run. **Even if you don't finish the tutorial, still submit!**\n",
    "Submitting is the only way we can verify that you attempted each tutorial, which is critical for us to be able to track your progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### TL;DR: Basic tutorial workflow\n",
    "\n",
    "1. Work through the tutorial, answering **Think!** questions and **Coding Exercises**.\n",
    "\n",
    "2. At the end of each tutorial, (even if the tutorial is incomplete) run the airtable submission code cell.\n",
    "\n",
    "3. Push the *Submission* button.\n",
    "\n",
    "4. If the last tutorial of the day, *Submission* button will also take you to the end of the day survey on a new page. Complete that and submit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Submission FAQs: \n",
    "\n",
    "1. What if I want to change my answers to previous discussion questions? \n",
    "> You are free to change and resubmit any of the answers and Think! questions as many times as you like. However, **please only run the airtable submission code and click on the link once you are ready to submit**.\n",
    "\n",
    "2. Okay, but what if I submitted my airtable anyway and really want to resubmit?\n",
    "> After making changes, you can re-run the airtable submission cell code cell. This will result in a second submission from you for our database. But, this will make Darryl sad as it will be more work for him to clean up the data later. \n",
    "\n",
    "3. HELP! I accidentally ran the code to generate the airtable submission button before I was ready to submit! what do I do?\n",
    "> If you run the code to generate the link, anything that happens afterwards will not be captured. Complete the tutorial and make sure to re-run the airtable submission again when you are finished before pressing the submission button. \n",
    "\n",
    "4. What if I want to work on this on my own later, should I wait to submit until I'm finished?\n",
    ">  Please submit wherever you are at the end of the day. It's great that you want to keep working on this, but it's important to see the places where we tried things that didn't quite work out, so we can fix them for next year.  \n",
    "\n",
    "Finally, we try to keep the airtable code as hidden as possible, but if you ever see any calls to `atform`  such as `atform.add_event()` in the coding exercises, just know that is for saving airtable information only. **It will not affect the code that is being run around it in any way**, so please do not modify, comment out, or worry about any of those lines of code.\n",
    "<br><br><br>\n",
    "\n",
    "Now, lets try submitting today's course to Airtable by running the next cell and clicking the button when it appears. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Airtable Submission Link\n",
    "from IPython import display as IPyDisplay\n",
    "IPyDisplay.HTML(\n",
    "    f\"\"\"\n",
    "  <div>\n",
    "    <a href= \"{atform.url()}\" target=\"_blank\">\n",
    "    <img src=\"https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/static/SurveyButton.png?raw=1\"\n",
    "  alt=\"button link to survey\" style=\"width:410px\"></a>\n",
    "    </div>\"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Bonus - 60 years of Machine Learning Research in one Plot\n",
    "\n",
    "By [Hendrik Strobelt](http://hendrik.strobelt.com) (MIT-IBM Watson AI Lab) with support from Benjamin Hoover.\n",
    "\n",
    "In this notebook we visualize a subset* of 3,300 articles retreived from the AllenAI [S2ORC dataset](https://github.com/allenai/s2orc). We represent each paper by a position that is output of a dimensionality reduction method applied to a vector representation of each paper. The vector representation is the output of a neural network.\n",
    "\n",
    "**Note:** The selection is very biased on the keywords and methodology we used to filter. Please see the details section to learn about what we did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install and Import `altair` and `vega_datasets`.\n",
    "!pip install altair vega_datasets --quiet\n",
    "\n",
    "import altair as alt  # altair is defining data visualizations\n",
    "\n",
    "# Source data files\n",
    "# Position data file maps ID to x,y positions\n",
    "# original link: http://gltr.io/temp/ml_regexv1_cs_ma_citation+_99perc.pos_umap_cosine_100_d0.1.json\n",
    "POS_FILE = 'https://osf.io/qyrfn/download'\n",
    "# original link: http://gltr.io/temp/ml_regexv1_cs_ma_citation+_99perc_clean.csv\n",
    "# Metadata file maps ID to title, abstract, author,....\n",
    "META_FILE = 'https://osf.io/vfdu6/download'\n",
    "\n",
    "# data loading and wrangling\n",
    "def load_data():\n",
    "  \"\"\"\n",
    "  Loading the data\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Merged read dataFrame combining id and paper_id;\n",
    "  \"\"\"\n",
    "  positions = pd.read_json(POS_FILE)\n",
    "  positions[['x', 'y']] = positions['pos'].to_list()\n",
    "  meta = pd.read_csv(META_FILE)\n",
    "  return positions.merge(meta, left_on='id', right_on='paper_id')\n",
    "\n",
    "\n",
    "# load data\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Define Visualization using ALtair\n",
    "YEAR_PERIOD = \"quinquennial\"  # @param\n",
    "selection = alt.selection_multi(fields=[YEAR_PERIOD], bind='legend')\n",
    "data[YEAR_PERIOD] = (data[\"year\"] / 5.0).apply(np.floor) * 5\n",
    "chart = alt.Chart(data[[\"x\", \"y\", \"authors\", \"title\", YEAR_PERIOD, \"citation_count\"]], width=800,\n",
    "                  height=800).mark_circle(radius=2, opacity=0.2).encode(\n",
    "    alt.Color(YEAR_PERIOD+':O',\n",
    "              scale=alt.Scale(scheme='viridis', reverse=False, clamp=True, domain=list(range(1955,2020,5))),\n",
    "              # legend=alt.Legend(title='Total Records')\n",
    "              ),\n",
    "    alt.Size('citation_count',\n",
    "              scale=alt.Scale(type=\"pow\", exponent=1, range=[15, 300])\n",
    "              ),\n",
    "       alt.X('x:Q',\n",
    "        scale=alt.Scale(zero=False), axis=alt.Axis(labels=False)\n",
    "    ),\n",
    "       alt.Y('y:Q',\n",
    "        scale=alt.Scale(zero=False), axis=alt.Axis(labels=False)\n",
    "    ),\n",
    "    tooltip=['title', 'authors'],\n",
    "    # size='citation_count',\n",
    "    # color=\"decade:O\",\n",
    "    opacity=alt.condition(selection, alt.value(.8), alt.value(0.2)),\n",
    "\n",
    ").add_selection(\n",
    "    selection\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Lets look at the Visualization. Each dot represents one paper. Close dots mean that the respective papers are closer related than distant ones. The color indicates the 5-year period of when the paper was published. The dot size indicates the citation count (within S2ORC corpus) as of July 2020. \n",
    "\n",
    "The view is **interactive** and allows for three main interactions. Try them and play around:\n",
    "1. Hover over a dot to see a tooltip (title, author)\n",
    "2. Select a year in the legend (right) to filter dots\n",
    "3. Zoom in/out with scroll -- double click resets view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Questions\n",
    "\n",
    "By playing around, can you find some answers to the following questions?\n",
    "\n",
    "1. Can you find topical clusters? What cluster might occur because of a filtering error?\n",
    "2. Can you see a temporal trend in the data and clusters?\n",
    "3. Can you determine when deep learning methods started booming ?\n",
    "4. Can you find the key papers that where written before the DL \"winter\" that define milestones for a cluster? (tip: look for large dots of different color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "1. As specified below, the data is already filtered for topics such as Computer Science/Mathematics.\n",
    "Filtering errors could occur if keywords in a paper are incorrectly tagged or if cases don't match etc.\n",
    "\n",
    "2. To look for temporal trends in the data/clusters, observe the color transitions in the above\n",
    "visualization. We see that a lot more papers were published in diversified topics,\n",
    "as we transitioned out of AI Winters.\n",
    "\n",
    "3. Based on the color of the clusters, we can infer that deep learning methods\n",
    "boomed between the 2010 and 2015 period.\n",
    "\n",
    "4. After filtering around the mid 1900's, hovering on the larger dots show\n",
    "the key papers before the DL winters.\n",
    "For instance, \"Neural networks and physical systems with emergent\n",
    "collective computational abilities\" by John J Hopfield (1980's)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Methods\n",
    "\n",
    "Here is what we did:\n",
    "1. Filtering of all papers who fullfilled the criterria:\n",
    "  - are categorized as `Computer Science` or `Mathematics` \n",
    "  - one of the following keywords appearing in title or abstract: `\"machine learning|artificial intelligence|neural network|(machine|computer) vision|perceptron|network architecture| RNN | CNN | LSTM | BLEU | MNIST | CIFAR |reinforcement learning|gradient descent| Imagenet \"`\n",
    "2. Per year, remove all papers that are below the 99 percentile of citation count in that year\n",
    "3. Embed each paper by using abstract + title in SPECTER model\n",
    "4. Project based on embedding using UMAP\n",
    "5. Visualize using Altair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Find Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Edit the `AUTHOR_FILTER` variable to full text search for authors.\n",
    "\n",
    "AUTHOR_FILTER = \"Rush \"  # @param space at the end means \"word border\"\n",
    "\n",
    "### Don't ignore case when searching...\n",
    "FLAGS = 0\n",
    "### uncomment do ignore case\n",
    "# FLAGS = re.IGNORECASE\n",
    "\n",
    "## --- FILTER CODE.. make it your own ---\n",
    "data['issel'] = data['authors'].str.contains(AUTHOR_FILTER, na=False, flags=FLAGS, )\n",
    "if data['issel'].mean()<0.0000000001:\n",
    "  print('No match found')\n",
    "\n",
    "## --- FROM HERE ON VIS CODE ---\n",
    "alt.Chart(data[[\"x\", \"y\", \"authors\", \"title\", YEAR_PERIOD, \"citation_count\", \"issel\"]], width=800,\n",
    "                  height=800) \\\n",
    "    .mark_circle(stroke=\"black\", strokeOpacity=1).encode(\n",
    "    alt.Color(YEAR_PERIOD+':O',\n",
    "              scale=alt.Scale(scheme='viridis', reverse=False),\n",
    "              # legend=alt.Legend(title='Total Records')\n",
    "              ),\n",
    "    alt.Size('citation_count',\n",
    "              scale=alt.Scale(type=\"pow\", exponent=1, range=[15, 300])\n",
    "              ),\n",
    "    alt.StrokeWidth('issel:Q', scale=alt.Scale(type=\"linear\", domain=[0,1], range=[0, 2]), legend=None),\n",
    "\n",
    "    alt.Opacity('issel:Q', scale=alt.Scale(type=\"linear\", domain=[0,1], range=[.2, 1]), legend=None),\n",
    "    alt.X('x:Q',\n",
    "        scale=alt.Scale(zero=False), axis=alt.Axis(labels=False)\n",
    "    ),\n",
    "    alt.Y('y:Q',\n",
    "        scale=alt.Scale(zero=False), axis=alt.Axis(labels=False)\n",
    "    ),\n",
    "    tooltip=['title', 'authors'],\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Appendix\n",
    "\n",
    "## Official PyTorch resources:\n",
    "\n",
    "### Tutorials\n",
    "- [https://pytorch.org/tutorials/](https://pytorch.org/tutorials/)\n",
    "\n",
    "### Documentation\n",
    "- [https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html) (tensor methods)\n",
    "\n",
    "- [https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view) (The view method in particular)\n",
    "\n",
    "- [https://pytorch.org/vision/stable/datasets.html](https://pytorch.org/vision/stable/datasets.html) (pre-loaded image datasets)\n",
    "\n",
    "## Google Colab Resources:\n",
    "- [https://research.google.com/colaboratory/faq.html](https://research.google.com/colaboratory/faq.html) (FAQ including guidance on GPU usage)\n",
    "\n",
    "## Books for reference:\n",
    "- [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/) (Deep Learning by Ian Goodfellow, Yoshua Bengio and Aaron Courville)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D1_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('th1p12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "540ee3e445d1da392d2b8a7d647dcbc2bd268a0d669d2281aeea3f4a5da62e9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
